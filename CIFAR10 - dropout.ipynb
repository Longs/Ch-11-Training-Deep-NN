{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#TODO: remove the following hack from https://stackoverflow.com/questions/69687794/unable-to-manually-load-cifar10-dataset\n",
    "#import ssl\n",
    "#ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "cifar10 = keras.datasets.cifar10\n",
    "(X_train_full, y_train_full), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "#Scale the pixel values\n",
    "X_train_full, X_test = X_train_full/255.0, X_test/255.0\n",
    "y_train_full, y_test = y_train_full.flatten(), y_test.flatten()\n",
    "\n",
    "X_train,X_valid,y_train,y_valid = train_test_split(X_train_full,y_train_full, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 22460), started 0:38:55 ago. (Use '!kill 22460' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-8719606d2349de11\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-8719606d2349de11\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "\n",
    "%tensorboard --logdir logs/fit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((37500, 32, 32, 3), (37500,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises from Geron chapter 11\n",
    "Nadam with 20 layers of 100 neurons, He initialization and  Elu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "elu_layer = partial(keras.layers.Dense,kernel_initializer=\"he_normal\", activation=\"elu\")\n",
    "\n",
    "\n",
    "def make_DNN_dropout (layer_function, n_hidden,use_normalization=False, dropout_rate=-1):\n",
    "    architecture = [keras.layers.Flatten(input_shape=[32,32,3])]\n",
    "    for _ in range(n_hidden):\n",
    "        architecture.append(layer_function(100))\n",
    "        if use_normalization:\n",
    "            architecture.append(keras.layers.BatchNormalization())\n",
    "        if dropout_rate > -1:\n",
    "            architecture.append(keras.layers.Dropout(dropout_rate))\n",
    "    architecture.append(keras.layers.Dense(10,activation=\"softmax\"))\n",
    "    return(keras.models.Sequential(architecture))\n",
    "\n",
    "def dropout_model():\n",
    "    return make_DNN_dropout(elu_layer,20,False,0.2)\n",
    "\n",
    "#First run used the values that worked well for he & elu without dropout - performed badly -  best validn accuracy was 22.34%\n",
    "#lrs=[0.001, 0.0005, 0.00025, 0.0001, 0.00005] \n",
    "#Tried 0.1 and 0.05 and  but both failed because of instability causing tensorboard: \"InvalidArgumentError: Nan in summary histogram for\"\n",
    "#lrs=[0.01, 0.025, 0.05, 0.075] #0.01 => 9.75% 0.025 failed in tensorboard\n",
    "# lrs=[0.05, 0.075]\n",
    "\n",
    "# valid_accuracy,test_accuracy,logs = tune_lrs(dropout_model, lrs, \"dropout\")\n",
    "\n",
    "# print(f\"validation accuracy: {valid_accuracy}\")\n",
    "# print(f\"test accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_lrs(model_function, lrs, prefix):\n",
    "\n",
    "    import datetime\n",
    "    import numpy as np\n",
    "\n",
    "    valid_accuracy=[]\n",
    "    test_accuracy=[]\n",
    "    logs=[]\n",
    "    best_models=[]\n",
    "    for lr in lrs:\n",
    "        model = model_function()\n",
    "\n",
    "        log_dir = \"logs/fit/\" + f\"{prefix}_{lr}_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "        checkpoint_file = \"models/\" + f\"{prefix}_{lr}_model.h5\"\n",
    "        tensorboard_cb = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "        #since y_train is numeric rarther than one-hot need to use \"SPARSE_categorical_cross_entropy\"\n",
    "        #model.compile(optimizer='nadam', loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "        model.compile(optimizer = keras.optimizers.Nadam(learning_rate=lr) , loss='sparse_categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "        logs.append(log_dir)\n",
    "\n",
    "\n",
    "        print(f\"fitting lr = {lr} Logged in: {log_dir} saved in {checkpoint_file}\")\n",
    "\n",
    "        model_checkpoint_cb = keras.callbacks.ModelCheckpoint(checkpoint_file, save_best_only=True)\n",
    "\n",
    "        model.fit(X_train,y_train, epochs=100, validation_data=(X_valid, y_valid), \n",
    "            callbacks=[keras.callbacks.EarlyStopping(patience=10), model_checkpoint_cb, tensorboard_cb])\n",
    "        \n",
    "        best_models.append(checkpoint_file)\n",
    "\n",
    "        valid_accuracy.append(model.evaluate(X_valid,y_valid)[1])\n",
    "        test_accuracy.append(model.evaluate(X_test,y_test)[1])\n",
    "\n",
    "        \n",
    "    print(f\"Best validation performance: {np.max(valid_accuracy):.2%} for lr: {lrs[np.argmax(valid_accuracy)]}\")\n",
    "    print(f\"Best test performance: {np.max(test_accuracy):.2%} for lr: {lrs[np.argmax(test_accuracy)]}\")\n",
    "    return valid_accuracy,test_accuracy,logs, best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting lr = 0.001 Logged in: logs/fit/dropout_0.001_20220704-131800 saved in models/dropout_0.001_model.h5\n",
      "Epoch 1/100\n",
      "   1/1172 [..............................] - ETA: 0s - loss: 9.8958 - accuracy: 0.1562WARNING:tensorflow:From c:\\Users\\micha\\anaconda3\\envs\\PandasNumpyMathplotlib\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "   2/1172 [..............................] - ETA: 43s - loss: 9.5808 - accuracy: 0.1562WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0080s vs `on_train_batch_end` time: 0.0639s). Check your callbacks.\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.5053 - accuracy: 0.1036 - val_loss: 2.3015 - val_accuracy: 0.1087\n",
      "Epoch 2/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.1998 - accuracy: 0.1558 - val_loss: 2.1789 - val_accuracy: 0.1737\n",
      "Epoch 3/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 2.1067 - accuracy: 0.1712 - val_loss: 2.0442 - val_accuracy: 0.1994\n",
      "Epoch 4/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 2.0758 - accuracy: 0.1795 - val_loss: 2.0161 - val_accuracy: 0.1957\n",
      "Epoch 5/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 2.0430 - accuracy: 0.1956 - val_loss: 1.9994 - val_accuracy: 0.2162\n",
      "Epoch 6/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 2.0245 - accuracy: 0.2095 - val_loss: 1.9814 - val_accuracy: 0.2313\n",
      "Epoch 7/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 2.0115 - accuracy: 0.2184 - val_loss: 2.1274 - val_accuracy: 0.1995\n",
      "Epoch 8/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 2.0040 - accuracy: 0.2259 - val_loss: 1.9451 - val_accuracy: 0.2618\n",
      "Epoch 9/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 2.0198 - accuracy: 0.2245 - val_loss: 1.9587 - val_accuracy: 0.2271\n",
      "Epoch 10/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.9871 - accuracy: 0.2357 - val_loss: 1.9729 - val_accuracy: 0.2479\n",
      "Epoch 11/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.9940 - accuracy: 0.2307 - val_loss: 1.9307 - val_accuracy: 0.2468\n",
      "Epoch 12/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 2.0321 - accuracy: 0.2129 - val_loss: 1.9698 - val_accuracy: 0.2361\n",
      "Epoch 13/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 2.0517 - accuracy: 0.2094 - val_loss: 1.9826 - val_accuracy: 0.2168\n",
      "Epoch 14/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 2.0542 - accuracy: 0.2104 - val_loss: 1.9663 - val_accuracy: 0.2554\n",
      "Epoch 15/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.9924 - accuracy: 0.2309 - val_loss: 2.0510 - val_accuracy: 0.2095\n",
      "Epoch 16/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 2.0674 - accuracy: 0.1925 - val_loss: 2.0526 - val_accuracy: 0.1832\n",
      "Epoch 17/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 2.0198 - accuracy: 0.2076 - val_loss: 2.0058 - val_accuracy: 0.2254\n",
      "Epoch 18/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 2.0016 - accuracy: 0.2158 - val_loss: 2.0155 - val_accuracy: 0.2119\n",
      "Epoch 19/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.9981 - accuracy: 0.2145 - val_loss: 2.0075 - val_accuracy: 0.2018\n",
      "Epoch 20/100\n",
      "1172/1172 [==============================] - 9s 8ms/step - loss: 2.0110 - accuracy: 0.2084 - val_loss: 1.9970 - val_accuracy: 0.2272\n",
      "Epoch 21/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.9766 - accuracy: 0.2244 - val_loss: 1.9872 - val_accuracy: 0.2041\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 1.9872 - accuracy: 0.2041\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 1.9799 - accuracy: 0.1998: 0s - loss: 1.9805 - accuracy: \n",
      "fitting lr = 0.0005 Logged in: logs/fit/dropout_0.0005_20220704-132134 saved in models/dropout_0.0005_model.h5\n",
      "Epoch 1/100\n",
      "   2/1172 [..............................] - ETA: 20:51 - loss: 8.7521 - accuracy: 0.1094WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0070s vs `on_train_batch_end` time: 2.1319s). Check your callbacks.\n",
      "1172/1172 [==============================] - 12s 11ms/step - loss: 2.5902 - accuracy: 0.0997 - val_loss: 2.3180 - val_accuracy: 0.0951\n",
      "Epoch 2/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 2.2867 - accuracy: 0.1289 - val_loss: 2.0748 - val_accuracy: 0.1902\n",
      "Epoch 3/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 2.1424 - accuracy: 0.1669 - val_loss: 2.1437 - val_accuracy: 0.1727\n",
      "Epoch 4/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 2.0994 - accuracy: 0.1776 - val_loss: 2.0262 - val_accuracy: 0.2131\n",
      "Epoch 5/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 2.0540 - accuracy: 0.1984 - val_loss: 2.0334 - val_accuracy: 0.2058\n",
      "Epoch 6/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 2.0213 - accuracy: 0.2129 - val_loss: 1.9593 - val_accuracy: 0.2391\n",
      "Epoch 7/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.9859 - accuracy: 0.2323 - val_loss: 1.9601 - val_accuracy: 0.2305\n",
      "Epoch 8/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.9645 - accuracy: 0.2429 - val_loss: 1.9430 - val_accuracy: 0.2442\n",
      "Epoch 9/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.9585 - accuracy: 0.2492 - val_loss: 1.9092 - val_accuracy: 0.2434\n",
      "Epoch 10/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.9470 - accuracy: 0.2578 - val_loss: 1.8700 - val_accuracy: 0.2813\n",
      "Epoch 11/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.9065 - accuracy: 0.2678 - val_loss: 1.8265 - val_accuracy: 0.3026\n",
      "Epoch 12/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.8840 - accuracy: 0.2812 - val_loss: 1.8491 - val_accuracy: 0.3020\n",
      "Epoch 13/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.8671 - accuracy: 0.2873 - val_loss: 1.8793 - val_accuracy: 0.2674\n",
      "Epoch 14/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.8594 - accuracy: 0.2957 - val_loss: 1.8114 - val_accuracy: 0.3089\n",
      "Epoch 15/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.8415 - accuracy: 0.3063 - val_loss: 1.8253 - val_accuracy: 0.3118\n",
      "Epoch 16/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.8300 - accuracy: 0.3089 - val_loss: 1.8060 - val_accuracy: 0.3270\n",
      "Epoch 17/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.8260 - accuracy: 0.3174 - val_loss: 1.7827 - val_accuracy: 0.3468\n",
      "Epoch 18/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.8129 - accuracy: 0.3239 - val_loss: 1.7645 - val_accuracy: 0.3520\n",
      "Epoch 19/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.8094 - accuracy: 0.3312 - val_loss: 1.8075 - val_accuracy: 0.3091\n",
      "Epoch 20/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.7975 - accuracy: 0.3387 - val_loss: 1.7831 - val_accuracy: 0.3486\n",
      "Epoch 21/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.8041 - accuracy: 0.3338 - val_loss: 1.7400 - val_accuracy: 0.3591\n",
      "Epoch 22/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.7833 - accuracy: 0.3439 - val_loss: 1.7629 - val_accuracy: 0.3608\n",
      "Epoch 23/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.7774 - accuracy: 0.3505 - val_loss: 1.7770 - val_accuracy: 0.3525\n",
      "Epoch 24/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.7743 - accuracy: 0.3533 - val_loss: 1.7374 - val_accuracy: 0.3763\n",
      "Epoch 25/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.7606 - accuracy: 0.3599 - val_loss: 1.7221 - val_accuracy: 0.3719\n",
      "Epoch 26/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.7680 - accuracy: 0.3540 - val_loss: 1.7272 - val_accuracy: 0.3698\n",
      "Epoch 27/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.7667 - accuracy: 0.3561 - val_loss: 1.7400 - val_accuracy: 0.3691\n",
      "Epoch 28/100\n",
      "1172/1172 [==============================] - 9s 8ms/step - loss: 1.7521 - accuracy: 0.3671 - val_loss: 1.7293 - val_accuracy: 0.3774\n",
      "Epoch 29/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.7325 - accuracy: 0.3682 - val_loss: 1.6998 - val_accuracy: 0.3758\n",
      "Epoch 30/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.7286 - accuracy: 0.3777 - val_loss: 1.6933 - val_accuracy: 0.3864\n",
      "Epoch 31/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.7299 - accuracy: 0.3737 - val_loss: 1.7038 - val_accuracy: 0.3823\n",
      "Epoch 32/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.7230 - accuracy: 0.3784 - val_loss: 1.7086 - val_accuracy: 0.3878\n",
      "Epoch 33/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.7169 - accuracy: 0.3829 - val_loss: 1.7093 - val_accuracy: 0.3884\n",
      "Epoch 34/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.7206 - accuracy: 0.3839 - val_loss: 1.6791 - val_accuracy: 0.3902\n",
      "Epoch 35/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.7130 - accuracy: 0.3833 - val_loss: 1.7291 - val_accuracy: 0.3792\n",
      "Epoch 36/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.7087 - accuracy: 0.3874 - val_loss: 1.6976 - val_accuracy: 0.3845\n",
      "Epoch 37/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.7068 - accuracy: 0.3854 - val_loss: 1.7088 - val_accuracy: 0.3886\n",
      "Epoch 38/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.7076 - accuracy: 0.3857 - val_loss: 1.7413 - val_accuracy: 0.3707\n",
      "Epoch 39/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.7034 - accuracy: 0.3893 - val_loss: 1.7310 - val_accuracy: 0.3772\n",
      "Epoch 40/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.6909 - accuracy: 0.3905 - val_loss: 1.6833 - val_accuracy: 0.3978\n",
      "Epoch 41/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6908 - accuracy: 0.3884 - val_loss: 1.7029 - val_accuracy: 0.3878\n",
      "Epoch 42/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.7029 - accuracy: 0.3842 - val_loss: 1.6424 - val_accuracy: 0.4107\n",
      "Epoch 43/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.7019 - accuracy: 0.3855 - val_loss: 1.6717 - val_accuracy: 0.3906\n",
      "Epoch 44/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6916 - accuracy: 0.3921 - val_loss: 1.6790 - val_accuracy: 0.3910\n",
      "Epoch 45/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6971 - accuracy: 0.3893 - val_loss: 1.6684 - val_accuracy: 0.4038\n",
      "Epoch 46/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6912 - accuracy: 0.3965 - val_loss: 1.6644 - val_accuracy: 0.4124\n",
      "Epoch 47/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6821 - accuracy: 0.3978 - val_loss: 1.6508 - val_accuracy: 0.4079\n",
      "Epoch 48/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6730 - accuracy: 0.4014 - val_loss: 1.6830 - val_accuracy: 0.3935\n",
      "Epoch 49/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6764 - accuracy: 0.4017 - val_loss: 1.6801 - val_accuracy: 0.4008\n",
      "Epoch 50/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.7505 - accuracy: 0.3702 - val_loss: 1.6552 - val_accuracy: 0.4124\n",
      "Epoch 51/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6691 - accuracy: 0.4028 - val_loss: 1.6539 - val_accuracy: 0.4154\n",
      "Epoch 52/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6789 - accuracy: 0.3990 - val_loss: 1.6393 - val_accuracy: 0.4172\n",
      "Epoch 53/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6600 - accuracy: 0.4052 - val_loss: 1.6395 - val_accuracy: 0.4121\n",
      "Epoch 54/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6519 - accuracy: 0.4099 - val_loss: 1.6396 - val_accuracy: 0.4183\n",
      "Epoch 55/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6765 - accuracy: 0.3994 - val_loss: 1.6880 - val_accuracy: 0.4026\n",
      "Epoch 56/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6750 - accuracy: 0.4022 - val_loss: 1.6287 - val_accuracy: 0.4194\n",
      "Epoch 57/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6501 - accuracy: 0.4080 - val_loss: 1.6270 - val_accuracy: 0.4210\n",
      "Epoch 58/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6702 - accuracy: 0.4032 - val_loss: 1.6080 - val_accuracy: 0.4210\n",
      "Epoch 59/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6599 - accuracy: 0.4073 - val_loss: 1.6301 - val_accuracy: 0.4219\n",
      "Epoch 60/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6562 - accuracy: 0.4059 - val_loss: 1.6636 - val_accuracy: 0.4083\n",
      "Epoch 61/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6547 - accuracy: 0.4051 - val_loss: 1.6182 - val_accuracy: 0.4175\n",
      "Epoch 62/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6384 - accuracy: 0.4180 - val_loss: 1.6423 - val_accuracy: 0.4213\n",
      "Epoch 63/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.7632 - accuracy: 0.3797 - val_loss: 1.7724 - val_accuracy: 0.3513\n",
      "Epoch 64/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6994 - accuracy: 0.3933 - val_loss: 1.6561 - val_accuracy: 0.4143\n",
      "Epoch 65/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6587 - accuracy: 0.4080 - val_loss: 1.6631 - val_accuracy: 0.4030\n",
      "Epoch 66/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6482 - accuracy: 0.4141 - val_loss: 1.6439 - val_accuracy: 0.4166\n",
      "Epoch 67/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6438 - accuracy: 0.4180 - val_loss: 1.6248 - val_accuracy: 0.4284\n",
      "Epoch 68/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6294 - accuracy: 0.4193 - val_loss: 1.6454 - val_accuracy: 0.4183\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 1.6454 - accuracy: 0.4183\n",
      "313/313 [==============================] - 0s 2ms/step - loss: 1.6433 - accuracy: 0.4243\n",
      "fitting lr = 0.00025 Logged in: logs/fit/dropout_0.00025_20220704-133244 saved in models/dropout_0.00025_model.h5\n",
      "Epoch 1/100\n",
      "   2/1172 [..............................] - ETA: 22:59 - loss: 6.0826 - accuracy: 0.1562WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0090s vs `on_train_batch_end` time: 2.3481s). Check your callbacks.\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.7080 - accuracy: 0.0995 - val_loss: 2.3048 - val_accuracy: 0.1185\n",
      "Epoch 2/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.3683 - accuracy: 0.1017 - val_loss: 2.3022 - val_accuracy: 0.1190\n",
      "Epoch 3/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 2.3066 - accuracy: 0.1210 - val_loss: 2.1286 - val_accuracy: 0.1630\n",
      "Epoch 4/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.1530 - accuracy: 0.1667 - val_loss: 2.1093 - val_accuracy: 0.1807\n",
      "Epoch 5/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 2.1028 - accuracy: 0.1774 - val_loss: 2.0367 - val_accuracy: 0.1863\n",
      "Epoch 6/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 2.0672 - accuracy: 0.1845 - val_loss: 2.0660 - val_accuracy: 0.1796\n",
      "Epoch 7/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 2.0383 - accuracy: 0.2018 - val_loss: 1.9671 - val_accuracy: 0.2346\n",
      "Epoch 8/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 2.0098 - accuracy: 0.2125 - val_loss: 2.0361 - val_accuracy: 0.2136\n",
      "Epoch 9/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.9902 - accuracy: 0.2244 - val_loss: 1.9467 - val_accuracy: 0.2426\n",
      "Epoch 10/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.9641 - accuracy: 0.2411 - val_loss: 1.9199 - val_accuracy: 0.2443\n",
      "Epoch 11/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.9429 - accuracy: 0.2527 - val_loss: 1.8958 - val_accuracy: 0.2688\n",
      "Epoch 12/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.9156 - accuracy: 0.2610 - val_loss: 1.8601 - val_accuracy: 0.2751\n",
      "Epoch 13/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.8901 - accuracy: 0.2712 - val_loss: 1.8397 - val_accuracy: 0.2755\n",
      "Epoch 14/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.8698 - accuracy: 0.2811 - val_loss: 1.8136 - val_accuracy: 0.2914\n",
      "Epoch 15/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.8541 - accuracy: 0.2918 - val_loss: 1.8323 - val_accuracy: 0.3054\n",
      "Epoch 16/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.8404 - accuracy: 0.3003 - val_loss: 1.8092 - val_accuracy: 0.3013\n",
      "Epoch 17/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.8192 - accuracy: 0.3100 - val_loss: 1.7749 - val_accuracy: 0.3236\n",
      "Epoch 18/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 1.7996 - accuracy: 0.3221 - val_loss: 1.7661 - val_accuracy: 0.3466\n",
      "Epoch 19/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.7741 - accuracy: 0.3314 - val_loss: 1.7439 - val_accuracy: 0.3487\n",
      "Epoch 20/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.7588 - accuracy: 0.3431 - val_loss: 1.7069 - val_accuracy: 0.3702\n",
      "Epoch 21/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.7443 - accuracy: 0.3539 - val_loss: 1.7319 - val_accuracy: 0.3578\n",
      "Epoch 22/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.7344 - accuracy: 0.3591 - val_loss: 1.6819 - val_accuracy: 0.3901\n",
      "Epoch 23/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.7192 - accuracy: 0.3695 - val_loss: 1.7011 - val_accuracy: 0.3800\n",
      "Epoch 24/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.7048 - accuracy: 0.3731 - val_loss: 1.6933 - val_accuracy: 0.3915\n",
      "Epoch 25/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6964 - accuracy: 0.3830 - val_loss: 1.6860 - val_accuracy: 0.3856\n",
      "Epoch 26/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6866 - accuracy: 0.3853 - val_loss: 1.6546 - val_accuracy: 0.4012\n",
      "Epoch 27/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6698 - accuracy: 0.3932 - val_loss: 1.6656 - val_accuracy: 0.4008\n",
      "Epoch 28/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6655 - accuracy: 0.3960 - val_loss: 1.6401 - val_accuracy: 0.4155\n",
      "Epoch 29/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6567 - accuracy: 0.4017 - val_loss: 1.6494 - val_accuracy: 0.3978\n",
      "Epoch 30/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6459 - accuracy: 0.4041 - val_loss: 1.6204 - val_accuracy: 0.4187\n",
      "Epoch 31/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6405 - accuracy: 0.4102 - val_loss: 1.6012 - val_accuracy: 0.4246\n",
      "Epoch 32/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6277 - accuracy: 0.4140 - val_loss: 1.6128 - val_accuracy: 0.4172\n",
      "Epoch 33/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6165 - accuracy: 0.4177 - val_loss: 1.6512 - val_accuracy: 0.4107\n",
      "Epoch 34/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6133 - accuracy: 0.4216 - val_loss: 1.6329 - val_accuracy: 0.4222\n",
      "Epoch 35/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.6120 - accuracy: 0.4221 - val_loss: 1.5804 - val_accuracy: 0.4325\n",
      "Epoch 36/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.5961 - accuracy: 0.4254 - val_loss: 1.5871 - val_accuracy: 0.4299\n",
      "Epoch 37/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.5951 - accuracy: 0.4291 - val_loss: 1.5799 - val_accuracy: 0.4345\n",
      "Epoch 38/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.5878 - accuracy: 0.4329 - val_loss: 1.5648 - val_accuracy: 0.4396\n",
      "Epoch 39/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.5765 - accuracy: 0.4346 - val_loss: 1.5582 - val_accuracy: 0.4446\n",
      "Epoch 40/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.5772 - accuracy: 0.4355 - val_loss: 1.5810 - val_accuracy: 0.4348\n",
      "Epoch 41/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.5659 - accuracy: 0.4403 - val_loss: 1.5760 - val_accuracy: 0.4298\n",
      "Epoch 42/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.5607 - accuracy: 0.4425 - val_loss: 1.5844 - val_accuracy: 0.4350\n",
      "Epoch 43/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.5544 - accuracy: 0.4479 - val_loss: 1.5613 - val_accuracy: 0.4482\n",
      "Epoch 44/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.5463 - accuracy: 0.4467 - val_loss: 1.5511 - val_accuracy: 0.4428\n",
      "Epoch 45/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.5388 - accuracy: 0.4516 - val_loss: 1.5416 - val_accuracy: 0.4516\n",
      "Epoch 46/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.5310 - accuracy: 0.4534 - val_loss: 1.5348 - val_accuracy: 0.4554\n",
      "Epoch 47/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.5311 - accuracy: 0.4549 - val_loss: 1.5325 - val_accuracy: 0.4584\n",
      "Epoch 48/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.5276 - accuracy: 0.4565 - val_loss: 1.5229 - val_accuracy: 0.4602\n",
      "Epoch 49/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.5201 - accuracy: 0.4585 - val_loss: 1.5716 - val_accuracy: 0.4438\n",
      "Epoch 50/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.5125 - accuracy: 0.4629 - val_loss: 1.5468 - val_accuracy: 0.4550\n",
      "Epoch 51/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.5155 - accuracy: 0.4608 - val_loss: 1.5294 - val_accuracy: 0.4605\n",
      "Epoch 52/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.5029 - accuracy: 0.4651 - val_loss: 1.5229 - val_accuracy: 0.4631\n",
      "Epoch 53/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.5016 - accuracy: 0.4640 - val_loss: 1.5283 - val_accuracy: 0.4634\n",
      "Epoch 54/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.4940 - accuracy: 0.4672 - val_loss: 1.5167 - val_accuracy: 0.4618\n",
      "Epoch 55/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.4890 - accuracy: 0.4710 - val_loss: 1.5131 - val_accuracy: 0.4646\n",
      "Epoch 56/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.4855 - accuracy: 0.4706 - val_loss: 1.5034 - val_accuracy: 0.4704\n",
      "Epoch 57/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.4754 - accuracy: 0.4791 - val_loss: 1.5184 - val_accuracy: 0.4630\n",
      "Epoch 58/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.4765 - accuracy: 0.4783 - val_loss: 1.5146 - val_accuracy: 0.4693\n",
      "Epoch 59/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.4686 - accuracy: 0.4798 - val_loss: 1.4960 - val_accuracy: 0.4689\n",
      "Epoch 60/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.4635 - accuracy: 0.4797 - val_loss: 1.5075 - val_accuracy: 0.4645\n",
      "Epoch 61/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.4688 - accuracy: 0.4777 - val_loss: 1.4982 - val_accuracy: 0.4750\n",
      "Epoch 62/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.4599 - accuracy: 0.4835 - val_loss: 1.5036 - val_accuracy: 0.4712\n",
      "Epoch 63/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.4534 - accuracy: 0.4857 - val_loss: 1.4945 - val_accuracy: 0.4773\n",
      "Epoch 64/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.4549 - accuracy: 0.4842 - val_loss: 1.5398 - val_accuracy: 0.4642\n",
      "Epoch 65/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.4463 - accuracy: 0.4887 - val_loss: 1.4972 - val_accuracy: 0.4758\n",
      "Epoch 66/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.4454 - accuracy: 0.4904 - val_loss: 1.5371 - val_accuracy: 0.4598\n",
      "Epoch 67/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.4513 - accuracy: 0.4883 - val_loss: 1.5277 - val_accuracy: 0.4609\n",
      "Epoch 68/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.4443 - accuracy: 0.4896 - val_loss: 1.4840 - val_accuracy: 0.4819\n",
      "Epoch 69/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.4357 - accuracy: 0.4947 - val_loss: 1.5179 - val_accuracy: 0.4739\n",
      "Epoch 70/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.4286 - accuracy: 0.4967 - val_loss: 1.4906 - val_accuracy: 0.4837\n",
      "Epoch 71/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.4320 - accuracy: 0.4933 - val_loss: 1.5108 - val_accuracy: 0.4808\n",
      "Epoch 72/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.4169 - accuracy: 0.5019 - val_loss: 1.5131 - val_accuracy: 0.4722\n",
      "Epoch 73/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.4225 - accuracy: 0.4961 - val_loss: 1.4895 - val_accuracy: 0.4765\n",
      "Epoch 74/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.4168 - accuracy: 0.4987 - val_loss: 1.5195 - val_accuracy: 0.4760\n",
      "Epoch 75/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.4094 - accuracy: 0.5040 - val_loss: 1.5213 - val_accuracy: 0.4688\n",
      "Epoch 76/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.4074 - accuracy: 0.5024 - val_loss: 1.4790 - val_accuracy: 0.4778\n",
      "Epoch 77/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.4081 - accuracy: 0.5032 - val_loss: 1.4993 - val_accuracy: 0.4797\n",
      "Epoch 78/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.3985 - accuracy: 0.5072 - val_loss: 1.4848 - val_accuracy: 0.4813\n",
      "Epoch 79/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.3915 - accuracy: 0.5106 - val_loss: 1.4874 - val_accuracy: 0.4838\n",
      "Epoch 80/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.3953 - accuracy: 0.5075 - val_loss: 1.4715 - val_accuracy: 0.4812\n",
      "Epoch 81/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.3912 - accuracy: 0.5099 - val_loss: 1.4869 - val_accuracy: 0.4767\n",
      "Epoch 82/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.3840 - accuracy: 0.5120 - val_loss: 1.4669 - val_accuracy: 0.4877\n",
      "Epoch 83/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.3799 - accuracy: 0.5123 - val_loss: 1.4833 - val_accuracy: 0.4826\n",
      "Epoch 84/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.3859 - accuracy: 0.5122 - val_loss: 1.4767 - val_accuracy: 0.4878\n",
      "Epoch 85/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.3846 - accuracy: 0.5114 - val_loss: 1.4720 - val_accuracy: 0.4863\n",
      "Epoch 86/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.3742 - accuracy: 0.5130 - val_loss: 1.4984 - val_accuracy: 0.4788\n",
      "Epoch 87/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.3758 - accuracy: 0.5119 - val_loss: 1.4766 - val_accuracy: 0.4814\n",
      "Epoch 88/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.3757 - accuracy: 0.5157 - val_loss: 1.4773 - val_accuracy: 0.4849\n",
      "Epoch 89/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.3737 - accuracy: 0.5158 - val_loss: 1.4646 - val_accuracy: 0.4922\n",
      "Epoch 90/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.3657 - accuracy: 0.5198 - val_loss: 1.4529 - val_accuracy: 0.4961\n",
      "Epoch 91/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.3533 - accuracy: 0.5250 - val_loss: 1.4933 - val_accuracy: 0.4886\n",
      "Epoch 92/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.3541 - accuracy: 0.5242 - val_loss: 1.4891 - val_accuracy: 0.4893\n",
      "Epoch 93/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.3576 - accuracy: 0.5212 - val_loss: 1.4657 - val_accuracy: 0.4918\n",
      "Epoch 94/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.3477 - accuracy: 0.5262 - val_loss: 1.4858 - val_accuracy: 0.4769\n",
      "Epoch 95/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.3530 - accuracy: 0.5239 - val_loss: 1.4682 - val_accuracy: 0.4960\n",
      "Epoch 96/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.3544 - accuracy: 0.5249 - val_loss: 1.5114 - val_accuracy: 0.4782\n",
      "Epoch 97/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.3440 - accuracy: 0.5301 - val_loss: 1.4646 - val_accuracy: 0.4925\n",
      "Epoch 98/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.3356 - accuracy: 0.5321 - val_loss: 1.4564 - val_accuracy: 0.4994\n",
      "Epoch 99/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.3295 - accuracy: 0.5334 - val_loss: 1.4658 - val_accuracy: 0.4897\n",
      "Epoch 100/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.3330 - accuracy: 0.5315 - val_loss: 1.4747 - val_accuracy: 0.4854\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 1.4747 - accuracy: 0.4854\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 1.4580 - accuracy: 0.4877\n",
      "fitting lr = 0.0001 Logged in: logs/fit/dropout_0.0001_20220704-134918 saved in models/dropout_0.0001_model.h5\n",
      "Epoch 1/100\n",
      "   2/1172 [..............................] - ETA: 21:16 - loss: 8.6902 - accuracy: 0.0312WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0080s vs `on_train_batch_end` time: 2.1746s). Check your callbacks.\n",
      "1172/1172 [==============================] - 12s 11ms/step - loss: 3.1010 - accuracy: 0.1007 - val_loss: 2.3286 - val_accuracy: 0.0976\n",
      "Epoch 2/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 2.4625 - accuracy: 0.0998 - val_loss: 2.3028 - val_accuracy: 0.1011\n",
      "Epoch 3/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 2.3890 - accuracy: 0.1022 - val_loss: 2.3032 - val_accuracy: 0.1006\n",
      "Epoch 4/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 2.3635 - accuracy: 0.1047 - val_loss: 2.2452 - val_accuracy: 0.1330\n",
      "Epoch 5/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 2.2400 - accuracy: 0.1472 - val_loss: 2.1057 - val_accuracy: 0.1622\n",
      "Epoch 6/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 2.1649 - accuracy: 0.1616 - val_loss: 2.0758 - val_accuracy: 0.1879\n",
      "Epoch 7/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 2.1354 - accuracy: 0.1675 - val_loss: 2.0530 - val_accuracy: 0.1942\n",
      "Epoch 8/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 2.0949 - accuracy: 0.1805 - val_loss: 2.0392 - val_accuracy: 0.1824\n",
      "Epoch 9/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 2.0658 - accuracy: 0.1914 - val_loss: 2.0068 - val_accuracy: 0.1966\n",
      "Epoch 10/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 2.0453 - accuracy: 0.2024 - val_loss: 1.9887 - val_accuracy: 0.2234\n",
      "Epoch 11/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 2.0214 - accuracy: 0.2154 - val_loss: 1.9818 - val_accuracy: 0.2244\n",
      "Epoch 12/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 2.0007 - accuracy: 0.2237 - val_loss: 1.9640 - val_accuracy: 0.2360\n",
      "Epoch 13/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.9740 - accuracy: 0.2361 - val_loss: 1.9111 - val_accuracy: 0.2582\n",
      "Epoch 14/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.9461 - accuracy: 0.2455 - val_loss: 1.8612 - val_accuracy: 0.2860\n",
      "Epoch 15/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.9204 - accuracy: 0.2606 - val_loss: 1.8242 - val_accuracy: 0.2918\n",
      "Epoch 16/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.9027 - accuracy: 0.2642 - val_loss: 1.8417 - val_accuracy: 0.2894\n",
      "Epoch 17/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.8872 - accuracy: 0.2742 - val_loss: 1.8029 - val_accuracy: 0.3065\n",
      "Epoch 18/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.8695 - accuracy: 0.2746 - val_loss: 1.8000 - val_accuracy: 0.2997\n",
      "Epoch 19/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.8525 - accuracy: 0.2869 - val_loss: 1.7940 - val_accuracy: 0.3211\n",
      "Epoch 20/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.8364 - accuracy: 0.2993 - val_loss: 1.7630 - val_accuracy: 0.3406\n",
      "Epoch 21/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.8196 - accuracy: 0.3089 - val_loss: 1.8262 - val_accuracy: 0.3286\n",
      "Epoch 22/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.8092 - accuracy: 0.3187 - val_loss: 1.7457 - val_accuracy: 0.3568\n",
      "Epoch 23/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.7874 - accuracy: 0.3309 - val_loss: 1.7196 - val_accuracy: 0.3725\n",
      "Epoch 24/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.7726 - accuracy: 0.3413 - val_loss: 1.6924 - val_accuracy: 0.3734\n",
      "Epoch 25/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.7563 - accuracy: 0.3501 - val_loss: 1.6845 - val_accuracy: 0.3890\n",
      "Epoch 26/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.7472 - accuracy: 0.3589 - val_loss: 1.6791 - val_accuracy: 0.3839\n",
      "Epoch 27/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.7358 - accuracy: 0.3591 - val_loss: 1.6671 - val_accuracy: 0.3881\n",
      "Epoch 28/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.7210 - accuracy: 0.3648 - val_loss: 1.6547 - val_accuracy: 0.4031\n",
      "Epoch 29/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.7085 - accuracy: 0.3784 - val_loss: 1.6502 - val_accuracy: 0.3930\n",
      "Epoch 30/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.7040 - accuracy: 0.3795 - val_loss: 1.6360 - val_accuracy: 0.4073\n",
      "Epoch 31/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6859 - accuracy: 0.3870 - val_loss: 1.6372 - val_accuracy: 0.4034\n",
      "Epoch 32/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6798 - accuracy: 0.3903 - val_loss: 1.6219 - val_accuracy: 0.4172\n",
      "Epoch 33/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6680 - accuracy: 0.3942 - val_loss: 1.6251 - val_accuracy: 0.4116\n",
      "Epoch 34/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.6624 - accuracy: 0.3973 - val_loss: 1.6168 - val_accuracy: 0.4229\n",
      "Epoch 35/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.6603 - accuracy: 0.3975 - val_loss: 1.6474 - val_accuracy: 0.4158\n",
      "Epoch 36/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6480 - accuracy: 0.4042 - val_loss: 1.6463 - val_accuracy: 0.4039\n",
      "Epoch 37/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.6316 - accuracy: 0.4125 - val_loss: 1.5941 - val_accuracy: 0.4261\n",
      "Epoch 38/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.6204 - accuracy: 0.4145 - val_loss: 1.5793 - val_accuracy: 0.4364\n",
      "Epoch 39/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6227 - accuracy: 0.4153 - val_loss: 1.5808 - val_accuracy: 0.4377\n",
      "Epoch 40/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 1.6139 - accuracy: 0.4186 - val_loss: 1.5632 - val_accuracy: 0.4407\n",
      "Epoch 41/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.6082 - accuracy: 0.4245 - val_loss: 1.5622 - val_accuracy: 0.4429\n",
      "Epoch 42/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.5992 - accuracy: 0.4244 - val_loss: 1.5521 - val_accuracy: 0.4420\n",
      "Epoch 43/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.5952 - accuracy: 0.4262 - val_loss: 1.6118 - val_accuracy: 0.4266\n",
      "Epoch 44/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.5815 - accuracy: 0.4331 - val_loss: 1.5599 - val_accuracy: 0.4457\n",
      "Epoch 45/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.5793 - accuracy: 0.4320 - val_loss: 1.5450 - val_accuracy: 0.4505\n",
      "Epoch 46/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.5714 - accuracy: 0.4370 - val_loss: 1.5455 - val_accuracy: 0.4485\n",
      "Epoch 47/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.5678 - accuracy: 0.4407 - val_loss: 1.5560 - val_accuracy: 0.4526\n",
      "Epoch 48/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.5597 - accuracy: 0.4452 - val_loss: 1.5521 - val_accuracy: 0.4560\n",
      "Epoch 49/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.5539 - accuracy: 0.4446 - val_loss: 1.5360 - val_accuracy: 0.4578\n",
      "Epoch 50/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.5458 - accuracy: 0.4493 - val_loss: 1.5670 - val_accuracy: 0.4515\n",
      "Epoch 51/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.5451 - accuracy: 0.4515 - val_loss: 1.5528 - val_accuracy: 0.4506\n",
      "Epoch 52/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.5388 - accuracy: 0.4537 - val_loss: 1.5166 - val_accuracy: 0.4676\n",
      "Epoch 53/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.5290 - accuracy: 0.4579 - val_loss: 1.5420 - val_accuracy: 0.4636\n",
      "Epoch 54/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.5269 - accuracy: 0.4566 - val_loss: 1.5142 - val_accuracy: 0.4686\n",
      "Epoch 55/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.5169 - accuracy: 0.4582 - val_loss: 1.5141 - val_accuracy: 0.4678\n",
      "Epoch 56/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.5134 - accuracy: 0.4603 - val_loss: 1.5501 - val_accuracy: 0.4618\n",
      "Epoch 57/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.5074 - accuracy: 0.4645 - val_loss: 1.5126 - val_accuracy: 0.4646\n",
      "Epoch 58/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.5045 - accuracy: 0.4671 - val_loss: 1.5045 - val_accuracy: 0.4678\n",
      "Epoch 59/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.5046 - accuracy: 0.4667 - val_loss: 1.4930 - val_accuracy: 0.4737\n",
      "Epoch 60/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.4926 - accuracy: 0.4737 - val_loss: 1.5472 - val_accuracy: 0.4574\n",
      "Epoch 61/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.4926 - accuracy: 0.4712 - val_loss: 1.4953 - val_accuracy: 0.4722\n",
      "Epoch 62/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.4879 - accuracy: 0.4738 - val_loss: 1.4894 - val_accuracy: 0.4747\n",
      "Epoch 63/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.4757 - accuracy: 0.4811 - val_loss: 1.4984 - val_accuracy: 0.4735\n",
      "Epoch 64/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.4712 - accuracy: 0.4792 - val_loss: 1.5123 - val_accuracy: 0.4717\n",
      "Epoch 65/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.4678 - accuracy: 0.4828 - val_loss: 1.5317 - val_accuracy: 0.4622\n",
      "Epoch 66/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.4623 - accuracy: 0.4849 - val_loss: 1.4796 - val_accuracy: 0.4777\n",
      "Epoch 67/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.4652 - accuracy: 0.4782 - val_loss: 1.5008 - val_accuracy: 0.4682\n",
      "Epoch 68/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.4587 - accuracy: 0.4834 - val_loss: 1.4863 - val_accuracy: 0.4773\n",
      "Epoch 69/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.4565 - accuracy: 0.4825 - val_loss: 1.5401 - val_accuracy: 0.4638\n",
      "Epoch 70/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.4440 - accuracy: 0.4896 - val_loss: 1.4794 - val_accuracy: 0.4807\n",
      "Epoch 71/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.4443 - accuracy: 0.4903 - val_loss: 1.4944 - val_accuracy: 0.4746\n",
      "Epoch 72/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.4401 - accuracy: 0.4899 - val_loss: 1.4956 - val_accuracy: 0.4818\n",
      "Epoch 73/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.4359 - accuracy: 0.4898 - val_loss: 1.4761 - val_accuracy: 0.4822\n",
      "Epoch 74/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.4336 - accuracy: 0.4949 - val_loss: 1.4847 - val_accuracy: 0.4790\n",
      "Epoch 75/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.4255 - accuracy: 0.4962 - val_loss: 1.4814 - val_accuracy: 0.4857\n",
      "Epoch 76/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.4184 - accuracy: 0.5002 - val_loss: 1.4765 - val_accuracy: 0.4826\n",
      "Epoch 77/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.4207 - accuracy: 0.4997 - val_loss: 1.4623 - val_accuracy: 0.4828\n",
      "Epoch 78/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.4189 - accuracy: 0.5009 - val_loss: 1.4722 - val_accuracy: 0.4825\n",
      "Epoch 79/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.4120 - accuracy: 0.5015 - val_loss: 1.4725 - val_accuracy: 0.4817\n",
      "Epoch 80/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.4093 - accuracy: 0.5012 - val_loss: 1.4594 - val_accuracy: 0.4850\n",
      "Epoch 81/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.4077 - accuracy: 0.5030 - val_loss: 1.4807 - val_accuracy: 0.4867\n",
      "Epoch 82/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.4016 - accuracy: 0.5047 - val_loss: 1.4791 - val_accuracy: 0.4814\n",
      "Epoch 83/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.4035 - accuracy: 0.5045 - val_loss: 1.4485 - val_accuracy: 0.4912\n",
      "Epoch 84/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.3977 - accuracy: 0.5082 - val_loss: 1.4591 - val_accuracy: 0.4890\n",
      "Epoch 85/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.3883 - accuracy: 0.5111 - val_loss: 1.4605 - val_accuracy: 0.4910\n",
      "Epoch 86/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.3836 - accuracy: 0.5115 - val_loss: 1.4730 - val_accuracy: 0.4896\n",
      "Epoch 87/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.3827 - accuracy: 0.5132 - val_loss: 1.4776 - val_accuracy: 0.4885\n",
      "Epoch 88/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.3743 - accuracy: 0.5178 - val_loss: 1.4590 - val_accuracy: 0.4890\n",
      "Epoch 89/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.3854 - accuracy: 0.5148 - val_loss: 1.4605 - val_accuracy: 0.4860\n",
      "Epoch 90/100\n",
      "1172/1172 [==============================] - 9s 8ms/step - loss: 1.3772 - accuracy: 0.5195 - val_loss: 1.4644 - val_accuracy: 0.4909\n",
      "Epoch 91/100\n",
      "1172/1172 [==============================] - 9s 8ms/step - loss: 1.3691 - accuracy: 0.5187 - val_loss: 1.4580 - val_accuracy: 0.4878\n",
      "Epoch 92/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.3696 - accuracy: 0.5163 - val_loss: 1.4465 - val_accuracy: 0.4966\n",
      "Epoch 93/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.3688 - accuracy: 0.5176 - val_loss: 1.4481 - val_accuracy: 0.4969\n",
      "Epoch 94/100\n",
      "1172/1172 [==============================] - 9s 8ms/step - loss: 1.3612 - accuracy: 0.5240 - val_loss: 1.4523 - val_accuracy: 0.4915\n",
      "Epoch 95/100\n",
      "1172/1172 [==============================] - 9s 8ms/step - loss: 1.3641 - accuracy: 0.5195 - val_loss: 1.4315 - val_accuracy: 0.4978\n",
      "Epoch 96/100\n",
      "1172/1172 [==============================] - 9s 8ms/step - loss: 1.3590 - accuracy: 0.5195 - val_loss: 1.4390 - val_accuracy: 0.5034\n",
      "Epoch 97/100\n",
      "1172/1172 [==============================] - 9s 8ms/step - loss: 1.3522 - accuracy: 0.5241 - val_loss: 1.4422 - val_accuracy: 0.4928\n",
      "Epoch 98/100\n",
      "1172/1172 [==============================] - 9s 8ms/step - loss: 1.3529 - accuracy: 0.5226 - val_loss: 1.4505 - val_accuracy: 0.4914\n",
      "Epoch 99/100\n",
      "1172/1172 [==============================] - 9s 8ms/step - loss: 1.3522 - accuracy: 0.5218 - val_loss: 1.4362 - val_accuracy: 0.4981\n",
      "Epoch 100/100\n",
      "1172/1172 [==============================] - 9s 8ms/step - loss: 1.3388 - accuracy: 0.5292 - val_loss: 1.4486 - val_accuracy: 0.4946\n",
      "391/391 [==============================] - 1s 1ms/step - loss: 1.4486 - accuracy: 0.4946\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 1.4403 - accuracy: 0.4966\n",
      "fitting lr = 5e-05 Logged in: logs/fit/dropout_5e-05_20220704-140554 saved in models/dropout_5e-05_model.h5\n",
      "Epoch 1/100\n",
      "   2/1172 [..............................] - ETA: 23:05 - loss: 8.1156 - accuracy: 0.1406WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0070s vs `on_train_batch_end` time: 2.3610s). Check your callbacks.\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 3.3757 - accuracy: 0.1031 - val_loss: 2.3595 - val_accuracy: 0.1097\n",
      "Epoch 2/100\n",
      "1172/1172 [==============================] - 9s 8ms/step - loss: 2.5757 - accuracy: 0.1012 - val_loss: 2.3194 - val_accuracy: 0.1056\n",
      "Epoch 3/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.4498 - accuracy: 0.1005 - val_loss: 2.3141 - val_accuracy: 0.1012\n",
      "Epoch 4/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.4000 - accuracy: 0.0992 - val_loss: 2.3061 - val_accuracy: 0.0995\n",
      "Epoch 5/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.3745 - accuracy: 0.1047 - val_loss: 2.2855 - val_accuracy: 0.1110\n",
      "Epoch 6/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.2725 - accuracy: 0.1414 - val_loss: 2.0964 - val_accuracy: 0.1967\n",
      "Epoch 7/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.1971 - accuracy: 0.1571 - val_loss: 2.0780 - val_accuracy: 0.1698\n",
      "Epoch 8/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 2.1610 - accuracy: 0.1639 - val_loss: 2.0654 - val_accuracy: 0.1821\n",
      "Epoch 9/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 2.1390 - accuracy: 0.1695 - val_loss: 2.0535 - val_accuracy: 0.1780\n",
      "Epoch 10/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 2.1159 - accuracy: 0.1730 - val_loss: 2.0469 - val_accuracy: 0.1818\n",
      "Epoch 11/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 2.0956 - accuracy: 0.1751 - val_loss: 2.0257 - val_accuracy: 0.1916\n",
      "Epoch 12/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 2.0813 - accuracy: 0.1818 - val_loss: 2.0274 - val_accuracy: 0.1906\n",
      "Epoch 13/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 2.0608 - accuracy: 0.1930 - val_loss: 2.0070 - val_accuracy: 0.1948\n",
      "Epoch 14/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.0384 - accuracy: 0.2042 - val_loss: 1.9566 - val_accuracy: 0.2310\n",
      "Epoch 15/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.0197 - accuracy: 0.2125 - val_loss: 1.9472 - val_accuracy: 0.2450\n",
      "Epoch 16/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.0078 - accuracy: 0.2205 - val_loss: 1.9120 - val_accuracy: 0.2461\n",
      "Epoch 17/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 1.9814 - accuracy: 0.2303 - val_loss: 1.8984 - val_accuracy: 0.2599\n",
      "Epoch 18/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 1.9681 - accuracy: 0.2399 - val_loss: 1.8835 - val_accuracy: 0.2746\n",
      "Epoch 19/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.9520 - accuracy: 0.2455 - val_loss: 1.8962 - val_accuracy: 0.2700\n",
      "Epoch 20/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.9369 - accuracy: 0.2495 - val_loss: 1.8893 - val_accuracy: 0.2625\n",
      "Epoch 21/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 1.9259 - accuracy: 0.2542 - val_loss: 1.8670 - val_accuracy: 0.2804\n",
      "Epoch 22/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 1.9182 - accuracy: 0.2564 - val_loss: 1.8450 - val_accuracy: 0.2919\n",
      "Epoch 23/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.9050 - accuracy: 0.2657 - val_loss: 1.8507 - val_accuracy: 0.2895\n",
      "Epoch 24/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.8919 - accuracy: 0.2686 - val_loss: 1.8646 - val_accuracy: 0.2871\n",
      "Epoch 25/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.8802 - accuracy: 0.2721 - val_loss: 1.8253 - val_accuracy: 0.3048\n",
      "Epoch 26/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.8685 - accuracy: 0.2816 - val_loss: 1.8119 - val_accuracy: 0.3194\n",
      "Epoch 27/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.8581 - accuracy: 0.2849 - val_loss: 1.7968 - val_accuracy: 0.3306\n",
      "Epoch 28/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.8516 - accuracy: 0.2930 - val_loss: 1.7721 - val_accuracy: 0.3310\n",
      "Epoch 29/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.8393 - accuracy: 0.2967 - val_loss: 1.7953 - val_accuracy: 0.3296\n",
      "Epoch 30/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.8293 - accuracy: 0.3039 - val_loss: 1.8062 - val_accuracy: 0.3425\n",
      "Epoch 31/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.8188 - accuracy: 0.3094 - val_loss: 1.7563 - val_accuracy: 0.3478\n",
      "Epoch 32/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.8131 - accuracy: 0.3176 - val_loss: 1.7389 - val_accuracy: 0.3618\n",
      "Epoch 33/100\n",
      "1172/1172 [==============================] - 9s 8ms/step - loss: 1.7972 - accuracy: 0.3254 - val_loss: 1.7432 - val_accuracy: 0.3581\n",
      "Epoch 34/100\n",
      "1172/1172 [==============================] - 9s 8ms/step - loss: 1.7923 - accuracy: 0.3278 - val_loss: 1.8225 - val_accuracy: 0.3388\n",
      "Epoch 35/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.7818 - accuracy: 0.3362 - val_loss: 1.7097 - val_accuracy: 0.3731\n",
      "Epoch 36/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.7734 - accuracy: 0.3405 - val_loss: 1.7008 - val_accuracy: 0.3798\n",
      "Epoch 37/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 1.7640 - accuracy: 0.3482 - val_loss: 1.6865 - val_accuracy: 0.3861\n",
      "Epoch 38/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.7582 - accuracy: 0.3500 - val_loss: 1.6799 - val_accuracy: 0.3878\n",
      "Epoch 39/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.7479 - accuracy: 0.3587 - val_loss: 1.6917 - val_accuracy: 0.3870\n",
      "Epoch 40/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.7446 - accuracy: 0.3589 - val_loss: 1.6921 - val_accuracy: 0.3883\n",
      "Epoch 41/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 1.7361 - accuracy: 0.3625 - val_loss: 1.6854 - val_accuracy: 0.3906\n",
      "Epoch 42/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 1.7257 - accuracy: 0.3677 - val_loss: 1.6499 - val_accuracy: 0.3990\n",
      "Epoch 43/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.7251 - accuracy: 0.3715 - val_loss: 1.6740 - val_accuracy: 0.3977\n",
      "Epoch 44/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.7183 - accuracy: 0.3692 - val_loss: 1.6553 - val_accuracy: 0.3955\n",
      "Epoch 45/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.7046 - accuracy: 0.3787 - val_loss: 1.6675 - val_accuracy: 0.4026\n",
      "Epoch 46/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.6994 - accuracy: 0.3785 - val_loss: 1.6306 - val_accuracy: 0.4078\n",
      "Epoch 47/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6983 - accuracy: 0.3810 - val_loss: 1.6436 - val_accuracy: 0.4023\n",
      "Epoch 48/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.6899 - accuracy: 0.3812 - val_loss: 1.6414 - val_accuracy: 0.4101\n",
      "Epoch 49/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.6823 - accuracy: 0.3879 - val_loss: 1.6443 - val_accuracy: 0.4077\n",
      "Epoch 50/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 1.6719 - accuracy: 0.3894 - val_loss: 1.6255 - val_accuracy: 0.4126\n",
      "Epoch 51/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 1.6731 - accuracy: 0.3890 - val_loss: 1.6112 - val_accuracy: 0.4162\n",
      "Epoch 52/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.6705 - accuracy: 0.3924 - val_loss: 1.6084 - val_accuracy: 0.4211\n",
      "Epoch 53/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.6546 - accuracy: 0.3973 - val_loss: 1.6361 - val_accuracy: 0.4128\n",
      "Epoch 54/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.6559 - accuracy: 0.3949 - val_loss: 1.6086 - val_accuracy: 0.4157\n",
      "Epoch 55/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.6486 - accuracy: 0.3983 - val_loss: 1.6009 - val_accuracy: 0.4210\n",
      "Epoch 56/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.6442 - accuracy: 0.4021 - val_loss: 1.5929 - val_accuracy: 0.4234\n",
      "Epoch 57/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.6461 - accuracy: 0.4001 - val_loss: 1.6098 - val_accuracy: 0.4198\n",
      "Epoch 58/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.6350 - accuracy: 0.4076 - val_loss: 1.5917 - val_accuracy: 0.4244\n",
      "Epoch 59/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.6344 - accuracy: 0.4072 - val_loss: 1.5887 - val_accuracy: 0.4311\n",
      "Epoch 60/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6277 - accuracy: 0.4114 - val_loss: 1.5890 - val_accuracy: 0.4309\n",
      "Epoch 61/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.6248 - accuracy: 0.4100 - val_loss: 1.5954 - val_accuracy: 0.4290\n",
      "Epoch 62/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.6127 - accuracy: 0.4172 - val_loss: 1.5902 - val_accuracy: 0.4292\n",
      "Epoch 63/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.6123 - accuracy: 0.4173 - val_loss: 1.5796 - val_accuracy: 0.4288\n",
      "Epoch 64/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.6011 - accuracy: 0.4172 - val_loss: 1.5731 - val_accuracy: 0.4353\n",
      "Epoch 65/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.6024 - accuracy: 0.4223 - val_loss: 1.5676 - val_accuracy: 0.4368\n",
      "Epoch 66/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6033 - accuracy: 0.4207 - val_loss: 1.5571 - val_accuracy: 0.4445\n",
      "Epoch 67/100\n",
      "1172/1172 [==============================] - 9s 8ms/step - loss: 1.5894 - accuracy: 0.4266 - val_loss: 1.5700 - val_accuracy: 0.4369\n",
      "Epoch 68/100\n",
      "1172/1172 [==============================] - 9s 8ms/step - loss: 1.5887 - accuracy: 0.4290 - val_loss: 1.5491 - val_accuracy: 0.4440\n",
      "Epoch 69/100\n",
      "1172/1172 [==============================] - 9s 8ms/step - loss: 1.5878 - accuracy: 0.4275 - val_loss: 1.5633 - val_accuracy: 0.4340\n",
      "Epoch 70/100\n",
      "1172/1172 [==============================] - 9s 8ms/step - loss: 1.5831 - accuracy: 0.4270 - val_loss: 1.5569 - val_accuracy: 0.4388\n",
      "Epoch 71/100\n",
      "1172/1172 [==============================] - 9s 8ms/step - loss: 1.5799 - accuracy: 0.4299 - val_loss: 1.5683 - val_accuracy: 0.4386\n",
      "Epoch 72/100\n",
      "1172/1172 [==============================] - 9s 8ms/step - loss: 1.5720 - accuracy: 0.4346 - val_loss: 1.5469 - val_accuracy: 0.4441\n",
      "Epoch 73/100\n",
      "1172/1172 [==============================] - 9s 8ms/step - loss: 1.5703 - accuracy: 0.4355 - val_loss: 1.5687 - val_accuracy: 0.4413\n",
      "Epoch 74/100\n",
      "1172/1172 [==============================] - 9s 8ms/step - loss: 1.5677 - accuracy: 0.4329 - val_loss: 1.5475 - val_accuracy: 0.4489\n",
      "Epoch 75/100\n",
      "1172/1172 [==============================] - 9s 8ms/step - loss: 1.5623 - accuracy: 0.4368 - val_loss: 1.5622 - val_accuracy: 0.4394\n",
      "Epoch 76/100\n",
      "1172/1172 [==============================] - 9s 8ms/step - loss: 1.5555 - accuracy: 0.4394 - val_loss: 1.5456 - val_accuracy: 0.4467\n",
      "Epoch 77/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 1.5575 - accuracy: 0.4415 - val_loss: 1.5221 - val_accuracy: 0.4510\n",
      "Epoch 78/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.5505 - accuracy: 0.4456 - val_loss: 1.5343 - val_accuracy: 0.4522\n",
      "Epoch 79/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.5498 - accuracy: 0.4428 - val_loss: 1.5355 - val_accuracy: 0.4546\n",
      "Epoch 80/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.5380 - accuracy: 0.4482 - val_loss: 1.5258 - val_accuracy: 0.4551\n",
      "Epoch 81/100\n",
      "1172/1172 [==============================] - 9s 8ms/step - loss: 1.5397 - accuracy: 0.4460 - val_loss: 1.5234 - val_accuracy: 0.4530\n",
      "Epoch 82/100\n",
      "1172/1172 [==============================] - 9s 8ms/step - loss: 1.5364 - accuracy: 0.4490 - val_loss: 1.5257 - val_accuracy: 0.4566\n",
      "Epoch 83/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.5357 - accuracy: 0.4516 - val_loss: 1.5153 - val_accuracy: 0.4581\n",
      "Epoch 84/100\n",
      "1172/1172 [==============================] - 9s 8ms/step - loss: 1.5256 - accuracy: 0.4553 - val_loss: 1.5174 - val_accuracy: 0.4549\n",
      "Epoch 85/100\n",
      "1172/1172 [==============================] - 9s 8ms/step - loss: 1.5272 - accuracy: 0.4558 - val_loss: 1.5257 - val_accuracy: 0.4582\n",
      "Epoch 86/100\n",
      "1172/1172 [==============================] - 9s 8ms/step - loss: 1.5217 - accuracy: 0.4594 - val_loss: 1.5258 - val_accuracy: 0.4536\n",
      "Epoch 87/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.5244 - accuracy: 0.4539 - val_loss: 1.5151 - val_accuracy: 0.4575\n",
      "Epoch 88/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.5136 - accuracy: 0.4579 - val_loss: 1.5180 - val_accuracy: 0.4579\n",
      "Epoch 89/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.5150 - accuracy: 0.4575 - val_loss: 1.5295 - val_accuracy: 0.4610\n",
      "Epoch 90/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.5099 - accuracy: 0.4615 - val_loss: 1.5292 - val_accuracy: 0.4586\n",
      "Epoch 91/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.5017 - accuracy: 0.4653 - val_loss: 1.5124 - val_accuracy: 0.4597\n",
      "Epoch 92/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.5094 - accuracy: 0.4603 - val_loss: 1.4992 - val_accuracy: 0.4664\n",
      "Epoch 93/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 1.5000 - accuracy: 0.4644 - val_loss: 1.5074 - val_accuracy: 0.4636\n",
      "Epoch 94/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.4963 - accuracy: 0.4651 - val_loss: 1.5295 - val_accuracy: 0.4588\n",
      "Epoch 95/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 1.4919 - accuracy: 0.4670 - val_loss: 1.4935 - val_accuracy: 0.4694\n",
      "Epoch 96/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.4948 - accuracy: 0.4660 - val_loss: 1.4999 - val_accuracy: 0.4652\n",
      "Epoch 97/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.4902 - accuracy: 0.4682 - val_loss: 1.5042 - val_accuracy: 0.4670\n",
      "Epoch 98/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.4843 - accuracy: 0.4703 - val_loss: 1.5017 - val_accuracy: 0.4652\n",
      "Epoch 99/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.4823 - accuracy: 0.4727 - val_loss: 1.5020 - val_accuracy: 0.4714\n",
      "Epoch 100/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.4738 - accuracy: 0.4730 - val_loss: 1.4952 - val_accuracy: 0.4688\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 1.4952 - accuracy: 0.4688\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 1.4845 - accuracy: 0.4698\n",
      "Best validation performance: 49.46% for lr: 0.0001\n",
      "Best test performance: 49.66% for lr: 0.0001\n",
      "validation accuracy: [0.2040800005197525, 0.4183200001716614, 0.4853599965572357, 0.4946399927139282, 0.46880000829696655]\n",
      "test accuracy: [0.19979999959468842, 0.4242999851703644, 0.4876999855041504, 0.4966000020503998, 0.4697999954223633]\n",
      "model: models/dropout_0.001_model.h5\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 1.9307 - accuracy: 0.2468\n",
      "validation accuracy: 24.68%\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 1.9178 - accuracy: 0.2556\n",
      "test accuracy 25.56%\n",
      "model: models/dropout_0.0005_model.h5\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 1.6080 - accuracy: 0.4210\n",
      "validation accuracy: 42.10%\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 1.5985 - accuracy: 0.4303\n",
      "test accuracy 43.03%\n",
      "model: models/dropout_0.00025_model.h5\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 1.4529 - accuracy: 0.4961\n",
      "validation accuracy: 49.61%\n",
      "313/313 [==============================] - 0s 2ms/step - loss: 1.4409 - accuracy: 0.4913\n",
      "test accuracy 49.13%\n",
      "model: models/dropout_0.0001_model.h5\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 1.4315 - accuracy: 0.4978\n",
      "validation accuracy: 49.78%\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 1.4230 - accuracy: 0.5012\n",
      "test accuracy 50.12%\n",
      "model: models/dropout_5e-05_model.h5\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 1.4935 - accuracy: 0.4694\n",
      "validation accuracy: 46.94%\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 1.4796 - accuracy: 0.4737\n",
      "test accuracy 47.37%\n",
      "Best test accuracy with 0.1 dropout: 50.12% with lr: 0.0001\n"
     ]
    }
   ],
   "source": [
    "#0.2 dropout rate was terrible so trying with 0.1\n",
    "def dropout_model_new():\n",
    "    return make_DNN_dropout(elu_layer,20,False,0.1)\n",
    "\n",
    "lrs=[0.001, 0.0005, 0.00025, 0.0001, 0.00005] # back to original he values\n",
    "\n",
    "valid_accuracy,test_accuracy,logs, best_models = tune_lrs(dropout_model_new, lrs, \"dropout\")\n",
    "\n",
    "print(f\"validation accuracy: {valid_accuracy}\")\n",
    "print(f\"test accuracy: {test_accuracy}\")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "test_acc = []\n",
    "for _,model_file in enumerate(best_models):\n",
    "    model = dropout_model_new()\n",
    "    model.compile(loss='sparse_categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "    model.load_weights(model_file)\n",
    "    print(f\"model: {model_file}\")\n",
    "    print(f\"validation accuracy: {model.evaluate(X_valid,y_valid)[1]:.2%}\")\n",
    "    test_acc.append(model.evaluate(X_test,y_test)[1])\n",
    "    print(f\"test accuracy {test_acc[-1]:.2%}\")\n",
    "\n",
    "print(f\"Best test accuracy with 0.1 dropout: {np.max(test_acc):.2%} with lr: {lrs[np.argmax(test_acc)]}\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Choice of dropout percentage is critical 20% failed completely, 10% worked reasonably well\n",
    "* Not full tuned because of time but roughly equivalent performance to Nadam with batch normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2556000053882599, 0.4302999973297119, 0.49129998683929443, 0.5012000203132629, 0.47369998693466187]\n",
      "['models/dropout_0.001_model.h5', 'models/dropout_0.0005_model.h5', 'models/dropout_0.00025_model.h5', 'models/dropout_0.0001_model.h5', 'models/dropout_5e-05_model.h5']\n"
     ]
    }
   ],
   "source": [
    "print(test_acc)\n",
    "print(best_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 1.4230 - accuracy: 0.5012\n",
      "test accuracy: 50.12%\n"
     ]
    }
   ],
   "source": [
    "model = dropout_model_new()\n",
    "model.compile(loss='sparse_categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "model.load_weights(\"models/dropout_0.0001_model.h5\")\n",
    "print(f\"test accuracy: {model.evaluate(X_test,y_test)[1]:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5015\n"
     ]
    }
   ],
   "source": [
    "#Monte-carlo dropout on best performing on validation set\n",
    "import numpy as np\n",
    "\n",
    "model = dropout_model_new()\n",
    "model.compile(loss='sparse_categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "model.load_weights(\"models/dropout_0.0001_model.h5\")\n",
    "\n",
    "\n",
    "#This is softmax classification so prediction will be probabilities for the 10 types\n",
    "y_preds = np.stack([model(X_test,training=True) for sample in range(2000)])\n",
    "y_pred = y_preds.mean(axis=0)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "accuracy = np.sum(y_pred == y_test)/len(y_test)\n",
    "print(accuracy)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5015\n"
     ]
    }
   ],
   "source": [
    "y_pred_1 = y_preds.mean(axis=0)\n",
    "y_pred_2 = np.argmax(y_pred_1, axis=1)\n",
    "accuracy = np.sum(y_pred_2 == y_test)/len(y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analysis of y_preds\n",
    "\n",
    "#so I don't accidentally mess up 12 minute of predictions\n",
    "my_copy = y_preds.copy()\n",
    "acc=[]\n",
    "\n",
    "for _ in range(2,2000):\n",
    "    preds = my_copy[:_,:,:]\n",
    "    pred_1 = preds.mean(axis=0)\n",
    "    y_pred = np.argmax(pred_1, axis=1)\n",
    "    accuracy = np.sum(y_pred == y_test)/len(y_test)\n",
    "    acc.append(accuracy)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 1.4796 - accuracy: 0.4737\n",
      "47.37%\n",
      "0.4723\n"
     ]
    }
   ],
   "source": [
    "#Trying on a sub-optimal model\n",
    "import numpy as np\n",
    "\n",
    "model = dropout_model_new()\n",
    "model.compile(loss='sparse_categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "model.load_weights('models/dropout_5e-05_model.h5')\n",
    "print(f\"{model.evaluate(X_test,y_test)[1]:.2%}\")\n",
    "\n",
    "\n",
    "#This is softmax classification so prediction will be probabilities for the 10 types\n",
    "y_preds_sub = np.stack([model(X_test,training=True) for sample in range(2000)])\n",
    "y_pred_sub_2 = y_preds_sub.mean(axis=0)\n",
    "y_pred_sub = np.argmax(y_pred_sub_2, axis=1)\n",
    "accuracy_sub = np.sum(y_pred_sub == y_test)/len(y_test)\n",
    "print(accuracy_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analysis of y_preds\n",
    "\n",
    "#so I don't accidentally mess up 12 minute of predictions\n",
    "my_copy = y_preds_sub.copy()\n",
    "acc_sub=[]\n",
    "\n",
    "for _ in range(2,2000):\n",
    "    preds = my_copy[:_,:,:]\n",
    "    pred_1 = preds.mean(axis=0)\n",
    "    y_pred = np.argmax(pred_1, axis=1)\n",
    "    accuracy = np.sum(y_pred == y_test)/len(y_test)\n",
    "    acc_sub.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "\n",
    "plt.plot(range(30,2000),acc_sub[28:])\n",
    "plt.title(\"Monte-carlo dropout accuracy vs full-network (sub opt)\")\n",
    "plt.xlabel(\"No of monte-carlo samples\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1.0))\n",
    "plt.axhline(y=0.4737)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Above graph shows full network performance as horizontal line\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alpha dropout\n",
    "\n",
    "* Alpha dropout is used with self-regularizing networks (dense and using SELU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting lr = 0.01 Logged in: logs/fit/selu_alpha_0.01_20220625-170019\n",
      "Epoch 1/100\n",
      "   1/1172 [..............................] - ETA: 0s - loss: 3.0522 - accuracy: 0.0938WARNING:tensorflow:From c:\\Users\\micha\\anaconda3\\envs\\PandasNumpyMathplotlib\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "   2/1172 [..............................] - ETA: 45s - loss: 3.1776 - accuracy: 0.1094WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0090s vs `on_train_batch_end` time: 0.0678s). Check your callbacks.\n",
      "1172/1172 [==============================] - 12s 11ms/step - loss: 84.9952 - accuracy: 0.0997 - val_loss: 2.3584 - val_accuracy: 0.1007\n",
      "Epoch 2/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.3660 - accuracy: 0.0954 - val_loss: 2.3846 - val_accuracy: 0.0946\n",
      "Epoch 3/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.3618 - accuracy: 0.0990 - val_loss: 2.3915 - val_accuracy: 0.0946\n",
      "Epoch 4/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3629 - accuracy: 0.0981 - val_loss: 2.4353 - val_accuracy: 0.1031\n",
      "Epoch 5/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 3.9666 - accuracy: 0.1013 - val_loss: 2.4621 - val_accuracy: 0.1016\n",
      "Epoch 6/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 53609.9453 - accuracy: 0.1002 - val_loss: 2.3345 - val_accuracy: 0.1010\n",
      "Epoch 7/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 15103.5684 - accuracy: 0.0997 - val_loss: 2.3378 - val_accuracy: 0.1010\n",
      "Epoch 8/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2334.0798 - accuracy: 0.0975 - val_loss: 2.3459 - val_accuracy: 0.0946\n",
      "Epoch 9/100\n",
      "1172/1172 [==============================] - 12s 11ms/step - loss: 26.0949 - accuracy: 0.0997 - val_loss: 2.4202 - val_accuracy: 0.1016\n",
      "Epoch 10/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3671 - accuracy: 0.0983 - val_loss: 2.4752 - val_accuracy: 0.1010\n",
      "Epoch 11/100\n",
      "1172/1172 [==============================] - 12s 11ms/step - loss: 2.3666 - accuracy: 0.1006 - val_loss: 2.4438 - val_accuracy: 0.0988\n",
      "Epoch 12/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 403889.5312 - accuracy: 0.0983 - val_loss: 2.3563 - val_accuracy: 0.1007\n",
      "Epoch 13/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 39090.7031 - accuracy: 0.0992 - val_loss: 2.3182 - val_accuracy: 0.1016\n",
      "Epoch 14/100\n",
      "1172/1172 [==============================] - 12s 11ms/step - loss: 36316.1406 - accuracy: 0.1003 - val_loss: 2.4247 - val_accuracy: 0.0970\n",
      "Epoch 15/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 473736.5312 - accuracy: 0.0996 - val_loss: 2.3570 - val_accuracy: 0.1031\n",
      "Epoch 16/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 4204.0820 - accuracy: 0.1000 - val_loss: 2.3457 - val_accuracy: 0.1007\n",
      "Epoch 17/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.3787 - accuracy: 0.0997 - val_loss: 2.4493 - val_accuracy: 0.1033\n",
      "Epoch 18/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.3777 - accuracy: 0.1002 - val_loss: 2.4248 - val_accuracy: 0.1024\n",
      "Epoch 19/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.3789 - accuracy: 0.0965 - val_loss: 2.4806 - val_accuracy: 0.0975\n",
      "Epoch 20/100\n",
      "1172/1172 [==============================] - 12s 11ms/step - loss: 71571.3828 - accuracy: 0.0981 - val_loss: 2.3753 - val_accuracy: 0.1010\n",
      "Epoch 21/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 440001.9062 - accuracy: 0.1010 - val_loss: 2.4542 - val_accuracy: 0.1031\n",
      "Epoch 22/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 549295.1250 - accuracy: 0.0984 - val_loss: 2.3671 - val_accuracy: 0.1033\n",
      "Epoch 23/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 15212.2041 - accuracy: 0.0994 - val_loss: 2.4035 - val_accuracy: 0.1016\n",
      "Logged in: logs/fit/selu_alpha_0.01_20220625-170019\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 2.4035 - accuracy: 0.1016\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.4056 - accuracy: 0.1000\n",
      "fitting lr = 0.005 Logged in: logs/fit/selu_alpha_0.005_20220625-170515\n",
      "Epoch 1/100\n",
      "   2/1172 [..............................] - ETA: 27:21 - loss: 3.2108 - accuracy: 0.1094WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0090s vs `on_train_batch_end` time: 2.7955s). Check your callbacks.\n",
      "1172/1172 [==============================] - 15s 13ms/step - loss: 28.0352 - accuracy: 0.0994 - val_loss: 2.3595 - val_accuracy: 0.0970\n",
      "Epoch 2/100\n",
      "1172/1172 [==============================] - 12s 11ms/step - loss: 2.3547 - accuracy: 0.0970 - val_loss: 2.4085 - val_accuracy: 0.0970\n",
      "Epoch 3/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3743 - accuracy: 0.0992 - val_loss: 2.3430 - val_accuracy: 0.1010\n",
      "Epoch 4/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3962 - accuracy: 0.1007 - val_loss: 2.3483 - val_accuracy: 0.1024\n",
      "Epoch 5/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3456 - accuracy: 0.0999 - val_loss: 2.3425 - val_accuracy: 0.0946\n",
      "Epoch 6/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3439 - accuracy: 0.1006 - val_loss: 2.3534 - val_accuracy: 0.1007\n",
      "Epoch 7/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3718 - accuracy: 0.1003 - val_loss: 2.3305 - val_accuracy: 0.0970\n",
      "Epoch 8/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3454 - accuracy: 0.0999 - val_loss: 2.3841 - val_accuracy: 0.0946\n",
      "Epoch 9/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3471 - accuracy: 0.0976 - val_loss: 2.3638 - val_accuracy: 0.0970\n",
      "Epoch 10/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3495 - accuracy: 0.0997 - val_loss: 2.4234 - val_accuracy: 0.0988\n",
      "Epoch 11/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3472 - accuracy: 0.0990 - val_loss: 2.3869 - val_accuracy: 0.1010\n",
      "Epoch 12/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3482 - accuracy: 0.1000 - val_loss: 2.3530 - val_accuracy: 0.0970\n",
      "Epoch 13/100\n",
      "1172/1172 [==============================] - 12s 11ms/step - loss: 5.6650 - accuracy: 0.1033 - val_loss: 2.4358 - val_accuracy: 0.0970\n",
      "Epoch 14/100\n",
      "1172/1172 [==============================] - 12s 11ms/step - loss: 2.4247 - accuracy: 0.0996 - val_loss: 2.3692 - val_accuracy: 0.0975\n",
      "Epoch 15/100\n",
      "1172/1172 [==============================] - 12s 11ms/step - loss: 4849.6138 - accuracy: 0.0977 - val_loss: 2.4107 - val_accuracy: 0.0946\n",
      "Epoch 16/100\n",
      "1172/1172 [==============================] - 12s 11ms/step - loss: 2.5795 - accuracy: 0.1021 - val_loss: 2.3783 - val_accuracy: 0.0988\n",
      "Epoch 17/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3681 - accuracy: 0.0996 - val_loss: 2.4207 - val_accuracy: 0.0946\n",
      "Logged in: logs/fit/selu_alpha_0.005_20220625-170515\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 2.4207 - accuracy: 0.0946\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.4142 - accuracy: 0.1000\n",
      "fitting lr = 0.0025 Logged in: logs/fit/selu_alpha_0.0025_20220625-170900\n",
      "Epoch 1/100\n",
      "   2/1172 [..............................] - ETA: 28:55 - loss: 2.9605 - accuracy: 0.0625WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0080s vs `on_train_batch_end` time: 2.9591s). Check your callbacks.\n",
      "1172/1172 [==============================] - 16s 13ms/step - loss: 2.3875 - accuracy: 0.0990 - val_loss: 2.3319 - val_accuracy: 0.0988\n",
      "Epoch 2/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.3351 - accuracy: 0.1001 - val_loss: 2.3189 - val_accuracy: 0.1033\n",
      "Epoch 3/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3349 - accuracy: 0.1001 - val_loss: 2.3273 - val_accuracy: 0.1033\n",
      "Epoch 4/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3373 - accuracy: 0.0996 - val_loss: 2.3305 - val_accuracy: 0.1031\n",
      "Epoch 5/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3316 - accuracy: 0.0963 - val_loss: 2.3213 - val_accuracy: 0.1031\n",
      "Epoch 6/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3311 - accuracy: 0.0991 - val_loss: 2.3729 - val_accuracy: 0.1031\n",
      "Epoch 7/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3421 - accuracy: 0.1022 - val_loss: 2.3273 - val_accuracy: 0.1031\n",
      "Epoch 8/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3312 - accuracy: 0.0954 - val_loss: 2.3353 - val_accuracy: 0.1031\n",
      "Epoch 9/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3399 - accuracy: 0.1020 - val_loss: 2.3193 - val_accuracy: 0.0975\n",
      "Epoch 10/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3312 - accuracy: 0.1025 - val_loss: 2.3619 - val_accuracy: 0.0975\n",
      "Epoch 11/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3323 - accuracy: 0.1026 - val_loss: 2.3252 - val_accuracy: 0.1033\n",
      "Epoch 12/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3325 - accuracy: 0.0999 - val_loss: 2.3522 - val_accuracy: 0.1024\n",
      "Logged in: logs/fit/selu_alpha_0.0025_20220625-170900\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 2.3522 - accuracy: 0.1024\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3562 - accuracy: 0.1000\n",
      "fitting lr = 0.001 Logged in: logs/fit/selu_alpha_0.001_20220625-171145\n",
      "Epoch 1/100\n",
      "   2/1172 [..............................] - ETA: 29:49 - loss: 2.9798 - accuracy: 0.1406WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0090s vs `on_train_batch_end` time: 3.0508s). Check your callbacks.\n",
      "1172/1172 [==============================] - 16s 14ms/step - loss: 2.3756 - accuracy: 0.1009 - val_loss: 2.3628 - val_accuracy: 0.0970\n",
      "Epoch 2/100\n",
      "1172/1172 [==============================] - 12s 11ms/step - loss: 2.3207 - accuracy: 0.1000 - val_loss: 2.3062 - val_accuracy: 0.1058\n",
      "Epoch 3/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.2513 - accuracy: 0.1279 - val_loss: 4.7739 - val_accuracy: 0.1192\n",
      "Epoch 4/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.1383 - accuracy: 0.1669 - val_loss: 9.8908 - val_accuracy: 0.1456\n",
      "Epoch 5/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.1204 - accuracy: 0.1726 - val_loss: 5.4515 - val_accuracy: 0.1423\n",
      "Epoch 6/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.1156 - accuracy: 0.1751 - val_loss: 4.8833 - val_accuracy: 0.1262\n",
      "Epoch 7/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.1096 - accuracy: 0.1763 - val_loss: 5.6468 - val_accuracy: 0.1256\n",
      "Epoch 8/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.1153 - accuracy: 0.1719 - val_loss: 7.2240 - val_accuracy: 0.1590\n",
      "Epoch 9/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.1233 - accuracy: 0.1731 - val_loss: 4.5247 - val_accuracy: 0.1623\n",
      "Epoch 10/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.0997 - accuracy: 0.1781 - val_loss: 6.5889 - val_accuracy: 0.1249\n",
      "Epoch 11/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.1032 - accuracy: 0.1773 - val_loss: 6.6591 - val_accuracy: 0.1668\n",
      "Epoch 12/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.1011 - accuracy: 0.1787 - val_loss: 4.8297 - val_accuracy: 0.1697\n",
      "Logged in: logs/fit/selu_alpha_0.001_20220625-171145\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 4.8297 - accuracy: 0.1697\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 4.8458 - accuracy: 0.1642\n",
      "fitting lr = 0.0005 Logged in: logs/fit/selu_alpha_0.0005_20220625-171429\n",
      "Epoch 1/100\n",
      "   2/1172 [..............................] - ETA: 30:36 - loss: 2.9627 - accuracy: 0.1250WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0090s vs `on_train_batch_end` time: 3.1296s). Check your callbacks.\n",
      "1172/1172 [==============================] - 16s 14ms/step - loss: 2.4191 - accuracy: 0.0957 - val_loss: 2.3307 - val_accuracy: 0.1010\n",
      "Epoch 2/100\n",
      "1172/1172 [==============================] - 12s 11ms/step - loss: 2.3131 - accuracy: 0.1012 - val_loss: 2.3093 - val_accuracy: 0.1033\n",
      "Epoch 3/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3117 - accuracy: 0.1009 - val_loss: 2.3120 - val_accuracy: 0.1010\n",
      "Epoch 4/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.3112 - accuracy: 0.0980 - val_loss: 2.3217 - val_accuracy: 0.0970\n",
      "Epoch 5/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3113 - accuracy: 0.0986 - val_loss: 2.3144 - val_accuracy: 0.1007\n",
      "Epoch 6/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.3107 - accuracy: 0.1004 - val_loss: 2.3164 - val_accuracy: 0.1031\n",
      "Epoch 7/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3108 - accuracy: 0.1008 - val_loss: 2.3164 - val_accuracy: 0.0946\n",
      "Epoch 8/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3122 - accuracy: 0.0975 - val_loss: 2.3115 - val_accuracy: 0.0946\n",
      "Epoch 9/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3113 - accuracy: 0.0986 - val_loss: 2.3202 - val_accuracy: 0.1016\n",
      "Epoch 10/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3115 - accuracy: 0.1019 - val_loss: 2.3089 - val_accuracy: 0.0977\n",
      "Epoch 11/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3106 - accuracy: 0.0987 - val_loss: 2.3241 - val_accuracy: 0.1031\n",
      "Epoch 12/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3133 - accuracy: 0.0974 - val_loss: 2.3096 - val_accuracy: 0.0946\n",
      "Epoch 13/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3122 - accuracy: 0.0999 - val_loss: 2.3276 - val_accuracy: 0.0975\n",
      "Epoch 14/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3106 - accuracy: 0.1013 - val_loss: 2.3126 - val_accuracy: 0.1010\n",
      "Epoch 15/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3112 - accuracy: 0.0992 - val_loss: 2.3104 - val_accuracy: 0.0946\n",
      "Epoch 16/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3112 - accuracy: 0.1012 - val_loss: 2.3151 - val_accuracy: 0.1024\n",
      "Epoch 17/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.2893 - accuracy: 0.1156 - val_loss: 2.4556 - val_accuracy: 0.1031\n",
      "Epoch 18/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.1643 - accuracy: 0.1621 - val_loss: 2.3607 - val_accuracy: 0.1121\n",
      "Epoch 19/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.1335 - accuracy: 0.1726 - val_loss: 2.2441 - val_accuracy: 0.1706\n",
      "Epoch 20/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.1203 - accuracy: 0.1707 - val_loss: 2.3087 - val_accuracy: 0.1643\n",
      "Epoch 21/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.1062 - accuracy: 0.1791 - val_loss: 2.3378 - val_accuracy: 0.1615\n",
      "Epoch 22/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.0956 - accuracy: 0.1793 - val_loss: 2.3005 - val_accuracy: 0.1576\n",
      "Epoch 23/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.0867 - accuracy: 0.1845 - val_loss: 2.2446 - val_accuracy: 0.1590\n",
      "Epoch 24/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.0792 - accuracy: 0.1897 - val_loss: 2.1408 - val_accuracy: 0.1807\n",
      "Epoch 25/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.0662 - accuracy: 0.1981 - val_loss: 2.2169 - val_accuracy: 0.1953\n",
      "Epoch 26/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.0533 - accuracy: 0.2087 - val_loss: 2.3567 - val_accuracy: 0.1453\n",
      "Epoch 27/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.0511 - accuracy: 0.2105 - val_loss: 2.3612 - val_accuracy: 0.2059\n",
      "Epoch 28/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.0561 - accuracy: 0.2092 - val_loss: 2.5772 - val_accuracy: 0.1184\n",
      "Epoch 29/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.1395 - accuracy: 0.1945 - val_loss: 2.3352 - val_accuracy: 0.1773\n",
      "Epoch 30/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.0777 - accuracy: 0.1847 - val_loss: 2.2295 - val_accuracy: 0.1830\n",
      "Epoch 31/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.0549 - accuracy: 0.2023 - val_loss: 2.5761 - val_accuracy: 0.1726\n",
      "Epoch 32/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.0568 - accuracy: 0.2035 - val_loss: 2.2691 - val_accuracy: 0.1649\n",
      "Epoch 33/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.0427 - accuracy: 0.2080 - val_loss: 2.4929 - val_accuracy: 0.1617\n",
      "Epoch 34/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.0251 - accuracy: 0.2187 - val_loss: 3.8701 - val_accuracy: 0.1592\n",
      "Logged in: logs/fit/selu_alpha_0.0005_20220625-171429\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 3.8701 - accuracy: 0.1592\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 3.8228 - accuracy: 0.1617\n",
      "fitting lr = 0.00025 Logged in: logs/fit/selu_alpha_0.00025_20220625-172157\n",
      "Epoch 1/100\n",
      "   2/1172 [..............................] - ETA: 29:17 - loss: 2.8611 - accuracy: 0.1406WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0090s vs `on_train_batch_end` time: 2.9940s). Check your callbacks.\n",
      "1172/1172 [==============================] - 16s 14ms/step - loss: 2.4764 - accuracy: 0.1026 - val_loss: 2.3317 - val_accuracy: 0.1033\n",
      "Epoch 2/100\n",
      "1172/1172 [==============================] - 12s 11ms/step - loss: 2.3193 - accuracy: 0.1013 - val_loss: 2.3187 - val_accuracy: 0.0946\n",
      "Epoch 3/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3103 - accuracy: 0.0993 - val_loss: 2.3132 - val_accuracy: 0.0970\n",
      "Epoch 4/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.3094 - accuracy: 0.1005 - val_loss: 2.3088 - val_accuracy: 0.0946\n",
      "Epoch 5/100\n",
      "1172/1172 [==============================] - 15s 13ms/step - loss: 2.3081 - accuracy: 0.0999 - val_loss: 2.3194 - val_accuracy: 0.0946\n",
      "Epoch 6/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3073 - accuracy: 0.1008 - val_loss: 2.3060 - val_accuracy: 0.1031\n",
      "Epoch 7/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3073 - accuracy: 0.0995 - val_loss: 2.3057 - val_accuracy: 0.1033\n",
      "Epoch 8/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3073 - accuracy: 0.1025 - val_loss: 2.3123 - val_accuracy: 0.1024\n",
      "Epoch 9/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3074 - accuracy: 0.1019 - val_loss: 2.3073 - val_accuracy: 0.1016\n",
      "Epoch 10/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3075 - accuracy: 0.0996 - val_loss: 2.3128 - val_accuracy: 0.1010\n",
      "Epoch 11/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3072 - accuracy: 0.1008 - val_loss: 2.3119 - val_accuracy: 0.1024\n",
      "Epoch 12/100\n",
      "1172/1172 [==============================] - 12s 11ms/step - loss: 2.3073 - accuracy: 0.0991 - val_loss: 2.3107 - val_accuracy: 0.0975\n",
      "Epoch 13/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3074 - accuracy: 0.1010 - val_loss: 2.3071 - val_accuracy: 0.1031\n",
      "Epoch 14/100\n",
      "1172/1172 [==============================] - 12s 11ms/step - loss: 2.3077 - accuracy: 0.0985 - val_loss: 2.3061 - val_accuracy: 0.0946\n",
      "Epoch 15/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3082 - accuracy: 0.0977 - val_loss: 2.3063 - val_accuracy: 0.1016\n",
      "Epoch 16/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.3070 - accuracy: 0.0999 - val_loss: 2.3074 - val_accuracy: 0.1016\n",
      "Epoch 17/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3073 - accuracy: 0.0991 - val_loss: 2.3060 - val_accuracy: 0.1031\n",
      "Logged in: logs/fit/selu_alpha_0.00025_20220625-172157\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 2.3060 - accuracy: 0.1031\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3062 - accuracy: 0.1000\n",
      "fitting lr = 0.0001 Logged in: logs/fit/selu_alpha_0.0001_20220625-172551\n",
      "Epoch 1/100\n",
      "   2/1172 [..............................] - ETA: 34:10 - loss: 3.2209 - accuracy: 0.0781WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0090s vs `on_train_batch_end` time: 3.4958s). Check your callbacks.\n",
      "1172/1172 [==============================] - 17s 15ms/step - loss: 2.6494 - accuracy: 0.1000 - val_loss: 2.5119 - val_accuracy: 0.0946\n",
      "Epoch 2/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3877 - accuracy: 0.1012 - val_loss: 2.3656 - val_accuracy: 0.0988\n",
      "Epoch 3/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3379 - accuracy: 0.0991 - val_loss: 2.3399 - val_accuracy: 0.1010\n",
      "Epoch 4/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3187 - accuracy: 0.1008 - val_loss: 2.3105 - val_accuracy: 0.0988\n",
      "Epoch 5/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3117 - accuracy: 0.1009 - val_loss: 2.3112 - val_accuracy: 0.1010\n",
      "Epoch 6/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.3086 - accuracy: 0.0997 - val_loss: 2.3051 - val_accuracy: 0.1024\n",
      "Epoch 7/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3069 - accuracy: 0.0990 - val_loss: 2.3062 - val_accuracy: 0.1010\n",
      "Epoch 8/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3065 - accuracy: 0.1030 - val_loss: 2.3066 - val_accuracy: 0.1010\n",
      "Epoch 9/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3055 - accuracy: 0.0987 - val_loss: 2.3033 - val_accuracy: 0.1033\n",
      "Epoch 10/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.3058 - accuracy: 0.0998 - val_loss: 2.3034 - val_accuracy: 0.1031\n",
      "Epoch 11/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.3052 - accuracy: 0.1025 - val_loss: 2.3066 - val_accuracy: 0.1031\n",
      "Epoch 12/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.2989 - accuracy: 0.1038 - val_loss: 5.5650 - val_accuracy: 0.1291\n",
      "Epoch 13/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.1539 - accuracy: 0.1622 - val_loss: 9.7762 - val_accuracy: 0.1593\n",
      "Epoch 14/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.1202 - accuracy: 0.1743 - val_loss: 8.3203 - val_accuracy: 0.1634\n",
      "Epoch 15/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.1087 - accuracy: 0.1799 - val_loss: 6.7348 - val_accuracy: 0.1658\n",
      "Epoch 16/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.1008 - accuracy: 0.1803 - val_loss: 7.6417 - val_accuracy: 0.1779\n",
      "Epoch 17/100\n",
      "1172/1172 [==============================] - 15s 12ms/step - loss: 2.0927 - accuracy: 0.1802 - val_loss: 7.3106 - val_accuracy: 0.1790\n",
      "Epoch 18/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.0858 - accuracy: 0.1812 - val_loss: 5.5071 - val_accuracy: 0.1874\n",
      "Epoch 19/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.0791 - accuracy: 0.1882 - val_loss: 8.6528 - val_accuracy: 0.1874\n",
      "Logged in: logs/fit/selu_alpha_0.0001_20220625-172551\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 8.6528 - accuracy: 0.1874\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 8.7695 - accuracy: 0.1874\n",
      "fitting lr = 5e-05 Logged in: logs/fit/selu_alpha_5e-05_20220625-173022\n",
      "Epoch 1/100\n",
      "   2/1172 [..............................] - ETA: 29:24 - loss: 2.8931 - accuracy: 0.0938WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0080s vs `on_train_batch_end` time: 3.0065s). Check your callbacks.\n",
      "1172/1172 [==============================] - 17s 15ms/step - loss: 2.7487 - accuracy: 0.1005 - val_loss: 2.4665 - val_accuracy: 0.1007\n",
      "Epoch 2/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.5191 - accuracy: 0.1012 - val_loss: 2.3900 - val_accuracy: 0.1024\n",
      "Epoch 3/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.4065 - accuracy: 0.1010 - val_loss: 2.3307 - val_accuracy: 0.1031\n",
      "Epoch 4/100\n",
      "1172/1172 [==============================] - 15s 12ms/step - loss: 2.3526 - accuracy: 0.1018 - val_loss: 2.3211 - val_accuracy: 0.1031\n",
      "Epoch 5/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.3309 - accuracy: 0.1002 - val_loss: 2.3100 - val_accuracy: 0.1031\n",
      "Epoch 6/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.3212 - accuracy: 0.1021 - val_loss: 2.3090 - val_accuracy: 0.1031\n",
      "Epoch 7/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.2925 - accuracy: 0.1163 - val_loss: 2.5902 - val_accuracy: 0.1649\n",
      "Epoch 8/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.1699 - accuracy: 0.1594 - val_loss: 3.0606 - val_accuracy: 0.1787\n",
      "Epoch 9/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.1434 - accuracy: 0.1625 - val_loss: 3.0975 - val_accuracy: 0.1744\n",
      "Epoch 10/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.1279 - accuracy: 0.1716 - val_loss: 3.3159 - val_accuracy: 0.1740\n",
      "Epoch 11/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.1197 - accuracy: 0.1692 - val_loss: 3.2324 - val_accuracy: 0.1766\n",
      "Epoch 12/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.1120 - accuracy: 0.1747 - val_loss: 3.0413 - val_accuracy: 0.1843\n",
      "Epoch 13/100\n",
      "1172/1172 [==============================] - 13s 12ms/step - loss: 2.1041 - accuracy: 0.1765 - val_loss: 3.0087 - val_accuracy: 0.1869\n",
      "Epoch 14/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.1021 - accuracy: 0.1781 - val_loss: 3.1198 - val_accuracy: 0.1885\n",
      "Epoch 15/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.0931 - accuracy: 0.1834 - val_loss: 3.5864 - val_accuracy: 0.1830\n",
      "Epoch 16/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.0892 - accuracy: 0.1806 - val_loss: 4.2611 - val_accuracy: 0.1862\n",
      "Logged in: logs/fit/selu_alpha_5e-05_20220625-173022\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 4.2611 - accuracy: 0.1862\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 4.2940 - accuracy: 0.1882: 0s - los\n",
      "Best validation performance: 18.74% for lr: 0.0001\n",
      "Best test performance: 18.82% for lr: 5e-05\n",
      "validation accuracy: [0.10159999877214432, 0.09455999732017517, 0.10239999741315842, 0.16967999935150146, 0.15919999778270721, 0.10311999917030334, 0.1873600035905838, 0.18624000251293182]\n",
      "test accuracy: [0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.16419999301433563, 0.16169999539852142, 0.10000000149011612, 0.1873999983072281, 0.1881999969482422]\n"
     ]
    }
   ],
   "source": [
    "#Start with 0.1 dropout\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from functools import partial\n",
    "\n",
    "selu_layer = partial(keras.layers.Dense,kernel_initializer=\"lecun_normal\", activation=\"selu\")\n",
    "\n",
    "def make_DNN_alpha (layer_function, n_hidden,use_normalization=False,dropout=0.1):\n",
    "    architecture = [keras.layers.Flatten(input_shape=[32,32,3])]\n",
    "    architecture.append(keras.layers.AlphaDropout(rate=dropout))\n",
    "    for _ in range(n_hidden):\n",
    "        architecture.append(layer_function(100))\n",
    "        architecture.append(keras.layers.AlphaDropout(rate=dropout))\n",
    "        \n",
    "    architecture.append(keras.layers.Dense(10,activation=\"softmax\"))\n",
    "    return(keras.models.Sequential(architecture))\n",
    "\n",
    "def selu_alpha_model():\n",
    "    return make_DNN_alpha(selu_layer,20,False,0.1)\n",
    "\n",
    "#selu_model().summary()\n",
    "\n",
    "lrs=[0.01, 0.005, 0.0025, 0.001, 0.0005, 0.00025, 0.0001, 0.00005]\n",
    "\n",
    "valid_accuracy,test_accuracy,logs, best_models = tune_lrs(selu_alpha_model, lrs, \"selu_alpha\")\n",
    "\n",
    "print(f\"validation accuracy: {valid_accuracy}\")\n",
    "print(f\"test accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting lr = 0.01 Logged in: logs/fit/selu_alpha_lower_0.01_20220625-173633\n",
      "Epoch 1/100\n",
      "   2/1172 [..............................] - ETA: 26:20 - loss: 3.4583 - accuracy: 0.1094WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0100s vs `on_train_batch_end` time: 2.6908s). Check your callbacks.\n",
      "1172/1172 [==============================] - 16s 13ms/step - loss: 2.5529 - accuracy: 0.0996 - val_loss: 2.4834 - val_accuracy: 0.1007\n",
      "Epoch 2/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 135298272.0000 - accuracy: 0.0995 - val_loss: 3.2790 - val_accuracy: 0.1033\n",
      "Epoch 3/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 3613.2688 - accuracy: 0.0981 - val_loss: 2.6648 - val_accuracy: 0.1033\n",
      "Epoch 4/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 136242.9844 - accuracy: 0.1025 - val_loss: 2.6153 - val_accuracy: 0.1033\n",
      "Epoch 5/100\n",
      "1172/1172 [==============================] - 13s 12ms/step - loss: 273.9124 - accuracy: 0.0998 - val_loss: 2.3337 - val_accuracy: 0.1024\n",
      "Epoch 6/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 18.6693 - accuracy: 0.1016 - val_loss: 2.3579 - val_accuracy: 0.1024\n",
      "Epoch 7/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 76.8151 - accuracy: 0.1013 - val_loss: 2.3823 - val_accuracy: 0.1024\n",
      "Epoch 8/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 615.9039 - accuracy: 0.1000 - val_loss: 2.3943 - val_accuracy: 0.1031\n",
      "Epoch 9/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 5.3356 - accuracy: 0.1004 - val_loss: 2.3925 - val_accuracy: 0.1010\n",
      "Epoch 10/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 18.6406 - accuracy: 0.1002 - val_loss: 2.6365 - val_accuracy: 0.0970\n",
      "Epoch 11/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 269.4761 - accuracy: 0.1013 - val_loss: 2.8578 - val_accuracy: 0.1010\n",
      "Epoch 12/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.7364 - accuracy: 0.0999 - val_loss: 2.6024 - val_accuracy: 0.0970\n",
      "Epoch 13/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.5229 - accuracy: 0.0966 - val_loss: 2.4344 - val_accuracy: 0.0970\n",
      "Epoch 14/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.4288 - accuracy: 0.1015 - val_loss: 2.8389 - val_accuracy: 0.1010\n",
      "Epoch 15/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 38445.6406 - accuracy: 0.1012 - val_loss: 10.2182 - val_accuracy: 0.0946\n",
      "Logged in: logs/fit/selu_alpha_lower_0.01_20220625-173633\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 10.2182 - accuracy: 0.0946\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 10.2085 - accuracy: 0.1000\n",
      "fitting lr = 0.005 Logged in: logs/fit/selu_alpha_lower_0.005_20220625-174001\n",
      "Epoch 1/100\n",
      "   2/1172 [..............................] - ETA: 27:15 - loss: 3.2156 - accuracy: 0.0938WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0100s vs `on_train_batch_end` time: 2.7865s). Check your callbacks.\n",
      "1172/1172 [==============================] - 16s 13ms/step - loss: 2.4371 - accuracy: 0.0996 - val_loss: 2.4613 - val_accuracy: 0.0988\n",
      "Epoch 2/100\n",
      "1172/1172 [==============================] - 12s 11ms/step - loss: 20.7526 - accuracy: 0.0985 - val_loss: 2.3761 - val_accuracy: 0.0946\n",
      "Epoch 3/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3263 - accuracy: 0.0994 - val_loss: 2.3263 - val_accuracy: 0.0967\n",
      "Epoch 4/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3328 - accuracy: 0.0990 - val_loss: 2.3354 - val_accuracy: 0.1024\n",
      "Epoch 5/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3321 - accuracy: 0.0990 - val_loss: 2.3440 - val_accuracy: 0.0946\n",
      "Epoch 6/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3400 - accuracy: 0.0999 - val_loss: 2.3824 - val_accuracy: 0.0970\n",
      "Epoch 7/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3368 - accuracy: 0.1011 - val_loss: 2.3551 - val_accuracy: 0.0946\n",
      "Epoch 8/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 3.4430 - accuracy: 0.0973 - val_loss: 2.3473 - val_accuracy: 0.0970\n",
      "Epoch 9/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3436 - accuracy: 0.0982 - val_loss: 2.3486 - val_accuracy: 0.1007\n",
      "Epoch 10/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3383 - accuracy: 0.0999 - val_loss: 2.3600 - val_accuracy: 0.1031\n",
      "Epoch 11/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3430 - accuracy: 0.1034 - val_loss: 2.3322 - val_accuracy: 0.1024\n",
      "Epoch 12/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3451 - accuracy: 0.1041 - val_loss: 2.3406 - val_accuracy: 0.1010\n",
      "Epoch 13/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3492 - accuracy: 0.0999 - val_loss: 2.4114 - val_accuracy: 0.1033\n",
      "Logged in: logs/fit/selu_alpha_lower_0.005_20220625-174001\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 2.4114 - accuracy: 0.1033\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.4112 - accuracy: 0.1000\n",
      "fitting lr = 0.0025 Logged in: logs/fit/selu_alpha_lower_0.0025_20220625-174302\n",
      "Epoch 1/100\n",
      "   2/1172 [..............................] - ETA: 28:26 - loss: 3.2574 - accuracy: 0.0469WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0100s vs `on_train_batch_end` time: 2.9072s). Check your callbacks.\n",
      "1172/1172 [==============================] - 16s 14ms/step - loss: 2.4286 - accuracy: 0.1020 - val_loss: 2.3147 - val_accuracy: 0.0970\n",
      "Epoch 2/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3355 - accuracy: 0.1029 - val_loss: 2.3789 - val_accuracy: 0.0946\n",
      "Epoch 3/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3387 - accuracy: 0.0989 - val_loss: 2.3162 - val_accuracy: 0.1024\n",
      "Epoch 4/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.3350 - accuracy: 0.0984 - val_loss: 2.3514 - val_accuracy: 0.1016\n",
      "Epoch 5/100\n",
      "1172/1172 [==============================] - 13s 12ms/step - loss: 2.3335 - accuracy: 0.0985 - val_loss: 2.3284 - val_accuracy: 0.0988\n",
      "Epoch 6/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3330 - accuracy: 0.1020 - val_loss: 2.3229 - val_accuracy: 0.0975\n",
      "Epoch 7/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3328 - accuracy: 0.1010 - val_loss: 2.3495 - val_accuracy: 0.1031\n",
      "Epoch 8/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3329 - accuracy: 0.1004 - val_loss: 2.3391 - val_accuracy: 0.0970\n",
      "Epoch 9/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.3498 - accuracy: 0.1003 - val_loss: 2.3305 - val_accuracy: 0.1010\n",
      "Epoch 10/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3361 - accuracy: 0.1005 - val_loss: 2.3414 - val_accuracy: 0.0946\n",
      "Epoch 11/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3414 - accuracy: 0.0992 - val_loss: 2.3429 - val_accuracy: 0.1010\n",
      "Logged in: logs/fit/selu_alpha_lower_0.0025_20220625-174302\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 2.3429 - accuracy: 0.1010\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 2.3459 - accuracy: 0.1000\n",
      "fitting lr = 0.001 Logged in: logs/fit/selu_alpha_lower_0.001_20220625-174542\n",
      "Epoch 1/100\n",
      "   2/1172 [..............................] - ETA: 28:55 - loss: 3.0513 - accuracy: 0.1094WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0100s vs `on_train_batch_end` time: 2.9571s). Check your callbacks.\n",
      "1172/1172 [==============================] - 16s 14ms/step - loss: 2.2495 - accuracy: 0.1421 - val_loss: 2.1731 - val_accuracy: 0.1844\n",
      "Epoch 2/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.1109 - accuracy: 0.1764 - val_loss: 2.4498 - val_accuracy: 0.1284\n",
      "Epoch 3/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.0966 - accuracy: 0.1819 - val_loss: 2.3810 - val_accuracy: 0.1758\n",
      "Epoch 4/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.0750 - accuracy: 0.1899 - val_loss: 2.8377 - val_accuracy: 0.1586\n",
      "Epoch 5/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.0877 - accuracy: 0.1870 - val_loss: 2.7830 - val_accuracy: 0.1888\n",
      "Epoch 6/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.0922 - accuracy: 0.1889 - val_loss: 2.7470 - val_accuracy: 0.1578\n",
      "Epoch 7/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.0833 - accuracy: 0.1904 - val_loss: 2.4942 - val_accuracy: 0.1036\n",
      "Epoch 8/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.1753 - accuracy: 0.1565 - val_loss: 2.1889 - val_accuracy: 0.1775\n",
      "Epoch 9/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.0777 - accuracy: 0.1838 - val_loss: 2.2072 - val_accuracy: 0.1756\n",
      "Epoch 10/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.0609 - accuracy: 0.1997 - val_loss: 2.1617 - val_accuracy: 0.1895\n",
      "Epoch 11/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.0396 - accuracy: 0.2116 - val_loss: 2.1681 - val_accuracy: 0.1869\n",
      "Epoch 12/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.0490 - accuracy: 0.2057 - val_loss: 2.1285 - val_accuracy: 0.1828\n",
      "Epoch 13/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.0361 - accuracy: 0.2108 - val_loss: 2.3404 - val_accuracy: 0.1499\n",
      "Epoch 14/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.0818 - accuracy: 0.1932 - val_loss: 2.1455 - val_accuracy: 0.1778\n",
      "Epoch 15/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.0851 - accuracy: 0.1785 - val_loss: 2.2579 - val_accuracy: 0.1272\n",
      "Epoch 16/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.1806 - accuracy: 0.1501 - val_loss: 2.1568 - val_accuracy: 0.1654\n",
      "Epoch 17/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.0600 - accuracy: 0.1893 - val_loss: 2.0972 - val_accuracy: 0.1946\n",
      "Epoch 18/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.0573 - accuracy: 0.1879 - val_loss: 2.1160 - val_accuracy: 0.1860\n",
      "Epoch 19/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.0341 - accuracy: 0.1907 - val_loss: 2.1244 - val_accuracy: 0.1991\n",
      "Epoch 20/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.0261 - accuracy: 0.1990 - val_loss: 2.5205 - val_accuracy: 0.1916\n",
      "Epoch 21/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.0135 - accuracy: 0.2079 - val_loss: 2.2578 - val_accuracy: 0.1997\n",
      "Epoch 22/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.0408 - accuracy: 0.2191 - val_loss: 2.2811 - val_accuracy: 0.1768\n",
      "Epoch 23/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.2654 - accuracy: 0.1610 - val_loss: 3.0393 - val_accuracy: 0.1078\n",
      "Epoch 24/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.1132 - accuracy: 0.1762 - val_loss: 2.8410 - val_accuracy: 0.0988\n",
      "Epoch 25/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.0871 - accuracy: 0.1781 - val_loss: 2.6199 - val_accuracy: 0.1552\n",
      "Epoch 26/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.0405 - accuracy: 0.1858 - val_loss: 2.2596 - val_accuracy: 0.1707\n",
      "Epoch 27/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.0264 - accuracy: 0.1985 - val_loss: 2.2581 - val_accuracy: 0.2003\n",
      "Logged in: logs/fit/selu_alpha_lower_0.001_20220625-174542\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 2.2581 - accuracy: 0.2003\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.2578 - accuracy: 0.1985\n",
      "fitting lr = 0.0005 Logged in: logs/fit/selu_alpha_lower_0.0005_20220625-175150\n",
      "Epoch 1/100\n",
      "   2/1172 [..............................] - ETA: 25:08 - loss: 3.1784 - accuracy: 0.0938WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0100s vs `on_train_batch_end` time: 2.5671s). Check your callbacks.\n",
      "1172/1172 [==============================] - 16s 14ms/step - loss: 2.2983 - accuracy: 0.1387 - val_loss: 2.1862 - val_accuracy: 0.1828\n",
      "Epoch 2/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.1037 - accuracy: 0.1798 - val_loss: 2.7532 - val_accuracy: 0.1804\n",
      "Epoch 3/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.0667 - accuracy: 0.1946 - val_loss: 2.2944 - val_accuracy: 0.1726\n",
      "Epoch 4/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.0566 - accuracy: 0.2082 - val_loss: 3.4616 - val_accuracy: 0.1603\n",
      "Epoch 5/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.9943 - accuracy: 0.2313 - val_loss: 2.6477 - val_accuracy: 0.2099\n",
      "Epoch 6/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.9543 - accuracy: 0.2546 - val_loss: 2.6865 - val_accuracy: 0.2192\n",
      "Epoch 7/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.9372 - accuracy: 0.2666 - val_loss: 2.7640 - val_accuracy: 0.2612\n",
      "Epoch 8/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.9167 - accuracy: 0.2783 - val_loss: 2.2421 - val_accuracy: 0.2542\n",
      "Epoch 9/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.9054 - accuracy: 0.2822 - val_loss: 2.2247 - val_accuracy: 0.2726\n",
      "Epoch 10/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.8964 - accuracy: 0.2890 - val_loss: 2.4532 - val_accuracy: 0.2801\n",
      "Epoch 11/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.8796 - accuracy: 0.2930 - val_loss: 2.4415 - val_accuracy: 0.2982\n",
      "Logged in: logs/fit/selu_alpha_lower_0.0005_20220625-175150\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 2.4415 - accuracy: 0.2982\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.4232 - accuracy: 0.3077\n",
      "fitting lr = 0.00025 Logged in: logs/fit/selu_alpha_lower_0.00025_20220625-175430\n",
      "Epoch 1/100\n",
      "   2/1172 [..............................] - ETA: 29:12 - loss: 3.1564 - accuracy: 0.0938WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0120s vs `on_train_batch_end` time: 2.9820s). Check your callbacks.\n",
      "1172/1172 [==============================] - 17s 14ms/step - loss: 2.3089 - accuracy: 0.1477 - val_loss: 2.1098 - val_accuracy: 0.1915\n",
      "Epoch 2/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.1240 - accuracy: 0.1697 - val_loss: 2.0575 - val_accuracy: 0.1828\n",
      "Epoch 3/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.0700 - accuracy: 0.1989 - val_loss: 2.0626 - val_accuracy: 0.2366\n",
      "Epoch 4/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.0028 - accuracy: 0.2302 - val_loss: 1.9477 - val_accuracy: 0.2676\n",
      "Epoch 5/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 1.9606 - accuracy: 0.2546 - val_loss: 2.0920 - val_accuracy: 0.2750\n",
      "Epoch 6/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.9344 - accuracy: 0.2685 - val_loss: 2.1418 - val_accuracy: 0.2686\n",
      "Epoch 7/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 1.9138 - accuracy: 0.2798 - val_loss: 2.1038 - val_accuracy: 0.2830\n",
      "Epoch 8/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.8989 - accuracy: 0.2838 - val_loss: 2.1269 - val_accuracy: 0.3013\n",
      "Epoch 9/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.8842 - accuracy: 0.2940 - val_loss: 2.1318 - val_accuracy: 0.2975\n",
      "Epoch 10/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.8693 - accuracy: 0.3014 - val_loss: 2.1850 - val_accuracy: 0.3200\n",
      "Epoch 11/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.8609 - accuracy: 0.3099 - val_loss: 2.0426 - val_accuracy: 0.3386\n",
      "Epoch 12/100\n",
      "1172/1172 [==============================] - 13s 12ms/step - loss: 1.8457 - accuracy: 0.3132 - val_loss: 1.9903 - val_accuracy: 0.3311\n",
      "Epoch 13/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.8358 - accuracy: 0.3198 - val_loss: 1.9932 - val_accuracy: 0.3404\n",
      "Epoch 14/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.8245 - accuracy: 0.3238 - val_loss: 2.1890 - val_accuracy: 0.3498\n",
      "Logged in: logs/fit/selu_alpha_lower_0.00025_20220625-175430\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 2.1890 - accuracy: 0.3498\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.1380 - accuracy: 0.3574\n",
      "fitting lr = 0.0001 Logged in: logs/fit/selu_alpha_lower_0.0001_20220625-175751\n",
      "Epoch 1/100\n",
      "   2/1172 [..............................] - ETA: 27:18 - loss: 2.8751 - accuracy: 0.1406WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0110s vs `on_train_batch_end` time: 2.7905s). Check your callbacks.\n",
      "1172/1172 [==============================] - 16s 14ms/step - loss: 2.5138 - accuracy: 0.1158 - val_loss: 2.1438 - val_accuracy: 0.1715\n",
      "Epoch 2/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.1900 - accuracy: 0.1624 - val_loss: 2.1048 - val_accuracy: 0.1901\n",
      "Epoch 3/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.1301 - accuracy: 0.1726 - val_loss: 2.0587 - val_accuracy: 0.1990\n",
      "Epoch 4/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.0987 - accuracy: 0.1793 - val_loss: 2.0542 - val_accuracy: 0.1966\n",
      "Epoch 5/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.0760 - accuracy: 0.1883 - val_loss: 2.0103 - val_accuracy: 0.2283\n",
      "Epoch 6/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.0410 - accuracy: 0.2108 - val_loss: 2.0424 - val_accuracy: 0.2425\n",
      "Epoch 7/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.9945 - accuracy: 0.2298 - val_loss: 2.1745 - val_accuracy: 0.2511\n",
      "Epoch 8/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 1.9688 - accuracy: 0.2419 - val_loss: 2.0466 - val_accuracy: 0.2614\n",
      "Epoch 9/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 1.9493 - accuracy: 0.2552 - val_loss: 2.1094 - val_accuracy: 0.2675\n",
      "Epoch 10/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 1.9331 - accuracy: 0.2650 - val_loss: 2.0167 - val_accuracy: 0.2863\n",
      "Epoch 11/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 1.9164 - accuracy: 0.2789 - val_loss: 2.0862 - val_accuracy: 0.2923\n",
      "Epoch 12/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 1.9068 - accuracy: 0.2829 - val_loss: 2.0092 - val_accuracy: 0.3051\n",
      "Epoch 13/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 1.8926 - accuracy: 0.2943 - val_loss: 2.0634 - val_accuracy: 0.3041\n",
      "Epoch 14/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 1.8830 - accuracy: 0.2957 - val_loss: 2.0372 - val_accuracy: 0.3254\n",
      "Epoch 15/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.8729 - accuracy: 0.3058 - val_loss: 2.0557 - val_accuracy: 0.3314\n",
      "Epoch 16/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 1.8626 - accuracy: 0.3132 - val_loss: 2.1349 - val_accuracy: 0.3275\n",
      "Epoch 17/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.8543 - accuracy: 0.3070 - val_loss: 2.0318 - val_accuracy: 0.3466\n",
      "Epoch 18/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.8442 - accuracy: 0.3174 - val_loss: 2.0238 - val_accuracy: 0.3496\n",
      "Epoch 19/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.8325 - accuracy: 0.3199 - val_loss: 2.1326 - val_accuracy: 0.3357\n",
      "Epoch 20/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.8266 - accuracy: 0.3235 - val_loss: 2.0203 - val_accuracy: 0.3415\n",
      "Epoch 21/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 1.8183 - accuracy: 0.3298 - val_loss: 2.0367 - val_accuracy: 0.3481\n",
      "Epoch 22/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.8216 - accuracy: 0.3281 - val_loss: 1.9511 - val_accuracy: 0.3570\n",
      "Epoch 23/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.8023 - accuracy: 0.3367 - val_loss: 2.0269 - val_accuracy: 0.3632\n",
      "Epoch 24/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.7947 - accuracy: 0.3424 - val_loss: 2.5260 - val_accuracy: 0.3175\n",
      "Epoch 25/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.7915 - accuracy: 0.3424 - val_loss: 1.9863 - val_accuracy: 0.3627\n",
      "Epoch 26/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.7842 - accuracy: 0.3483 - val_loss: 2.0490 - val_accuracy: 0.3694\n",
      "Epoch 27/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.7739 - accuracy: 0.3548 - val_loss: 2.1404 - val_accuracy: 0.3558\n",
      "Epoch 28/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 1.7662 - accuracy: 0.3541 - val_loss: 2.0775 - val_accuracy: 0.3710\n",
      "Epoch 29/100\n",
      "1172/1172 [==============================] - 13s 12ms/step - loss: 1.7574 - accuracy: 0.3586 - val_loss: 1.9753 - val_accuracy: 0.3755\n",
      "Epoch 30/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.7550 - accuracy: 0.3605 - val_loss: 2.0270 - val_accuracy: 0.3762\n",
      "Epoch 31/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.7531 - accuracy: 0.3633 - val_loss: 2.1007 - val_accuracy: 0.3689\n",
      "Epoch 32/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.7421 - accuracy: 0.3711 - val_loss: 1.9769 - val_accuracy: 0.3844\n",
      "Logged in: logs/fit/selu_alpha_lower_0.0001_20220625-175751\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 1.9769 - accuracy: 0.3844\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.9431 - accuracy: 0.3875\n",
      "fitting lr = 5e-05 Logged in: logs/fit/selu_alpha_lower_5e-05_20220625-180518\n",
      "Epoch 1/100\n",
      "   2/1172 [..............................] - ETA: 28:52 - loss: 3.0564 - accuracy: 0.0781WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0110s vs `on_train_batch_end` time: 2.9506s). Check your callbacks.\n",
      "1172/1172 [==============================] - 17s 14ms/step - loss: 2.6189 - accuracy: 0.1003 - val_loss: 2.3513 - val_accuracy: 0.1010\n",
      "Epoch 2/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.2915 - accuracy: 0.1429 - val_loss: 2.1150 - val_accuracy: 0.1798\n",
      "Epoch 3/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.1837 - accuracy: 0.1622 - val_loss: 2.0706 - val_accuracy: 0.1924\n",
      "Epoch 4/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.1419 - accuracy: 0.1671 - val_loss: 2.0700 - val_accuracy: 0.1985\n",
      "Epoch 5/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.1196 - accuracy: 0.1742 - val_loss: 2.0768 - val_accuracy: 0.2026\n",
      "Epoch 6/100\n",
      "1172/1172 [==============================] - 13s 12ms/step - loss: 2.0979 - accuracy: 0.1841 - val_loss: 2.0301 - val_accuracy: 0.2135\n",
      "Epoch 7/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.0810 - accuracy: 0.1953 - val_loss: 2.0361 - val_accuracy: 0.2253\n",
      "Epoch 8/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.0534 - accuracy: 0.2109 - val_loss: 2.0206 - val_accuracy: 0.2325\n",
      "Epoch 9/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.0271 - accuracy: 0.2214 - val_loss: 1.9961 - val_accuracy: 0.2485\n",
      "Epoch 10/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.0008 - accuracy: 0.2307 - val_loss: 1.9768 - val_accuracy: 0.2558\n",
      "Epoch 11/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 1.9891 - accuracy: 0.2340 - val_loss: 2.0015 - val_accuracy: 0.2562\n",
      "Epoch 12/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.9784 - accuracy: 0.2410 - val_loss: 1.9944 - val_accuracy: 0.2594\n",
      "Epoch 13/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.9589 - accuracy: 0.2530 - val_loss: 1.9224 - val_accuracy: 0.2837\n",
      "Epoch 14/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.9540 - accuracy: 0.2555 - val_loss: 1.9308 - val_accuracy: 0.3004\n",
      "Epoch 15/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.9421 - accuracy: 0.2637 - val_loss: 1.9284 - val_accuracy: 0.2996\n",
      "Epoch 16/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.9303 - accuracy: 0.2722 - val_loss: 1.9340 - val_accuracy: 0.3038\n",
      "Epoch 17/100\n",
      "1172/1172 [==============================] - 12s 11ms/step - loss: 1.9252 - accuracy: 0.2799 - val_loss: 1.9632 - val_accuracy: 0.3032\n",
      "Epoch 18/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.9162 - accuracy: 0.2819 - val_loss: 1.9241 - val_accuracy: 0.3106\n",
      "Epoch 19/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.9057 - accuracy: 0.2875 - val_loss: 1.9739 - val_accuracy: 0.3134\n",
      "Epoch 20/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.9032 - accuracy: 0.2905 - val_loss: 1.9181 - val_accuracy: 0.3165\n",
      "Epoch 21/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.8956 - accuracy: 0.2899 - val_loss: 1.9384 - val_accuracy: 0.3161\n",
      "Epoch 22/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.8898 - accuracy: 0.2943 - val_loss: 1.9352 - val_accuracy: 0.3220\n",
      "Epoch 23/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.8838 - accuracy: 0.2992 - val_loss: 1.9392 - val_accuracy: 0.3250\n",
      "Epoch 24/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.8876 - accuracy: 0.2971 - val_loss: 1.9760 - val_accuracy: 0.3215\n",
      "Epoch 25/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.8736 - accuracy: 0.3081 - val_loss: 1.9481 - val_accuracy: 0.3230\n",
      "Epoch 26/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.8746 - accuracy: 0.3015 - val_loss: 1.9776 - val_accuracy: 0.3227\n",
      "Epoch 27/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.8708 - accuracy: 0.3083 - val_loss: 1.9594 - val_accuracy: 0.3242\n",
      "Epoch 28/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.8668 - accuracy: 0.3073 - val_loss: 2.0001 - val_accuracy: 0.3273\n",
      "Epoch 29/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.8617 - accuracy: 0.3121 - val_loss: 2.0090 - val_accuracy: 0.3293\n",
      "Epoch 30/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.8562 - accuracy: 0.3136 - val_loss: 2.0666 - val_accuracy: 0.3297\n",
      "Logged in: logs/fit/selu_alpha_lower_5e-05_20220625-180518\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 2.0666 - accuracy: 0.3297\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.0441 - accuracy: 0.3358\n",
      "Best validation performance: 38.44% for lr: 0.0001\n",
      "Best test performance: 38.75% for lr: 0.0001\n",
      "validation accuracy: [0.09455999732017517, 0.10328000038862228, 0.10103999823331833, 0.20032000541687012, 0.2982400059700012, 0.349839985370636, 0.38440001010894775, 0.3296799957752228]\n",
      "test accuracy: [0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.19850000739097595, 0.3077000081539154, 0.35740000009536743, 0.38749998807907104, 0.3357999920845032]\n"
     ]
    }
   ],
   "source": [
    "def selu_alpha_model_lower():\n",
    "    return make_DNN_SD(selu_layer,20,False,0.05)\n",
    "\n",
    "lrs=[0.01, 0.005, 0.0025, 0.001, 0.0005, 0.00025, 0.0001, 0.00005]\n",
    "\n",
    "valid_accuracy,test_accuracy,logs = tune_lrs(selu_alpha_model_lower, lrs, \"selu_alpha_lower\")\n",
    "\n",
    "print(f\"validation accuracy: {valid_accuracy}\")\n",
    "print(f\"test accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting lr = 0.01 Logged in: logs/fit/selu_alpha_even_lower_0.01_20220625-184902\n",
      "Epoch 1/100\n",
      "   2/1172 [..............................] - ETA: 30:40 - loss: 3.7050 - accuracy: 0.1094WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0110s vs `on_train_batch_end` time: 3.1346s). Check your callbacks.\n",
      "1172/1172 [==============================] - 17s 14ms/step - loss: 2.5601 - accuracy: 0.0984 - val_loss: 2.6347 - val_accuracy: 0.1024\n",
      "Epoch 2/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 113155808.0000 - accuracy: 0.0979 - val_loss: 2.6239 - val_accuracy: 0.0970\n",
      "Epoch 3/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 12628.3018 - accuracy: 0.0974 - val_loss: 2.5162 - val_accuracy: 0.1031\n",
      "Epoch 4/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.6899 - accuracy: 0.0998 - val_loss: 2.3431 - val_accuracy: 0.0988\n",
      "Epoch 5/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.5689 - accuracy: 0.1010 - val_loss: 2.3517 - val_accuracy: 0.0946\n",
      "Epoch 6/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.4800 - accuracy: 0.0963 - val_loss: 2.4749 - val_accuracy: 0.1010\n",
      "Epoch 7/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.4060 - accuracy: 0.1009 - val_loss: 2.3766 - val_accuracy: 0.1016\n",
      "Epoch 8/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.3698 - accuracy: 0.0993 - val_loss: 2.4334 - val_accuracy: 0.1033\n",
      "Epoch 9/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.3403 - accuracy: 0.0990 - val_loss: 2.3494 - val_accuracy: 0.0970\n",
      "Epoch 10/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 170.9793 - accuracy: 0.0991 - val_loss: 2.3541 - val_accuracy: 0.1010\n",
      "Epoch 11/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.3081 - accuracy: 0.0997 - val_loss: 2.3561 - val_accuracy: 0.1010\n",
      "Epoch 12/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.3076 - accuracy: 0.1006 - val_loss: 2.3598 - val_accuracy: 0.1010\n",
      "Epoch 13/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.3086 - accuracy: 0.0986 - val_loss: 2.3566 - val_accuracy: 0.1033\n",
      "Epoch 14/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.3087 - accuracy: 0.0994 - val_loss: 2.3402 - val_accuracy: 0.1033\n",
      "Epoch 15/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.3095 - accuracy: 0.1010 - val_loss: 2.3349 - val_accuracy: 0.1033\n",
      "Epoch 16/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.3107 - accuracy: 0.1001 - val_loss: 2.3517 - val_accuracy: 0.1033\n",
      "Epoch 17/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.3131 - accuracy: 0.0989 - val_loss: 2.3339 - val_accuracy: 0.0970\n",
      "Epoch 18/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.3146 - accuracy: 0.1000 - val_loss: 2.3810 - val_accuracy: 0.1033\n",
      "Epoch 19/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.3193 - accuracy: 0.1008 - val_loss: 2.3779 - val_accuracy: 0.1033\n",
      "Epoch 20/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.3205 - accuracy: 0.1005 - val_loss: 2.3703 - val_accuracy: 0.1033\n",
      "Epoch 21/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.3250 - accuracy: 0.0997 - val_loss: 2.3437 - val_accuracy: 0.1033\n",
      "Epoch 22/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.3266 - accuracy: 0.1004 - val_loss: 2.3786 - val_accuracy: 0.1010\n",
      "Epoch 23/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.3316 - accuracy: 0.1002 - val_loss: 2.3952 - val_accuracy: 0.1010\n",
      "Epoch 24/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.3334 - accuracy: 0.0996 - val_loss: 2.3769 - val_accuracy: 0.0970\n",
      "Epoch 25/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.3383 - accuracy: 0.0978 - val_loss: 2.4148 - val_accuracy: 0.1010\n",
      "Epoch 26/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.3392 - accuracy: 0.0996 - val_loss: 2.3985 - val_accuracy: 0.1010\n",
      "Epoch 27/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.3436 - accuracy: 0.0992 - val_loss: 2.4210 - val_accuracy: 0.1033\n",
      "Logged in: logs/fit/selu_alpha_even_lower_0.01_20220625-184902\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 2.4210 - accuracy: 0.1033\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 2.4180 - accuracy: 0.1000\n",
      "fitting lr = 0.005 Logged in: logs/fit/selu_alpha_even_lower_0.005_20220625-185537\n",
      "Epoch 1/100\n",
      "   2/1172 [..............................] - ETA: 30:43 - loss: 3.2041 - accuracy: 0.1094WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0100s vs `on_train_batch_end` time: 3.1416s). Check your callbacks.\n",
      "1172/1172 [==============================] - 17s 14ms/step - loss: 2.4503 - accuracy: 0.1012 - val_loss: 2.3242 - val_accuracy: 0.1031\n",
      "Epoch 2/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 3.3724 - accuracy: 0.0999 - val_loss: 2.3483 - val_accuracy: 0.1031\n",
      "Epoch 3/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.3407 - accuracy: 0.0999 - val_loss: 2.4026 - val_accuracy: 0.0975\n",
      "Epoch 4/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.3451 - accuracy: 0.1003 - val_loss: 2.3975 - val_accuracy: 0.0975\n",
      "Epoch 5/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.3471 - accuracy: 0.1034 - val_loss: 2.3630 - val_accuracy: 0.0970\n",
      "Epoch 6/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.3509 - accuracy: 0.0984 - val_loss: 2.3535 - val_accuracy: 0.0970\n",
      "Epoch 7/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.3514 - accuracy: 0.1001 - val_loss: 2.3510 - val_accuracy: 0.0988\n",
      "Epoch 8/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.3550 - accuracy: 0.0999 - val_loss: 2.3795 - val_accuracy: 0.0946\n",
      "Epoch 9/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.6115 - accuracy: 0.1010 - val_loss: 2.3432 - val_accuracy: 0.1033\n",
      "Epoch 10/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 10.0469 - accuracy: 0.1003 - val_loss: 2.3639 - val_accuracy: 0.0988\n",
      "Epoch 11/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.3717 - accuracy: 0.1018 - val_loss: 2.4108 - val_accuracy: 0.1010\n",
      "Logged in: logs/fit/selu_alpha_even_lower_0.005_20220625-185537\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 2.4108 - accuracy: 0.1010\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 2.4115 - accuracy: 0.1000\n",
      "fitting lr = 0.0025 Logged in: logs/fit/selu_alpha_even_lower_0.0025_20220625-185825\n",
      "Epoch 1/100\n",
      "   2/1172 [..............................] - ETA: 33:06 - loss: 3.0422 - accuracy: 0.1094WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0100s vs `on_train_batch_end` time: 3.3859s). Check your callbacks.\n",
      "1172/1172 [==============================] - 17s 15ms/step - loss: 2.3855 - accuracy: 0.1085 - val_loss: 2.4268 - val_accuracy: 0.1008\n",
      "Epoch 2/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.4082 - accuracy: 0.1213 - val_loss: 2.1617 - val_accuracy: 0.1557\n",
      "Epoch 3/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.1584 - accuracy: 0.1647 - val_loss: 2.1186 - val_accuracy: 0.1879\n",
      "Epoch 4/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.1341 - accuracy: 0.1719 - val_loss: 2.1893 - val_accuracy: 0.1719\n",
      "Epoch 5/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.3212 - accuracy: 0.1235 - val_loss: 2.1525 - val_accuracy: 0.1625\n",
      "Epoch 6/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.1723 - accuracy: 0.1592 - val_loss: 3.0437 - val_accuracy: 0.1491\n",
      "Epoch 7/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.1445 - accuracy: 0.1715 - val_loss: 2.2813 - val_accuracy: 0.1784\n",
      "Epoch 8/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.1342 - accuracy: 0.1707 - val_loss: 2.3778 - val_accuracy: 0.1678\n",
      "Epoch 9/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.1021 - accuracy: 0.1729 - val_loss: 2.0792 - val_accuracy: 0.1912\n",
      "Epoch 10/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.0876 - accuracy: 0.1785 - val_loss: 2.0850 - val_accuracy: 0.1826\n",
      "Epoch 11/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.0847 - accuracy: 0.1833 - val_loss: 2.1134 - val_accuracy: 0.1667\n",
      "Epoch 12/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.0966 - accuracy: 0.1775 - val_loss: 2.0706 - val_accuracy: 0.1835\n",
      "Epoch 13/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.1636 - accuracy: 0.1708 - val_loss: 2.3448 - val_accuracy: 0.1527\n",
      "Epoch 14/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.1000 - accuracy: 0.1728 - val_loss: 2.1039 - val_accuracy: 0.1582\n",
      "Epoch 15/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.0730 - accuracy: 0.1740 - val_loss: 2.0747 - val_accuracy: 0.1657\n",
      "Epoch 16/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.1484 - accuracy: 0.1686 - val_loss: 2.1355 - val_accuracy: 0.1710\n",
      "Epoch 17/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.1012 - accuracy: 0.1779 - val_loss: 2.0814 - val_accuracy: 0.1770\n",
      "Epoch 18/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.2578 - accuracy: 0.1319 - val_loss: 2.3509 - val_accuracy: 0.0946\n",
      "Epoch 19/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.3386 - accuracy: 0.1014 - val_loss: 2.3627 - val_accuracy: 0.1033\n",
      "Epoch 20/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.3370 - accuracy: 0.0985 - val_loss: 2.3433 - val_accuracy: 0.1033\n",
      "Epoch 21/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.3377 - accuracy: 0.1007 - val_loss: 2.3336 - val_accuracy: 0.1010\n",
      "Epoch 22/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.3363 - accuracy: 0.0993 - val_loss: 2.3409 - val_accuracy: 0.1007\n",
      "Logged in: logs/fit/selu_alpha_even_lower_0.0025_20220625-185825\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 2.3409 - accuracy: 0.1007\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 2.3400 - accuracy: 0.1000\n",
      "fitting lr = 0.001 Logged in: logs/fit/selu_alpha_even_lower_0.001_20220625-190352\n",
      "Epoch 1/100\n",
      "   2/1172 [..............................] - ETA: 26:49 - loss: 2.8301 - accuracy: 0.0781WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0090s vs `on_train_batch_end` time: 2.7417s). Check your callbacks.\n",
      "1172/1172 [==============================] - 17s 14ms/step - loss: 2.2197 - accuracy: 0.1543 - val_loss: 2.1805 - val_accuracy: 0.1759\n",
      "Epoch 2/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.0869 - accuracy: 0.1891 - val_loss: 2.2023 - val_accuracy: 0.2102\n",
      "Epoch 3/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.0419 - accuracy: 0.2087 - val_loss: 2.3627 - val_accuracy: 0.2055\n",
      "Epoch 4/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 1.9945 - accuracy: 0.2322 - val_loss: 1.9740 - val_accuracy: 0.2312\n",
      "Epoch 5/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.0417 - accuracy: 0.2122 - val_loss: 2.0738 - val_accuracy: 0.1698\n",
      "Epoch 6/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.0379 - accuracy: 0.2021 - val_loss: 2.0024 - val_accuracy: 0.2207\n",
      "Epoch 7/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.1032 - accuracy: 0.1925 - val_loss: 2.0802 - val_accuracy: 0.1766\n",
      "Epoch 8/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.0354 - accuracy: 0.2160 - val_loss: 2.3475 - val_accuracy: 0.2034\n",
      "Epoch 9/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 1.9708 - accuracy: 0.2415 - val_loss: 2.3271 - val_accuracy: 0.2074\n",
      "Epoch 10/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 1.9515 - accuracy: 0.2500 - val_loss: 1.9873 - val_accuracy: 0.2422\n",
      "Epoch 11/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 1.9491 - accuracy: 0.2513 - val_loss: 2.0302 - val_accuracy: 0.2581\n",
      "Epoch 12/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 1.9280 - accuracy: 0.2666 - val_loss: 2.0463 - val_accuracy: 0.2490\n",
      "Epoch 13/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 1.9305 - accuracy: 0.2626 - val_loss: 2.2777 - val_accuracy: 0.2463\n",
      "Epoch 14/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 1.9120 - accuracy: 0.2677 - val_loss: 2.0452 - val_accuracy: 0.2210\n",
      "Logged in: logs/fit/selu_alpha_even_lower_0.001_20220625-190352\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 2.0452 - accuracy: 0.2210\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.0424 - accuracy: 0.2242\n",
      "fitting lr = 0.0005 Logged in: logs/fit/selu_alpha_even_lower_0.0005_20220625-190725\n",
      "Epoch 1/100\n",
      "   2/1172 [..............................] - ETA: 34:00 - loss: 3.0439 - accuracy: 0.0781WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0110s vs `on_train_batch_end` time: 3.4777s). Check your callbacks.\n",
      "1172/1172 [==============================] - 18s 15ms/step - loss: 2.2017 - accuracy: 0.1674 - val_loss: 2.1195 - val_accuracy: 0.1812\n",
      "Epoch 2/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 1.9921 - accuracy: 0.2440 - val_loss: 1.9331 - val_accuracy: 0.2833\n",
      "Epoch 3/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 1.9260 - accuracy: 0.2757 - val_loss: 1.9532 - val_accuracy: 0.3066\n",
      "Epoch 4/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 1.8840 - accuracy: 0.3029 - val_loss: 1.8491 - val_accuracy: 0.3315\n",
      "Epoch 5/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 1.8513 - accuracy: 0.3211 - val_loss: 1.8467 - val_accuracy: 0.3487\n",
      "Epoch 6/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 1.8188 - accuracy: 0.3377 - val_loss: 1.8302 - val_accuracy: 0.3582\n",
      "Epoch 7/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 1.7920 - accuracy: 0.3526 - val_loss: 1.9072 - val_accuracy: 0.3706\n",
      "Epoch 8/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 1.7709 - accuracy: 0.3618 - val_loss: 1.8456 - val_accuracy: 0.3579\n",
      "Epoch 9/100\n",
      "1172/1172 [==============================] - 15s 12ms/step - loss: 1.7580 - accuracy: 0.3651 - val_loss: 1.7315 - val_accuracy: 0.3893\n",
      "Epoch 10/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 1.7361 - accuracy: 0.3724 - val_loss: 1.7926 - val_accuracy: 0.3891\n",
      "Epoch 11/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 1.7250 - accuracy: 0.3773 - val_loss: 1.7238 - val_accuracy: 0.3988\n",
      "Epoch 12/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 1.7082 - accuracy: 0.3850 - val_loss: 1.8064 - val_accuracy: 0.4002\n",
      "Epoch 13/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 1.6975 - accuracy: 0.3905 - val_loss: 1.8418 - val_accuracy: 0.4029\n",
      "Epoch 14/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 1.6837 - accuracy: 0.3977 - val_loss: 1.7297 - val_accuracy: 0.4015\n",
      "Epoch 15/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 1.6699 - accuracy: 0.4020 - val_loss: 1.7096 - val_accuracy: 0.4004\n",
      "Epoch 16/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 1.6695 - accuracy: 0.4008 - val_loss: 1.7036 - val_accuracy: 0.4158\n",
      "Epoch 17/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 1.6587 - accuracy: 0.4058 - val_loss: 1.7030 - val_accuracy: 0.4148\n",
      "Epoch 18/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 1.6491 - accuracy: 0.4106 - val_loss: 1.7536 - val_accuracy: 0.4125\n",
      "Epoch 19/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 1.6389 - accuracy: 0.4175 - val_loss: 1.7318 - val_accuracy: 0.4190\n",
      "Epoch 20/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 1.6340 - accuracy: 0.4187 - val_loss: 1.6374 - val_accuracy: 0.4359\n",
      "Epoch 21/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 1.6248 - accuracy: 0.4189 - val_loss: 1.7028 - val_accuracy: 0.4367\n",
      "Epoch 22/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 1.6216 - accuracy: 0.4214 - val_loss: 1.7105 - val_accuracy: 0.4471\n",
      "Epoch 23/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 1.6145 - accuracy: 0.4249 - val_loss: 1.6858 - val_accuracy: 0.4309\n",
      "Epoch 24/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 1.6098 - accuracy: 0.4267 - val_loss: 1.7239 - val_accuracy: 0.4290\n",
      "Epoch 25/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 1.6003 - accuracy: 0.4285 - val_loss: 1.7227 - val_accuracy: 0.4226\n",
      "Epoch 26/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 1.5947 - accuracy: 0.4326 - val_loss: 1.6835 - val_accuracy: 0.4462\n",
      "Epoch 27/100\n",
      "1172/1172 [==============================] - 15s 13ms/step - loss: 1.5968 - accuracy: 0.4329 - val_loss: 1.6259 - val_accuracy: 0.4430\n",
      "Epoch 28/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 1.5822 - accuracy: 0.4373 - val_loss: 1.7082 - val_accuracy: 0.4395\n",
      "Epoch 29/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 1.5779 - accuracy: 0.4405 - val_loss: 1.6581 - val_accuracy: 0.4492\n",
      "Epoch 30/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 1.5828 - accuracy: 0.4381 - val_loss: 1.6910 - val_accuracy: 0.4446\n",
      "Epoch 31/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 1.5643 - accuracy: 0.4451 - val_loss: 1.8024 - val_accuracy: 0.4305\n",
      "Epoch 32/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 1.5623 - accuracy: 0.4472 - val_loss: 1.6843 - val_accuracy: 0.4462\n",
      "Epoch 33/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5770 - accuracy: 0.4424 - val_loss: 1.6778 - val_accuracy: 0.4358\n",
      "Epoch 34/100\n",
      "1172/1172 [==============================] - 12s 11ms/step - loss: 1.5518 - accuracy: 0.4501 - val_loss: 1.7813 - val_accuracy: 0.4479\n",
      "Epoch 35/100\n",
      "1172/1172 [==============================] - 12s 11ms/step - loss: 1.5981 - accuracy: 0.4386 - val_loss: 1.7174 - val_accuracy: 0.4310\n",
      "Epoch 36/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5526 - accuracy: 0.4469 - val_loss: 1.6826 - val_accuracy: 0.4534\n",
      "Epoch 37/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.7614 - accuracy: 0.3921 - val_loss: 1.8232 - val_accuracy: 0.4044\n",
      "Logged in: logs/fit/selu_alpha_even_lower_0.0005_20220625-190725\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 1.8232 - accuracy: 0.4044\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 1.8246 - accuracy: 0.4083\n",
      "fitting lr = 0.00025 Logged in: logs/fit/selu_alpha_even_lower_0.00025_20220625-191623\n",
      "Epoch 1/100\n",
      "   2/1172 [..............................] - ETA: 27:18 - loss: 3.1194 - accuracy: 0.1094WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0089s vs `on_train_batch_end` time: 2.7925s). Check your callbacks.\n",
      "1172/1172 [==============================] - 15s 13ms/step - loss: 2.2321 - accuracy: 0.1718 - val_loss: 2.0422 - val_accuracy: 0.2314\n",
      "Epoch 2/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.0125 - accuracy: 0.2358 - val_loss: 1.9153 - val_accuracy: 0.2831\n",
      "Epoch 3/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.9509 - accuracy: 0.2646 - val_loss: 1.8869 - val_accuracy: 0.3039\n",
      "Epoch 4/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.8974 - accuracy: 0.2955 - val_loss: 1.8695 - val_accuracy: 0.3346\n",
      "Epoch 5/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.8574 - accuracy: 0.3127 - val_loss: 1.9277 - val_accuracy: 0.3333\n",
      "Epoch 6/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.8339 - accuracy: 0.3286 - val_loss: 1.8511 - val_accuracy: 0.3486\n",
      "Epoch 7/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.7987 - accuracy: 0.3442 - val_loss: 1.8168 - val_accuracy: 0.3673\n",
      "Epoch 8/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.7659 - accuracy: 0.3626 - val_loss: 1.7367 - val_accuracy: 0.3935\n",
      "Epoch 9/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.7332 - accuracy: 0.3735 - val_loss: 1.9067 - val_accuracy: 0.3756\n",
      "Epoch 10/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.7190 - accuracy: 0.3789 - val_loss: 1.6845 - val_accuracy: 0.4048\n",
      "Epoch 11/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.7000 - accuracy: 0.3848 - val_loss: 1.7153 - val_accuracy: 0.4134\n",
      "Epoch 12/100\n",
      "1172/1172 [==============================] - 12s 11ms/step - loss: 1.6818 - accuracy: 0.3958 - val_loss: 1.7297 - val_accuracy: 0.3974\n",
      "Epoch 13/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.6713 - accuracy: 0.3978 - val_loss: 1.7064 - val_accuracy: 0.4268\n",
      "Epoch 14/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.6588 - accuracy: 0.4047 - val_loss: 1.6826 - val_accuracy: 0.4247\n",
      "Epoch 15/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.6454 - accuracy: 0.4096 - val_loss: 1.6955 - val_accuracy: 0.4210\n",
      "Epoch 16/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.6292 - accuracy: 0.4158 - val_loss: 1.6749 - val_accuracy: 0.4266\n",
      "Epoch 17/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.6215 - accuracy: 0.4213 - val_loss: 1.6448 - val_accuracy: 0.4344\n",
      "Epoch 18/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.6117 - accuracy: 0.4228 - val_loss: 1.7188 - val_accuracy: 0.4226\n",
      "Epoch 19/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.6116 - accuracy: 0.4233 - val_loss: 1.7130 - val_accuracy: 0.4429\n",
      "Epoch 20/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5911 - accuracy: 0.4305 - val_loss: 1.5819 - val_accuracy: 0.4550\n",
      "Epoch 21/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5823 - accuracy: 0.4331 - val_loss: 1.6347 - val_accuracy: 0.4479\n",
      "Epoch 22/100\n",
      "1172/1172 [==============================] - 12s 11ms/step - loss: 1.5806 - accuracy: 0.4344 - val_loss: 1.6802 - val_accuracy: 0.4449\n",
      "Epoch 23/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5688 - accuracy: 0.4373 - val_loss: 1.6650 - val_accuracy: 0.4399\n",
      "Epoch 24/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5601 - accuracy: 0.4427 - val_loss: 1.6484 - val_accuracy: 0.4621\n",
      "Epoch 25/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5563 - accuracy: 0.4435 - val_loss: 1.6309 - val_accuracy: 0.4575\n",
      "Epoch 26/100\n",
      "1172/1172 [==============================] - 12s 11ms/step - loss: 1.5483 - accuracy: 0.4486 - val_loss: 1.6382 - val_accuracy: 0.4618\n",
      "Epoch 27/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5428 - accuracy: 0.4490 - val_loss: 1.5906 - val_accuracy: 0.4679\n",
      "Epoch 28/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5314 - accuracy: 0.4544 - val_loss: 1.6347 - val_accuracy: 0.4552\n",
      "Epoch 29/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5317 - accuracy: 0.4511 - val_loss: 1.6343 - val_accuracy: 0.4682\n",
      "Epoch 30/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5183 - accuracy: 0.4554 - val_loss: 1.6470 - val_accuracy: 0.4612\n",
      "Logged in: logs/fit/selu_alpha_even_lower_0.00025_20220625-191623\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 1.6470 - accuracy: 0.4612\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 1.6222 - accuracy: 0.4750\n",
      "fitting lr = 0.0001 Logged in: logs/fit/selu_alpha_even_lower_0.0001_20220625-192258\n",
      "Epoch 1/100\n",
      "   2/1172 [..............................] - ETA: 23:58 - loss: 3.2159 - accuracy: 0.0938WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0110s vs `on_train_batch_end` time: 2.4465s). Check your callbacks.\n",
      "1172/1172 [==============================] - 15s 13ms/step - loss: 2.3156 - accuracy: 0.1513 - val_loss: 2.0737 - val_accuracy: 0.2012\n",
      "Epoch 2/100\n",
      "1172/1172 [==============================] - 12s 11ms/step - loss: 2.1199 - accuracy: 0.1912 - val_loss: 1.9463 - val_accuracy: 0.2557\n",
      "Epoch 3/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.0057 - accuracy: 0.2480 - val_loss: 1.8842 - val_accuracy: 0.3097\n",
      "Epoch 4/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.9419 - accuracy: 0.2771 - val_loss: 1.8457 - val_accuracy: 0.3241\n",
      "Epoch 5/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.9000 - accuracy: 0.2935 - val_loss: 1.8167 - val_accuracy: 0.3313\n",
      "Epoch 6/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.8711 - accuracy: 0.3115 - val_loss: 1.8218 - val_accuracy: 0.3430\n",
      "Epoch 7/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.8463 - accuracy: 0.3220 - val_loss: 1.7707 - val_accuracy: 0.3666\n",
      "Epoch 8/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.8179 - accuracy: 0.3381 - val_loss: 1.7512 - val_accuracy: 0.3627\n",
      "Epoch 9/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.7969 - accuracy: 0.3470 - val_loss: 1.7586 - val_accuracy: 0.3771\n",
      "Epoch 10/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.7775 - accuracy: 0.3527 - val_loss: 1.7235 - val_accuracy: 0.3878\n",
      "Epoch 11/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.7600 - accuracy: 0.3595 - val_loss: 1.7037 - val_accuracy: 0.3965\n",
      "Epoch 12/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.7416 - accuracy: 0.3687 - val_loss: 1.6881 - val_accuracy: 0.3950\n",
      "Epoch 13/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.7278 - accuracy: 0.3787 - val_loss: 1.6567 - val_accuracy: 0.4128\n",
      "Epoch 14/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.7112 - accuracy: 0.3825 - val_loss: 1.7024 - val_accuracy: 0.4025\n",
      "Epoch 15/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.6975 - accuracy: 0.3860 - val_loss: 1.6475 - val_accuracy: 0.4138\n",
      "Epoch 16/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.6878 - accuracy: 0.3915 - val_loss: 1.6497 - val_accuracy: 0.4207\n",
      "Epoch 17/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.6817 - accuracy: 0.3936 - val_loss: 1.6068 - val_accuracy: 0.4277\n",
      "Epoch 18/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.6707 - accuracy: 0.3976 - val_loss: 1.6095 - val_accuracy: 0.4278\n",
      "Epoch 19/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.6642 - accuracy: 0.4027 - val_loss: 1.6497 - val_accuracy: 0.4203\n",
      "Epoch 20/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.6472 - accuracy: 0.4089 - val_loss: 1.5961 - val_accuracy: 0.4332\n",
      "Epoch 21/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.6390 - accuracy: 0.4123 - val_loss: 1.6118 - val_accuracy: 0.4350\n",
      "Epoch 22/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.6380 - accuracy: 0.4136 - val_loss: 1.5685 - val_accuracy: 0.4418\n",
      "Epoch 23/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.6268 - accuracy: 0.4151 - val_loss: 1.6207 - val_accuracy: 0.4370\n",
      "Epoch 24/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.6211 - accuracy: 0.4196 - val_loss: 1.5972 - val_accuracy: 0.4431\n",
      "Epoch 25/100\n",
      "1172/1172 [==============================] - 12s 11ms/step - loss: 1.6074 - accuracy: 0.4230 - val_loss: 1.6223 - val_accuracy: 0.4346\n",
      "Epoch 26/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.6042 - accuracy: 0.4236 - val_loss: 1.5762 - val_accuracy: 0.4503\n",
      "Epoch 27/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5988 - accuracy: 0.4255 - val_loss: 1.5989 - val_accuracy: 0.4466\n",
      "Epoch 28/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5924 - accuracy: 0.4276 - val_loss: 1.5891 - val_accuracy: 0.4441\n",
      "Epoch 29/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5822 - accuracy: 0.4310 - val_loss: 1.5874 - val_accuracy: 0.4455\n",
      "Epoch 30/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5769 - accuracy: 0.4344 - val_loss: 1.5768 - val_accuracy: 0.4525\n",
      "Epoch 31/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5757 - accuracy: 0.4358 - val_loss: 1.6057 - val_accuracy: 0.4497\n",
      "Epoch 32/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5732 - accuracy: 0.4369 - val_loss: 1.5638 - val_accuracy: 0.4554\n",
      "Epoch 33/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5628 - accuracy: 0.4407 - val_loss: 1.5610 - val_accuracy: 0.4508\n",
      "Epoch 34/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5587 - accuracy: 0.4439 - val_loss: 1.5394 - val_accuracy: 0.4626\n",
      "Epoch 35/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5525 - accuracy: 0.4406 - val_loss: 1.5434 - val_accuracy: 0.4608\n",
      "Epoch 36/100\n",
      "1172/1172 [==============================] - 12s 11ms/step - loss: 1.5475 - accuracy: 0.4457 - val_loss: 1.5775 - val_accuracy: 0.4562\n",
      "Epoch 37/100\n",
      "1172/1172 [==============================] - 12s 11ms/step - loss: 1.5416 - accuracy: 0.4509 - val_loss: 1.5817 - val_accuracy: 0.4586\n",
      "Epoch 38/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5386 - accuracy: 0.4485 - val_loss: 1.5432 - val_accuracy: 0.4633\n",
      "Epoch 39/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5304 - accuracy: 0.4525 - val_loss: 1.5336 - val_accuracy: 0.4651\n",
      "Epoch 40/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5261 - accuracy: 0.4542 - val_loss: 1.5522 - val_accuracy: 0.4690\n",
      "Epoch 41/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5209 - accuracy: 0.4530 - val_loss: 1.5713 - val_accuracy: 0.4675\n",
      "Epoch 42/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5149 - accuracy: 0.4583 - val_loss: 1.5644 - val_accuracy: 0.4701\n",
      "Epoch 43/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5154 - accuracy: 0.4573 - val_loss: 1.5811 - val_accuracy: 0.4648\n",
      "Epoch 44/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5080 - accuracy: 0.4598 - val_loss: 1.5243 - val_accuracy: 0.4706\n",
      "Epoch 45/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5024 - accuracy: 0.4627 - val_loss: 1.5336 - val_accuracy: 0.4701\n",
      "Epoch 46/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.4989 - accuracy: 0.4639 - val_loss: 1.5190 - val_accuracy: 0.4800\n",
      "Epoch 47/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5024 - accuracy: 0.4630 - val_loss: 1.5202 - val_accuracy: 0.4760\n",
      "Epoch 48/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.4910 - accuracy: 0.4675 - val_loss: 1.5253 - val_accuracy: 0.4738\n",
      "Epoch 49/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.4848 - accuracy: 0.4710 - val_loss: 1.5410 - val_accuracy: 0.4722\n",
      "Epoch 50/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.4848 - accuracy: 0.4697 - val_loss: 1.5635 - val_accuracy: 0.4714\n",
      "Epoch 51/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.4767 - accuracy: 0.4745 - val_loss: 1.5254 - val_accuracy: 0.4818\n",
      "Epoch 52/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.4767 - accuracy: 0.4734 - val_loss: 1.5301 - val_accuracy: 0.4785\n",
      "Epoch 53/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.4681 - accuracy: 0.4739 - val_loss: 1.5197 - val_accuracy: 0.4806\n",
      "Epoch 54/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.4669 - accuracy: 0.4748 - val_loss: 1.5219 - val_accuracy: 0.4762\n",
      "Epoch 55/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.4679 - accuracy: 0.4741 - val_loss: 1.5328 - val_accuracy: 0.4802\n",
      "Epoch 56/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.4615 - accuracy: 0.4758 - val_loss: 1.5220 - val_accuracy: 0.4813\n",
      "Logged in: logs/fit/selu_alpha_even_lower_0.0001_20220625-192258\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 1.5220 - accuracy: 0.4813\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 1.5005 - accuracy: 0.4911\n",
      "fitting lr = 5e-05 Logged in: logs/fit/selu_alpha_even_lower_5e-05_20220625-193505\n",
      "Epoch 1/100\n",
      "   2/1172 [..............................] - ETA: 25:05 - loss: 2.7428 - accuracy: 0.1562WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0110s vs `on_train_batch_end` time: 2.5621s). Check your callbacks.\n",
      "1172/1172 [==============================] - 16s 13ms/step - loss: 2.3814 - accuracy: 0.1437 - val_loss: 2.0688 - val_accuracy: 0.2017\n",
      "Epoch 2/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.1505 - accuracy: 0.1948 - val_loss: 1.9673 - val_accuracy: 0.2582\n",
      "Epoch 3/100\n",
      "1172/1172 [==============================] - 13s 12ms/step - loss: 2.0564 - accuracy: 0.2258 - val_loss: 1.9492 - val_accuracy: 0.2667\n",
      "Epoch 4/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.0207 - accuracy: 0.2394 - val_loss: 1.9277 - val_accuracy: 0.2889\n",
      "Epoch 5/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 1.9887 - accuracy: 0.2554 - val_loss: 1.8906 - val_accuracy: 0.3092\n",
      "Epoch 6/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 1.9554 - accuracy: 0.2717 - val_loss: 1.8368 - val_accuracy: 0.3309\n",
      "Epoch 7/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 1.9229 - accuracy: 0.2934 - val_loss: 1.8172 - val_accuracy: 0.3430\n",
      "Epoch 8/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.8947 - accuracy: 0.3074 - val_loss: 1.7893 - val_accuracy: 0.3573\n",
      "Epoch 9/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.8699 - accuracy: 0.3214 - val_loss: 1.7806 - val_accuracy: 0.3597\n",
      "Epoch 10/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.8489 - accuracy: 0.3298 - val_loss: 1.7498 - val_accuracy: 0.3705\n",
      "Epoch 11/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.8328 - accuracy: 0.3356 - val_loss: 1.7378 - val_accuracy: 0.3757\n",
      "Epoch 12/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.8175 - accuracy: 0.3386 - val_loss: 1.7171 - val_accuracy: 0.3777\n",
      "Epoch 13/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.8066 - accuracy: 0.3450 - val_loss: 1.7485 - val_accuracy: 0.3823\n",
      "Epoch 14/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.7965 - accuracy: 0.3503 - val_loss: 1.7173 - val_accuracy: 0.3873\n",
      "Epoch 15/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.7787 - accuracy: 0.3541 - val_loss: 1.6957 - val_accuracy: 0.3918\n",
      "Epoch 16/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.7730 - accuracy: 0.3579 - val_loss: 1.6736 - val_accuracy: 0.3986\n",
      "Epoch 17/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.7596 - accuracy: 0.3653 - val_loss: 1.7034 - val_accuracy: 0.3985\n",
      "Epoch 18/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.7448 - accuracy: 0.3700 - val_loss: 1.6944 - val_accuracy: 0.4010\n",
      "Epoch 19/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.7381 - accuracy: 0.3713 - val_loss: 1.6663 - val_accuracy: 0.4058\n",
      "Epoch 20/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.7308 - accuracy: 0.3753 - val_loss: 1.6444 - val_accuracy: 0.4095\n",
      "Epoch 21/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.7256 - accuracy: 0.3776 - val_loss: 1.6478 - val_accuracy: 0.4143\n",
      "Epoch 22/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.7144 - accuracy: 0.3831 - val_loss: 1.6617 - val_accuracy: 0.4072\n",
      "Epoch 23/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.7061 - accuracy: 0.3851 - val_loss: 1.6349 - val_accuracy: 0.4179\n",
      "Epoch 24/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.6990 - accuracy: 0.3891 - val_loss: 1.6445 - val_accuracy: 0.4225\n",
      "Epoch 25/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.6931 - accuracy: 0.3897 - val_loss: 1.6582 - val_accuracy: 0.4187\n",
      "Epoch 26/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.6808 - accuracy: 0.3923 - val_loss: 1.6364 - val_accuracy: 0.4206\n",
      "Epoch 27/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.6787 - accuracy: 0.3945 - val_loss: 1.6207 - val_accuracy: 0.4250\n",
      "Epoch 28/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.6766 - accuracy: 0.3970 - val_loss: 1.6353 - val_accuracy: 0.4246\n",
      "Epoch 29/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.6615 - accuracy: 0.4015 - val_loss: 1.6118 - val_accuracy: 0.4296\n",
      "Epoch 30/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.6615 - accuracy: 0.4051 - val_loss: 1.6092 - val_accuracy: 0.4348\n",
      "Epoch 31/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.6532 - accuracy: 0.4066 - val_loss: 1.6341 - val_accuracy: 0.4268\n",
      "Epoch 32/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.6448 - accuracy: 0.4096 - val_loss: 1.6026 - val_accuracy: 0.4338\n",
      "Epoch 33/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.6397 - accuracy: 0.4107 - val_loss: 1.6028 - val_accuracy: 0.4387\n",
      "Epoch 34/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.6391 - accuracy: 0.4109 - val_loss: 1.6002 - val_accuracy: 0.4449\n",
      "Epoch 35/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.6301 - accuracy: 0.4138 - val_loss: 1.5841 - val_accuracy: 0.4461\n",
      "Epoch 36/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.6284 - accuracy: 0.4128 - val_loss: 1.6025 - val_accuracy: 0.4433\n",
      "Epoch 37/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.6255 - accuracy: 0.4182 - val_loss: 1.5760 - val_accuracy: 0.4473\n",
      "Epoch 38/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.6202 - accuracy: 0.4210 - val_loss: 1.5725 - val_accuracy: 0.4502\n",
      "Epoch 39/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.6128 - accuracy: 0.4221 - val_loss: 1.5606 - val_accuracy: 0.4459\n",
      "Epoch 40/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.6099 - accuracy: 0.4235 - val_loss: 1.5868 - val_accuracy: 0.4509\n",
      "Epoch 41/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.6001 - accuracy: 0.4229 - val_loss: 1.5767 - val_accuracy: 0.4450\n",
      "Epoch 42/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5998 - accuracy: 0.4286 - val_loss: 1.5742 - val_accuracy: 0.4539\n",
      "Epoch 43/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5982 - accuracy: 0.4281 - val_loss: 1.5769 - val_accuracy: 0.4550\n",
      "Epoch 44/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5942 - accuracy: 0.4280 - val_loss: 1.5753 - val_accuracy: 0.4506\n",
      "Epoch 45/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5866 - accuracy: 0.4317 - val_loss: 1.5843 - val_accuracy: 0.4582\n",
      "Epoch 46/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5870 - accuracy: 0.4332 - val_loss: 1.5486 - val_accuracy: 0.4584\n",
      "Epoch 47/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5768 - accuracy: 0.4367 - val_loss: 1.5934 - val_accuracy: 0.4522\n",
      "Epoch 48/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5757 - accuracy: 0.4362 - val_loss: 1.5859 - val_accuracy: 0.4568\n",
      "Epoch 49/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5701 - accuracy: 0.4379 - val_loss: 1.5603 - val_accuracy: 0.4545\n",
      "Epoch 50/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5692 - accuracy: 0.4374 - val_loss: 1.5569 - val_accuracy: 0.4631\n",
      "Epoch 51/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5628 - accuracy: 0.4403 - val_loss: 1.5597 - val_accuracy: 0.4648\n",
      "Epoch 52/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5645 - accuracy: 0.4395 - val_loss: 1.5559 - val_accuracy: 0.4646\n",
      "Epoch 53/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5576 - accuracy: 0.4407 - val_loss: 1.5768 - val_accuracy: 0.4571\n",
      "Epoch 54/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5533 - accuracy: 0.4424 - val_loss: 1.5388 - val_accuracy: 0.4639\n",
      "Epoch 55/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5484 - accuracy: 0.4480 - val_loss: 1.5244 - val_accuracy: 0.4722\n",
      "Epoch 56/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5531 - accuracy: 0.4464 - val_loss: 1.5576 - val_accuracy: 0.4654\n",
      "Epoch 57/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5470 - accuracy: 0.4471 - val_loss: 1.5716 - val_accuracy: 0.4599\n",
      "Epoch 58/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5443 - accuracy: 0.4470 - val_loss: 1.5614 - val_accuracy: 0.4686\n",
      "Epoch 59/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5372 - accuracy: 0.4496 - val_loss: 1.5308 - val_accuracy: 0.4694\n",
      "Epoch 60/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5375 - accuracy: 0.4472 - val_loss: 1.5540 - val_accuracy: 0.4679\n",
      "Epoch 61/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5334 - accuracy: 0.4488 - val_loss: 1.5558 - val_accuracy: 0.4675\n",
      "Epoch 62/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5318 - accuracy: 0.4518 - val_loss: 1.5216 - val_accuracy: 0.4722\n",
      "Epoch 63/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5261 - accuracy: 0.4540 - val_loss: 1.5271 - val_accuracy: 0.4748\n",
      "Epoch 64/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5207 - accuracy: 0.4551 - val_loss: 1.5100 - val_accuracy: 0.4722\n",
      "Epoch 65/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5201 - accuracy: 0.4589 - val_loss: 1.5525 - val_accuracy: 0.4672\n",
      "Epoch 66/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5212 - accuracy: 0.4561 - val_loss: 1.5096 - val_accuracy: 0.4766\n",
      "Epoch 67/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5183 - accuracy: 0.4553 - val_loss: 1.4908 - val_accuracy: 0.4752\n",
      "Epoch 68/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5146 - accuracy: 0.4598 - val_loss: 1.5388 - val_accuracy: 0.4674\n",
      "Epoch 69/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5118 - accuracy: 0.4608 - val_loss: 1.5414 - val_accuracy: 0.4725\n",
      "Epoch 70/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5134 - accuracy: 0.4621 - val_loss: 1.5299 - val_accuracy: 0.4748\n",
      "Epoch 71/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.5074 - accuracy: 0.4619 - val_loss: 1.5522 - val_accuracy: 0.4743\n",
      "Epoch 72/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.5021 - accuracy: 0.4615 - val_loss: 1.5208 - val_accuracy: 0.4786\n",
      "Epoch 73/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.5037 - accuracy: 0.4619 - val_loss: 1.5199 - val_accuracy: 0.4799\n",
      "Epoch 74/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.4989 - accuracy: 0.4650 - val_loss: 1.5041 - val_accuracy: 0.4804\n",
      "Epoch 75/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.4906 - accuracy: 0.4674 - val_loss: 1.4938 - val_accuracy: 0.4794\n",
      "Epoch 76/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.4957 - accuracy: 0.4635 - val_loss: 1.5206 - val_accuracy: 0.4798\n",
      "Epoch 77/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.4908 - accuracy: 0.4656 - val_loss: 1.5027 - val_accuracy: 0.4799\n",
      "Logged in: logs/fit/selu_alpha_even_lower_5e-05_20220625-193505\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 1.5027 - accuracy: 0.4799\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 1.4937 - accuracy: 0.4868\n",
      "Best validation performance: 48.13% for lr: 0.0001\n",
      "Best test performance: 49.11% for lr: 0.0001\n",
      "validation accuracy: [0.10328000038862228, 0.10103999823331833, 0.10072000324726105, 0.22103999555110931, 0.4043999910354614, 0.4611999988555908, 0.48127999901771545, 0.47991999983787537]\n",
      "test accuracy: [0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.22419999539852142, 0.4083000123500824, 0.4749999940395355, 0.491100013256073, 0.4867999851703644]\n"
     ]
    }
   ],
   "source": [
    "def selu_alpha_model_even_lower():\n",
    "    return make_DNN_SD(selu_layer,20,False,0.025)\n",
    "\n",
    "lrs=[0.01, 0.005, 0.0025, 0.001, 0.0005, 0.00025, 0.0001, 0.00005]\n",
    "\n",
    "valid_accuracy,test_accuracy,logs = tune_lrs(selu_alpha_model_even_lower, lrs, \"selu_alpha_even_lower\")\n",
    "\n",
    "print(f\"validation accuracy: {valid_accuracy}\")\n",
    "print(f\"test accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 1s 2ms/step - loss: 1.5190 - accuracy: 0.4800\n",
      "validation accuracy: 48.00%\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 1.4999 - accuracy: 0.4875\n",
      "test accuracy 48.75%\n"
     ]
    }
   ],
   "source": [
    "logs\n",
    "#best was 0.0001 \"selu_alpha_even_lower_0.0001_model.h5\"\n",
    "def selu_alpha_model_even_lower():\n",
    "    return make_DNN_SD(selu_layer,20,False,0.025)\n",
    "\n",
    "\n",
    "model = selu_alpha_model_even_lower()\n",
    "model.compile(optimizer = keras.optimizers.Nadam(learning_rate=0.0001) , loss='sparse_categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "model.load_weights(\"selu_alpha_even_lower_0.0001_model.h5\")\n",
    "#best_model = keras.models.load_model(\"cifar10_elu_he_0.000_model.h5\")\n",
    "print(f\"validation accuracy: {model.evaluate(X_valid,y_valid)[1]:.2%}\")\n",
    "print(f\"test accuracy {model.evaluate(X_test,y_test)[1]:.2%}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48248\n"
     ]
    }
   ],
   "source": [
    "#MC_dropout\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#This is softmax classification so prediction will be prtobabilities for the 10 types\n",
    "y_preds = np.stack([model(X_valid,training=True) for sample in range(100)])\n",
    "y_pred = y_preds.mean(axis=0)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "accuracy = np.sum(y_pred == y_valid)/len(y_valid)\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('PandasNumpyMathplotlib')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40a6d3daba109dd36035c486cbe134237beb3103005e5f3b9c526cde33e5461d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
