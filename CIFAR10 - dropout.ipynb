{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#TODO: remove the following hack from https://stackoverflow.com/questions/69687794/unable-to-manually-load-cifar10-dataset\n",
    "#import ssl\n",
    "#ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "cifar10 = keras.datasets.cifar10\n",
    "(X_train_full, y_train_full), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "#Scale the pixel values\n",
    "X_train_full, X_test = X_train_full/255.0, X_test/255.0\n",
    "y_train_full, y_test = y_train_full.flatten(), y_test.flatten()\n",
    "\n",
    "X_train,X_valid,y_train,y_valid = train_test_split(X_train_full,y_train_full, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 22460), started 0:38:55 ago. (Use '!kill 22460' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-8719606d2349de11\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-8719606d2349de11\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "\n",
    "%tensorboard --logdir logs/fit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((37500, 32, 32, 3), (37500,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises from Geron chapter 11\n",
    "Nadam with 20 layers of 100 neurons, He initialization and  Elu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "elu_layer = partial(keras.layers.Dense,kernel_initializer=\"he_normal\", activation=\"elu\")\n",
    "\n",
    "\n",
    "def make_DNN_dropout (layer_function, n_hidden,use_normalization=False, dropout_rate=-1):\n",
    "    architecture = [keras.layers.Flatten(input_shape=[32,32,3])]\n",
    "    for _ in range(n_hidden):\n",
    "        architecture.append(layer_function(100))\n",
    "        if use_normalization:\n",
    "            architecture.append(keras.layers.BatchNormalization())\n",
    "        if dropout_rate > -1:\n",
    "            architecture.append(keras.layers.Dropout(dropout_rate))\n",
    "    architecture.append(keras.layers.Dense(10,activation=\"softmax\"))\n",
    "    return(keras.models.Sequential(architecture))\n",
    "\n",
    "def dropout_model():\n",
    "    return make_DNN_dropout(elu_layer,20,False,0.2)\n",
    "\n",
    "#First run used the values that worked well for he & elu without dropout - performed badly -  best validn accuracy was 22.34%\n",
    "#lrs=[0.001, 0.0005, 0.00025, 0.0001, 0.00005] \n",
    "#Tried 0.1 and 0.05 and  but both failed because of instability causing tensorboard: \"InvalidArgumentError: Nan in summary histogram for\"\n",
    "#lrs=[0.01, 0.025, 0.05, 0.075] #0.01 => 9.75% 0.025 failed in tensorboard\n",
    "# lrs=[0.05, 0.075]\n",
    "\n",
    "# valid_accuracy,test_accuracy,logs = tune_lrs(dropout_model, lrs, \"dropout\")\n",
    "\n",
    "# print(f\"validation accuracy: {valid_accuracy}\")\n",
    "# print(f\"test accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_lrs(model_function, lrs, prefix):\n",
    "\n",
    "    import datetime\n",
    "    import numpy as np\n",
    "\n",
    "    valid_accuracy=[]\n",
    "    test_accuracy=[]\n",
    "    logs=[]\n",
    "    best_models=[]\n",
    "    for lr in lrs:\n",
    "        model = model_function()\n",
    "\n",
    "        log_dir = \"logs/fit/\" + f\"{prefix}_{lr}_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "        checkpoint_file = \"models/\" + f\"{prefix}_{lr}_model.h5\"\n",
    "        tensorboard_cb = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "        #since y_train is numeric rarther than one-hot need to use \"SPARSE_categorical_cross_entropy\"\n",
    "        #model.compile(optimizer='nadam', loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "        model.compile(optimizer = keras.optimizers.Nadam(learning_rate=lr) , loss='sparse_categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "        logs.append(log_dir)\n",
    "\n",
    "\n",
    "        print(f\"fitting lr = {lr} Logged in: {log_dir} saved in {checkpoint_file}\")\n",
    "\n",
    "        model_checkpoint_cb = keras.callbacks.ModelCheckpoint(checkpoint_file, save_best_only=True)\n",
    "\n",
    "        model.fit(X_train,y_train, epochs=100, validation_data=(X_valid, y_valid), \n",
    "            callbacks=[keras.callbacks.EarlyStopping(patience=10), model_checkpoint_cb, tensorboard_cb])\n",
    "        \n",
    "        best_models.append(checkpoint_file)\n",
    "\n",
    "        valid_accuracy.append(model.evaluate(X_valid,y_valid)[1])\n",
    "        test_accuracy.append(model.evaluate(X_test,y_test)[1])\n",
    "\n",
    "        \n",
    "    print(f\"Best validation performance: {np.max(valid_accuracy):.2%} for lr: {lrs[np.argmax(valid_accuracy)]}\")\n",
    "    print(f\"Best test performance: {np.max(test_accuracy):.2%} for lr: {lrs[np.argmax(test_accuracy)]}\")\n",
    "    return valid_accuracy,test_accuracy,logs, best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting lr = 0.001 Logged in: logs/fit/dropout_0.001_20220704-131800 saved in models/dropout_0.001_model.h5\n",
      "Epoch 1/100\n",
      "   1/1172 [..............................] - ETA: 0s - loss: 9.8958 - accuracy: 0.1562WARNING:tensorflow:From c:\\Users\\micha\\anaconda3\\envs\\PandasNumpyMathplotlib\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "   2/1172 [..............................] - ETA: 43s - loss: 9.5808 - accuracy: 0.1562WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0080s vs `on_train_batch_end` time: 0.0639s). Check your callbacks.\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.5053 - accuracy: 0.1036 - val_loss: 2.3015 - val_accuracy: 0.1087\n",
      "Epoch 2/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.1998 - accuracy: 0.1558 - val_loss: 2.1789 - val_accuracy: 0.1737\n",
      "Epoch 3/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 2.1067 - accuracy: 0.1712 - val_loss: 2.0442 - val_accuracy: 0.1994\n",
      "Epoch 4/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 2.0758 - accuracy: 0.1795 - val_loss: 2.0161 - val_accuracy: 0.1957\n",
      "Epoch 5/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 2.0430 - accuracy: 0.1956 - val_loss: 1.9994 - val_accuracy: 0.2162\n",
      "Epoch 6/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 2.0245 - accuracy: 0.2095 - val_loss: 1.9814 - val_accuracy: 0.2313\n",
      "Epoch 7/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 2.0115 - accuracy: 0.2184 - val_loss: 2.1274 - val_accuracy: 0.1995\n",
      "Epoch 8/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 2.0040 - accuracy: 0.2259 - val_loss: 1.9451 - val_accuracy: 0.2618\n",
      "Epoch 9/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 2.0198 - accuracy: 0.2245 - val_loss: 1.9587 - val_accuracy: 0.2271\n",
      "Epoch 10/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.9871 - accuracy: 0.2357 - val_loss: 1.9729 - val_accuracy: 0.2479\n",
      "Epoch 11/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.9940 - accuracy: 0.2307 - val_loss: 1.9307 - val_accuracy: 0.2468\n",
      "Epoch 12/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 2.0321 - accuracy: 0.2129 - val_loss: 1.9698 - val_accuracy: 0.2361\n",
      "Epoch 13/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 2.0517 - accuracy: 0.2094 - val_loss: 1.9826 - val_accuracy: 0.2168\n",
      "Epoch 14/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 2.0542 - accuracy: 0.2104 - val_loss: 1.9663 - val_accuracy: 0.2554\n",
      "Epoch 15/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.9924 - accuracy: 0.2309 - val_loss: 2.0510 - val_accuracy: 0.2095\n",
      "Epoch 16/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 2.0674 - accuracy: 0.1925 - val_loss: 2.0526 - val_accuracy: 0.1832\n",
      "Epoch 17/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 2.0198 - accuracy: 0.2076 - val_loss: 2.0058 - val_accuracy: 0.2254\n",
      "Epoch 18/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 2.0016 - accuracy: 0.2158 - val_loss: 2.0155 - val_accuracy: 0.2119\n",
      "Epoch 19/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.9981 - accuracy: 0.2145 - val_loss: 2.0075 - val_accuracy: 0.2018\n",
      "Epoch 20/100\n",
      "1172/1172 [==============================] - 9s 8ms/step - loss: 2.0110 - accuracy: 0.2084 - val_loss: 1.9970 - val_accuracy: 0.2272\n",
      "Epoch 21/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.9766 - accuracy: 0.2244 - val_loss: 1.9872 - val_accuracy: 0.2041\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 1.9872 - accuracy: 0.2041\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 1.9799 - accuracy: 0.1998: 0s - loss: 1.9805 - accuracy: \n",
      "fitting lr = 0.0005 Logged in: logs/fit/dropout_0.0005_20220704-132134 saved in models/dropout_0.0005_model.h5\n",
      "Epoch 1/100\n",
      "   2/1172 [..............................] - ETA: 20:51 - loss: 8.7521 - accuracy: 0.1094WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0070s vs `on_train_batch_end` time: 2.1319s). Check your callbacks.\n",
      "1172/1172 [==============================] - 12s 11ms/step - loss: 2.5902 - accuracy: 0.0997 - val_loss: 2.3180 - val_accuracy: 0.0951\n",
      "Epoch 2/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 2.2867 - accuracy: 0.1289 - val_loss: 2.0748 - val_accuracy: 0.1902\n",
      "Epoch 3/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 2.1424 - accuracy: 0.1669 - val_loss: 2.1437 - val_accuracy: 0.1727\n",
      "Epoch 4/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 2.0994 - accuracy: 0.1776 - val_loss: 2.0262 - val_accuracy: 0.2131\n",
      "Epoch 5/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 2.0540 - accuracy: 0.1984 - val_loss: 2.0334 - val_accuracy: 0.2058\n",
      "Epoch 6/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 2.0213 - accuracy: 0.2129 - val_loss: 1.9593 - val_accuracy: 0.2391\n",
      "Epoch 7/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.9859 - accuracy: 0.2323 - val_loss: 1.9601 - val_accuracy: 0.2305\n",
      "Epoch 8/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.9645 - accuracy: 0.2429 - val_loss: 1.9430 - val_accuracy: 0.2442\n",
      "Epoch 9/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.9585 - accuracy: 0.2492 - val_loss: 1.9092 - val_accuracy: 0.2434\n",
      "Epoch 10/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.9470 - accuracy: 0.2578 - val_loss: 1.8700 - val_accuracy: 0.2813\n",
      "Epoch 11/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.9065 - accuracy: 0.2678 - val_loss: 1.8265 - val_accuracy: 0.3026\n",
      "Epoch 12/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.8840 - accuracy: 0.2812 - val_loss: 1.8491 - val_accuracy: 0.3020\n",
      "Epoch 13/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.8671 - accuracy: 0.2873 - val_loss: 1.8793 - val_accuracy: 0.2674\n",
      "Epoch 14/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.8594 - accuracy: 0.2957 - val_loss: 1.8114 - val_accuracy: 0.3089\n",
      "Epoch 15/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.8415 - accuracy: 0.3063 - val_loss: 1.8253 - val_accuracy: 0.3118\n",
      "Epoch 16/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.8300 - accuracy: 0.3089 - val_loss: 1.8060 - val_accuracy: 0.3270\n",
      "Epoch 17/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.8260 - accuracy: 0.3174 - val_loss: 1.7827 - val_accuracy: 0.3468\n",
      "Epoch 18/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.8129 - accuracy: 0.3239 - val_loss: 1.7645 - val_accuracy: 0.3520\n",
      "Epoch 19/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.8094 - accuracy: 0.3312 - val_loss: 1.8075 - val_accuracy: 0.3091\n",
      "Epoch 20/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.7975 - accuracy: 0.3387 - val_loss: 1.7831 - val_accuracy: 0.3486\n",
      "Epoch 21/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.8041 - accuracy: 0.3338 - val_loss: 1.7400 - val_accuracy: 0.3591\n",
      "Epoch 22/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.7833 - accuracy: 0.3439 - val_loss: 1.7629 - val_accuracy: 0.3608\n",
      "Epoch 23/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.7774 - accuracy: 0.3505 - val_loss: 1.7770 - val_accuracy: 0.3525\n",
      "Epoch 24/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.7743 - accuracy: 0.3533 - val_loss: 1.7374 - val_accuracy: 0.3763\n",
      "Epoch 25/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.7606 - accuracy: 0.3599 - val_loss: 1.7221 - val_accuracy: 0.3719\n",
      "Epoch 26/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.7680 - accuracy: 0.3540 - val_loss: 1.7272 - val_accuracy: 0.3698\n",
      "Epoch 27/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.7667 - accuracy: 0.3561 - val_loss: 1.7400 - val_accuracy: 0.3691\n",
      "Epoch 28/100\n",
      "1172/1172 [==============================] - 9s 8ms/step - loss: 1.7521 - accuracy: 0.3671 - val_loss: 1.7293 - val_accuracy: 0.3774\n",
      "Epoch 29/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.7325 - accuracy: 0.3682 - val_loss: 1.6998 - val_accuracy: 0.3758\n",
      "Epoch 30/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.7286 - accuracy: 0.3777 - val_loss: 1.6933 - val_accuracy: 0.3864\n",
      "Epoch 31/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.7299 - accuracy: 0.3737 - val_loss: 1.7038 - val_accuracy: 0.3823\n",
      "Epoch 32/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.7230 - accuracy: 0.3784 - val_loss: 1.7086 - val_accuracy: 0.3878\n",
      "Epoch 33/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.7169 - accuracy: 0.3829 - val_loss: 1.7093 - val_accuracy: 0.3884\n",
      "Epoch 34/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.7206 - accuracy: 0.3839 - val_loss: 1.6791 - val_accuracy: 0.3902\n",
      "Epoch 35/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.7130 - accuracy: 0.3833 - val_loss: 1.7291 - val_accuracy: 0.3792\n",
      "Epoch 36/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.7087 - accuracy: 0.3874 - val_loss: 1.6976 - val_accuracy: 0.3845\n",
      "Epoch 37/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.7068 - accuracy: 0.3854 - val_loss: 1.7088 - val_accuracy: 0.3886\n",
      "Epoch 38/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.7076 - accuracy: 0.3857 - val_loss: 1.7413 - val_accuracy: 0.3707\n",
      "Epoch 39/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.7034 - accuracy: 0.3893 - val_loss: 1.7310 - val_accuracy: 0.3772\n",
      "Epoch 40/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.6909 - accuracy: 0.3905 - val_loss: 1.6833 - val_accuracy: 0.3978\n",
      "Epoch 41/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6908 - accuracy: 0.3884 - val_loss: 1.7029 - val_accuracy: 0.3878\n",
      "Epoch 42/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.7029 - accuracy: 0.3842 - val_loss: 1.6424 - val_accuracy: 0.4107\n",
      "Epoch 43/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.7019 - accuracy: 0.3855 - val_loss: 1.6717 - val_accuracy: 0.3906\n",
      "Epoch 44/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6916 - accuracy: 0.3921 - val_loss: 1.6790 - val_accuracy: 0.3910\n",
      "Epoch 45/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6971 - accuracy: 0.3893 - val_loss: 1.6684 - val_accuracy: 0.4038\n",
      "Epoch 46/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6912 - accuracy: 0.3965 - val_loss: 1.6644 - val_accuracy: 0.4124\n",
      "Epoch 47/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6821 - accuracy: 0.3978 - val_loss: 1.6508 - val_accuracy: 0.4079\n",
      "Epoch 48/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6730 - accuracy: 0.4014 - val_loss: 1.6830 - val_accuracy: 0.3935\n",
      "Epoch 49/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6764 - accuracy: 0.4017 - val_loss: 1.6801 - val_accuracy: 0.4008\n",
      "Epoch 50/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.7505 - accuracy: 0.3702 - val_loss: 1.6552 - val_accuracy: 0.4124\n",
      "Epoch 51/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6691 - accuracy: 0.4028 - val_loss: 1.6539 - val_accuracy: 0.4154\n",
      "Epoch 52/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6789 - accuracy: 0.3990 - val_loss: 1.6393 - val_accuracy: 0.4172\n",
      "Epoch 53/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6600 - accuracy: 0.4052 - val_loss: 1.6395 - val_accuracy: 0.4121\n",
      "Epoch 54/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6519 - accuracy: 0.4099 - val_loss: 1.6396 - val_accuracy: 0.4183\n",
      "Epoch 55/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6765 - accuracy: 0.3994 - val_loss: 1.6880 - val_accuracy: 0.4026\n",
      "Epoch 56/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6750 - accuracy: 0.4022 - val_loss: 1.6287 - val_accuracy: 0.4194\n",
      "Epoch 57/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6501 - accuracy: 0.4080 - val_loss: 1.6270 - val_accuracy: 0.4210\n",
      "Epoch 58/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6702 - accuracy: 0.4032 - val_loss: 1.6080 - val_accuracy: 0.4210\n",
      "Epoch 59/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6599 - accuracy: 0.4073 - val_loss: 1.6301 - val_accuracy: 0.4219\n",
      "Epoch 60/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6562 - accuracy: 0.4059 - val_loss: 1.6636 - val_accuracy: 0.4083\n",
      "Epoch 61/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6547 - accuracy: 0.4051 - val_loss: 1.6182 - val_accuracy: 0.4175\n",
      "Epoch 62/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6384 - accuracy: 0.4180 - val_loss: 1.6423 - val_accuracy: 0.4213\n",
      "Epoch 63/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.7632 - accuracy: 0.3797 - val_loss: 1.7724 - val_accuracy: 0.3513\n",
      "Epoch 64/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6994 - accuracy: 0.3933 - val_loss: 1.6561 - val_accuracy: 0.4143\n",
      "Epoch 65/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6587 - accuracy: 0.4080 - val_loss: 1.6631 - val_accuracy: 0.4030\n",
      "Epoch 66/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6482 - accuracy: 0.4141 - val_loss: 1.6439 - val_accuracy: 0.4166\n",
      "Epoch 67/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6438 - accuracy: 0.4180 - val_loss: 1.6248 - val_accuracy: 0.4284\n",
      "Epoch 68/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6294 - accuracy: 0.4193 - val_loss: 1.6454 - val_accuracy: 0.4183\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 1.6454 - accuracy: 0.4183\n",
      "313/313 [==============================] - 0s 2ms/step - loss: 1.6433 - accuracy: 0.4243\n",
      "fitting lr = 0.00025 Logged in: logs/fit/dropout_0.00025_20220704-133244 saved in models/dropout_0.00025_model.h5\n",
      "Epoch 1/100\n",
      "   2/1172 [..............................] - ETA: 22:59 - loss: 6.0826 - accuracy: 0.1562WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0090s vs `on_train_batch_end` time: 2.3481s). Check your callbacks.\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.7080 - accuracy: 0.0995 - val_loss: 2.3048 - val_accuracy: 0.1185\n",
      "Epoch 2/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.3683 - accuracy: 0.1017 - val_loss: 2.3022 - val_accuracy: 0.1190\n",
      "Epoch 3/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 2.3066 - accuracy: 0.1210 - val_loss: 2.1286 - val_accuracy: 0.1630\n",
      "Epoch 4/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.1530 - accuracy: 0.1667 - val_loss: 2.1093 - val_accuracy: 0.1807\n",
      "Epoch 5/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 2.1028 - accuracy: 0.1774 - val_loss: 2.0367 - val_accuracy: 0.1863\n",
      "Epoch 6/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 2.0672 - accuracy: 0.1845 - val_loss: 2.0660 - val_accuracy: 0.1796\n",
      "Epoch 7/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 2.0383 - accuracy: 0.2018 - val_loss: 1.9671 - val_accuracy: 0.2346\n",
      "Epoch 8/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 2.0098 - accuracy: 0.2125 - val_loss: 2.0361 - val_accuracy: 0.2136\n",
      "Epoch 9/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.9902 - accuracy: 0.2244 - val_loss: 1.9467 - val_accuracy: 0.2426\n",
      "Epoch 10/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.9641 - accuracy: 0.2411 - val_loss: 1.9199 - val_accuracy: 0.2443\n",
      "Epoch 11/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.9429 - accuracy: 0.2527 - val_loss: 1.8958 - val_accuracy: 0.2688\n",
      "Epoch 12/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.9156 - accuracy: 0.2610 - val_loss: 1.8601 - val_accuracy: 0.2751\n",
      "Epoch 13/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.8901 - accuracy: 0.2712 - val_loss: 1.8397 - val_accuracy: 0.2755\n",
      "Epoch 14/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.8698 - accuracy: 0.2811 - val_loss: 1.8136 - val_accuracy: 0.2914\n",
      "Epoch 15/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.8541 - accuracy: 0.2918 - val_loss: 1.8323 - val_accuracy: 0.3054\n",
      "Epoch 16/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.8404 - accuracy: 0.3003 - val_loss: 1.8092 - val_accuracy: 0.3013\n",
      "Epoch 17/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.8192 - accuracy: 0.3100 - val_loss: 1.7749 - val_accuracy: 0.3236\n",
      "Epoch 18/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 1.7996 - accuracy: 0.3221 - val_loss: 1.7661 - val_accuracy: 0.3466\n",
      "Epoch 19/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.7741 - accuracy: 0.3314 - val_loss: 1.7439 - val_accuracy: 0.3487\n",
      "Epoch 20/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.7588 - accuracy: 0.3431 - val_loss: 1.7069 - val_accuracy: 0.3702\n",
      "Epoch 21/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.7443 - accuracy: 0.3539 - val_loss: 1.7319 - val_accuracy: 0.3578\n",
      "Epoch 22/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.7344 - accuracy: 0.3591 - val_loss: 1.6819 - val_accuracy: 0.3901\n",
      "Epoch 23/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.7192 - accuracy: 0.3695 - val_loss: 1.7011 - val_accuracy: 0.3800\n",
      "Epoch 24/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.7048 - accuracy: 0.3731 - val_loss: 1.6933 - val_accuracy: 0.3915\n",
      "Epoch 25/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6964 - accuracy: 0.3830 - val_loss: 1.6860 - val_accuracy: 0.3856\n",
      "Epoch 26/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6866 - accuracy: 0.3853 - val_loss: 1.6546 - val_accuracy: 0.4012\n",
      "Epoch 27/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6698 - accuracy: 0.3932 - val_loss: 1.6656 - val_accuracy: 0.4008\n",
      "Epoch 28/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6655 - accuracy: 0.3960 - val_loss: 1.6401 - val_accuracy: 0.4155\n",
      "Epoch 29/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6567 - accuracy: 0.4017 - val_loss: 1.6494 - val_accuracy: 0.3978\n",
      "Epoch 30/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6459 - accuracy: 0.4041 - val_loss: 1.6204 - val_accuracy: 0.4187\n",
      "Epoch 31/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6405 - accuracy: 0.4102 - val_loss: 1.6012 - val_accuracy: 0.4246\n",
      "Epoch 32/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6277 - accuracy: 0.4140 - val_loss: 1.6128 - val_accuracy: 0.4172\n",
      "Epoch 33/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6165 - accuracy: 0.4177 - val_loss: 1.6512 - val_accuracy: 0.4107\n",
      "Epoch 34/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6133 - accuracy: 0.4216 - val_loss: 1.6329 - val_accuracy: 0.4222\n",
      "Epoch 35/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.6120 - accuracy: 0.4221 - val_loss: 1.5804 - val_accuracy: 0.4325\n",
      "Epoch 36/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.5961 - accuracy: 0.4254 - val_loss: 1.5871 - val_accuracy: 0.4299\n",
      "Epoch 37/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.5951 - accuracy: 0.4291 - val_loss: 1.5799 - val_accuracy: 0.4345\n",
      "Epoch 38/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.5878 - accuracy: 0.4329 - val_loss: 1.5648 - val_accuracy: 0.4396\n",
      "Epoch 39/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.5765 - accuracy: 0.4346 - val_loss: 1.5582 - val_accuracy: 0.4446\n",
      "Epoch 40/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.5772 - accuracy: 0.4355 - val_loss: 1.5810 - val_accuracy: 0.4348\n",
      "Epoch 41/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.5659 - accuracy: 0.4403 - val_loss: 1.5760 - val_accuracy: 0.4298\n",
      "Epoch 42/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.5607 - accuracy: 0.4425 - val_loss: 1.5844 - val_accuracy: 0.4350\n",
      "Epoch 43/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.5544 - accuracy: 0.4479 - val_loss: 1.5613 - val_accuracy: 0.4482\n",
      "Epoch 44/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.5463 - accuracy: 0.4467 - val_loss: 1.5511 - val_accuracy: 0.4428\n",
      "Epoch 45/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.5388 - accuracy: 0.4516 - val_loss: 1.5416 - val_accuracy: 0.4516\n",
      "Epoch 46/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.5310 - accuracy: 0.4534 - val_loss: 1.5348 - val_accuracy: 0.4554\n",
      "Epoch 47/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.5311 - accuracy: 0.4549 - val_loss: 1.5325 - val_accuracy: 0.4584\n",
      "Epoch 48/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.5276 - accuracy: 0.4565 - val_loss: 1.5229 - val_accuracy: 0.4602\n",
      "Epoch 49/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.5201 - accuracy: 0.4585 - val_loss: 1.5716 - val_accuracy: 0.4438\n",
      "Epoch 50/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.5125 - accuracy: 0.4629 - val_loss: 1.5468 - val_accuracy: 0.4550\n",
      "Epoch 51/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.5155 - accuracy: 0.4608 - val_loss: 1.5294 - val_accuracy: 0.4605\n",
      "Epoch 52/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.5029 - accuracy: 0.4651 - val_loss: 1.5229 - val_accuracy: 0.4631\n",
      "Epoch 53/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.5016 - accuracy: 0.4640 - val_loss: 1.5283 - val_accuracy: 0.4634\n",
      "Epoch 54/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.4940 - accuracy: 0.4672 - val_loss: 1.5167 - val_accuracy: 0.4618\n",
      "Epoch 55/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.4890 - accuracy: 0.4710 - val_loss: 1.5131 - val_accuracy: 0.4646\n",
      "Epoch 56/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.4855 - accuracy: 0.4706 - val_loss: 1.5034 - val_accuracy: 0.4704\n",
      "Epoch 57/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.4754 - accuracy: 0.4791 - val_loss: 1.5184 - val_accuracy: 0.4630\n",
      "Epoch 58/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.4765 - accuracy: 0.4783 - val_loss: 1.5146 - val_accuracy: 0.4693\n",
      "Epoch 59/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.4686 - accuracy: 0.4798 - val_loss: 1.4960 - val_accuracy: 0.4689\n",
      "Epoch 60/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.4635 - accuracy: 0.4797 - val_loss: 1.5075 - val_accuracy: 0.4645\n",
      "Epoch 61/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.4688 - accuracy: 0.4777 - val_loss: 1.4982 - val_accuracy: 0.4750\n",
      "Epoch 62/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.4599 - accuracy: 0.4835 - val_loss: 1.5036 - val_accuracy: 0.4712\n",
      "Epoch 63/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.4534 - accuracy: 0.4857 - val_loss: 1.4945 - val_accuracy: 0.4773\n",
      "Epoch 64/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.4549 - accuracy: 0.4842 - val_loss: 1.5398 - val_accuracy: 0.4642\n",
      "Epoch 65/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.4463 - accuracy: 0.4887 - val_loss: 1.4972 - val_accuracy: 0.4758\n",
      "Epoch 66/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.4454 - accuracy: 0.4904 - val_loss: 1.5371 - val_accuracy: 0.4598\n",
      "Epoch 67/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.4513 - accuracy: 0.4883 - val_loss: 1.5277 - val_accuracy: 0.4609\n",
      "Epoch 68/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.4443 - accuracy: 0.4896 - val_loss: 1.4840 - val_accuracy: 0.4819\n",
      "Epoch 69/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.4357 - accuracy: 0.4947 - val_loss: 1.5179 - val_accuracy: 0.4739\n",
      "Epoch 70/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.4286 - accuracy: 0.4967 - val_loss: 1.4906 - val_accuracy: 0.4837\n",
      "Epoch 71/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.4320 - accuracy: 0.4933 - val_loss: 1.5108 - val_accuracy: 0.4808\n",
      "Epoch 72/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.4169 - accuracy: 0.5019 - val_loss: 1.5131 - val_accuracy: 0.4722\n",
      "Epoch 73/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.4225 - accuracy: 0.4961 - val_loss: 1.4895 - val_accuracy: 0.4765\n",
      "Epoch 74/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.4168 - accuracy: 0.4987 - val_loss: 1.5195 - val_accuracy: 0.4760\n",
      "Epoch 75/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.4094 - accuracy: 0.5040 - val_loss: 1.5213 - val_accuracy: 0.4688\n",
      "Epoch 76/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.4074 - accuracy: 0.5024 - val_loss: 1.4790 - val_accuracy: 0.4778\n",
      "Epoch 77/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.4081 - accuracy: 0.5032 - val_loss: 1.4993 - val_accuracy: 0.4797\n",
      "Epoch 78/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.3985 - accuracy: 0.5072 - val_loss: 1.4848 - val_accuracy: 0.4813\n",
      "Epoch 79/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.3915 - accuracy: 0.5106 - val_loss: 1.4874 - val_accuracy: 0.4838\n",
      "Epoch 80/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.3953 - accuracy: 0.5075 - val_loss: 1.4715 - val_accuracy: 0.4812\n",
      "Epoch 81/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.3912 - accuracy: 0.5099 - val_loss: 1.4869 - val_accuracy: 0.4767\n",
      "Epoch 82/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.3840 - accuracy: 0.5120 - val_loss: 1.4669 - val_accuracy: 0.4877\n",
      "Epoch 83/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.3799 - accuracy: 0.5123 - val_loss: 1.4833 - val_accuracy: 0.4826\n",
      "Epoch 84/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.3859 - accuracy: 0.5122 - val_loss: 1.4767 - val_accuracy: 0.4878\n",
      "Epoch 85/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.3846 - accuracy: 0.5114 - val_loss: 1.4720 - val_accuracy: 0.4863\n",
      "Epoch 86/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.3742 - accuracy: 0.5130 - val_loss: 1.4984 - val_accuracy: 0.4788\n",
      "Epoch 87/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.3758 - accuracy: 0.5119 - val_loss: 1.4766 - val_accuracy: 0.4814\n",
      "Epoch 88/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.3757 - accuracy: 0.5157 - val_loss: 1.4773 - val_accuracy: 0.4849\n",
      "Epoch 89/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.3737 - accuracy: 0.5158 - val_loss: 1.4646 - val_accuracy: 0.4922\n",
      "Epoch 90/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.3657 - accuracy: 0.5198 - val_loss: 1.4529 - val_accuracy: 0.4961\n",
      "Epoch 91/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.3533 - accuracy: 0.5250 - val_loss: 1.4933 - val_accuracy: 0.4886\n",
      "Epoch 92/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.3541 - accuracy: 0.5242 - val_loss: 1.4891 - val_accuracy: 0.4893\n",
      "Epoch 93/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.3576 - accuracy: 0.5212 - val_loss: 1.4657 - val_accuracy: 0.4918\n",
      "Epoch 94/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.3477 - accuracy: 0.5262 - val_loss: 1.4858 - val_accuracy: 0.4769\n",
      "Epoch 95/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.3530 - accuracy: 0.5239 - val_loss: 1.4682 - val_accuracy: 0.4960\n",
      "Epoch 96/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.3544 - accuracy: 0.5249 - val_loss: 1.5114 - val_accuracy: 0.4782\n",
      "Epoch 97/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.3440 - accuracy: 0.5301 - val_loss: 1.4646 - val_accuracy: 0.4925\n",
      "Epoch 98/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.3356 - accuracy: 0.5321 - val_loss: 1.4564 - val_accuracy: 0.4994\n",
      "Epoch 99/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.3295 - accuracy: 0.5334 - val_loss: 1.4658 - val_accuracy: 0.4897\n",
      "Epoch 100/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.3330 - accuracy: 0.5315 - val_loss: 1.4747 - val_accuracy: 0.4854\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 1.4747 - accuracy: 0.4854\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 1.4580 - accuracy: 0.4877\n",
      "fitting lr = 0.0001 Logged in: logs/fit/dropout_0.0001_20220704-134918 saved in models/dropout_0.0001_model.h5\n",
      "Epoch 1/100\n",
      "   2/1172 [..............................] - ETA: 21:16 - loss: 8.6902 - accuracy: 0.0312WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0080s vs `on_train_batch_end` time: 2.1746s). Check your callbacks.\n",
      "1172/1172 [==============================] - 12s 11ms/step - loss: 3.1010 - accuracy: 0.1007 - val_loss: 2.3286 - val_accuracy: 0.0976\n",
      "Epoch 2/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 2.4625 - accuracy: 0.0998 - val_loss: 2.3028 - val_accuracy: 0.1011\n",
      "Epoch 3/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 2.3890 - accuracy: 0.1022 - val_loss: 2.3032 - val_accuracy: 0.1006\n",
      "Epoch 4/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 2.3635 - accuracy: 0.1047 - val_loss: 2.2452 - val_accuracy: 0.1330\n",
      "Epoch 5/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 2.2400 - accuracy: 0.1472 - val_loss: 2.1057 - val_accuracy: 0.1622\n",
      "Epoch 6/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 2.1649 - accuracy: 0.1616 - val_loss: 2.0758 - val_accuracy: 0.1879\n",
      "Epoch 7/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 2.1354 - accuracy: 0.1675 - val_loss: 2.0530 - val_accuracy: 0.1942\n",
      "Epoch 8/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 2.0949 - accuracy: 0.1805 - val_loss: 2.0392 - val_accuracy: 0.1824\n",
      "Epoch 9/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 2.0658 - accuracy: 0.1914 - val_loss: 2.0068 - val_accuracy: 0.1966\n",
      "Epoch 10/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 2.0453 - accuracy: 0.2024 - val_loss: 1.9887 - val_accuracy: 0.2234\n",
      "Epoch 11/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 2.0214 - accuracy: 0.2154 - val_loss: 1.9818 - val_accuracy: 0.2244\n",
      "Epoch 12/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 2.0007 - accuracy: 0.2237 - val_loss: 1.9640 - val_accuracy: 0.2360\n",
      "Epoch 13/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.9740 - accuracy: 0.2361 - val_loss: 1.9111 - val_accuracy: 0.2582\n",
      "Epoch 14/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.9461 - accuracy: 0.2455 - val_loss: 1.8612 - val_accuracy: 0.2860\n",
      "Epoch 15/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.9204 - accuracy: 0.2606 - val_loss: 1.8242 - val_accuracy: 0.2918\n",
      "Epoch 16/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.9027 - accuracy: 0.2642 - val_loss: 1.8417 - val_accuracy: 0.2894\n",
      "Epoch 17/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.8872 - accuracy: 0.2742 - val_loss: 1.8029 - val_accuracy: 0.3065\n",
      "Epoch 18/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.8695 - accuracy: 0.2746 - val_loss: 1.8000 - val_accuracy: 0.2997\n",
      "Epoch 19/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.8525 - accuracy: 0.2869 - val_loss: 1.7940 - val_accuracy: 0.3211\n",
      "Epoch 20/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.8364 - accuracy: 0.2993 - val_loss: 1.7630 - val_accuracy: 0.3406\n",
      "Epoch 21/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.8196 - accuracy: 0.3089 - val_loss: 1.8262 - val_accuracy: 0.3286\n",
      "Epoch 22/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.8092 - accuracy: 0.3187 - val_loss: 1.7457 - val_accuracy: 0.3568\n",
      "Epoch 23/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.7874 - accuracy: 0.3309 - val_loss: 1.7196 - val_accuracy: 0.3725\n",
      "Epoch 24/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.7726 - accuracy: 0.3413 - val_loss: 1.6924 - val_accuracy: 0.3734\n",
      "Epoch 25/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.7563 - accuracy: 0.3501 - val_loss: 1.6845 - val_accuracy: 0.3890\n",
      "Epoch 26/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.7472 - accuracy: 0.3589 - val_loss: 1.6791 - val_accuracy: 0.3839\n",
      "Epoch 27/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.7358 - accuracy: 0.3591 - val_loss: 1.6671 - val_accuracy: 0.3881\n",
      "Epoch 28/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.7210 - accuracy: 0.3648 - val_loss: 1.6547 - val_accuracy: 0.4031\n",
      "Epoch 29/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.7085 - accuracy: 0.3784 - val_loss: 1.6502 - val_accuracy: 0.3930\n",
      "Epoch 30/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.7040 - accuracy: 0.3795 - val_loss: 1.6360 - val_accuracy: 0.4073\n",
      "Epoch 31/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6859 - accuracy: 0.3870 - val_loss: 1.6372 - val_accuracy: 0.4034\n",
      "Epoch 32/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6798 - accuracy: 0.3903 - val_loss: 1.6219 - val_accuracy: 0.4172\n",
      "Epoch 33/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6680 - accuracy: 0.3942 - val_loss: 1.6251 - val_accuracy: 0.4116\n",
      "Epoch 34/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.6624 - accuracy: 0.3973 - val_loss: 1.6168 - val_accuracy: 0.4229\n",
      "Epoch 35/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.6603 - accuracy: 0.3975 - val_loss: 1.6474 - val_accuracy: 0.4158\n",
      "Epoch 36/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6480 - accuracy: 0.4042 - val_loss: 1.6463 - val_accuracy: 0.4039\n",
      "Epoch 37/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.6316 - accuracy: 0.4125 - val_loss: 1.5941 - val_accuracy: 0.4261\n",
      "Epoch 38/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.6204 - accuracy: 0.4145 - val_loss: 1.5793 - val_accuracy: 0.4364\n",
      "Epoch 39/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6227 - accuracy: 0.4153 - val_loss: 1.5808 - val_accuracy: 0.4377\n",
      "Epoch 40/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 1.6139 - accuracy: 0.4186 - val_loss: 1.5632 - val_accuracy: 0.4407\n",
      "Epoch 41/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.6082 - accuracy: 0.4245 - val_loss: 1.5622 - val_accuracy: 0.4429\n",
      "Epoch 42/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.5992 - accuracy: 0.4244 - val_loss: 1.5521 - val_accuracy: 0.4420\n",
      "Epoch 43/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.5952 - accuracy: 0.4262 - val_loss: 1.6118 - val_accuracy: 0.4266\n",
      "Epoch 44/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.5815 - accuracy: 0.4331 - val_loss: 1.5599 - val_accuracy: 0.4457\n",
      "Epoch 45/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.5793 - accuracy: 0.4320 - val_loss: 1.5450 - val_accuracy: 0.4505\n",
      "Epoch 46/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.5714 - accuracy: 0.4370 - val_loss: 1.5455 - val_accuracy: 0.4485\n",
      "Epoch 47/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.5678 - accuracy: 0.4407 - val_loss: 1.5560 - val_accuracy: 0.4526\n",
      "Epoch 48/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.5597 - accuracy: 0.4452 - val_loss: 1.5521 - val_accuracy: 0.4560\n",
      "Epoch 49/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.5539 - accuracy: 0.4446 - val_loss: 1.5360 - val_accuracy: 0.4578\n",
      "Epoch 50/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.5458 - accuracy: 0.4493 - val_loss: 1.5670 - val_accuracy: 0.4515\n",
      "Epoch 51/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.5451 - accuracy: 0.4515 - val_loss: 1.5528 - val_accuracy: 0.4506\n",
      "Epoch 52/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.5388 - accuracy: 0.4537 - val_loss: 1.5166 - val_accuracy: 0.4676\n",
      "Epoch 53/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.5290 - accuracy: 0.4579 - val_loss: 1.5420 - val_accuracy: 0.4636\n",
      "Epoch 54/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.5269 - accuracy: 0.4566 - val_loss: 1.5142 - val_accuracy: 0.4686\n",
      "Epoch 55/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.5169 - accuracy: 0.4582 - val_loss: 1.5141 - val_accuracy: 0.4678\n",
      "Epoch 56/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.5134 - accuracy: 0.4603 - val_loss: 1.5501 - val_accuracy: 0.4618\n",
      "Epoch 57/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.5074 - accuracy: 0.4645 - val_loss: 1.5126 - val_accuracy: 0.4646\n",
      "Epoch 58/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.5045 - accuracy: 0.4671 - val_loss: 1.5045 - val_accuracy: 0.4678\n",
      "Epoch 59/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.5046 - accuracy: 0.4667 - val_loss: 1.4930 - val_accuracy: 0.4737\n",
      "Epoch 60/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.4926 - accuracy: 0.4737 - val_loss: 1.5472 - val_accuracy: 0.4574\n",
      "Epoch 61/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.4926 - accuracy: 0.4712 - val_loss: 1.4953 - val_accuracy: 0.4722\n",
      "Epoch 62/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.4879 - accuracy: 0.4738 - val_loss: 1.4894 - val_accuracy: 0.4747\n",
      "Epoch 63/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.4757 - accuracy: 0.4811 - val_loss: 1.4984 - val_accuracy: 0.4735\n",
      "Epoch 64/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.4712 - accuracy: 0.4792 - val_loss: 1.5123 - val_accuracy: 0.4717\n",
      "Epoch 65/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.4678 - accuracy: 0.4828 - val_loss: 1.5317 - val_accuracy: 0.4622\n",
      "Epoch 66/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.4623 - accuracy: 0.4849 - val_loss: 1.4796 - val_accuracy: 0.4777\n",
      "Epoch 67/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.4652 - accuracy: 0.4782 - val_loss: 1.5008 - val_accuracy: 0.4682\n",
      "Epoch 68/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.4587 - accuracy: 0.4834 - val_loss: 1.4863 - val_accuracy: 0.4773\n",
      "Epoch 69/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.4565 - accuracy: 0.4825 - val_loss: 1.5401 - val_accuracy: 0.4638\n",
      "Epoch 70/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.4440 - accuracy: 0.4896 - val_loss: 1.4794 - val_accuracy: 0.4807\n",
      "Epoch 71/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.4443 - accuracy: 0.4903 - val_loss: 1.4944 - val_accuracy: 0.4746\n",
      "Epoch 72/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.4401 - accuracy: 0.4899 - val_loss: 1.4956 - val_accuracy: 0.4818\n",
      "Epoch 73/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.4359 - accuracy: 0.4898 - val_loss: 1.4761 - val_accuracy: 0.4822\n",
      "Epoch 74/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.4336 - accuracy: 0.4949 - val_loss: 1.4847 - val_accuracy: 0.4790\n",
      "Epoch 75/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.4255 - accuracy: 0.4962 - val_loss: 1.4814 - val_accuracy: 0.4857\n",
      "Epoch 76/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.4184 - accuracy: 0.5002 - val_loss: 1.4765 - val_accuracy: 0.4826\n",
      "Epoch 77/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.4207 - accuracy: 0.4997 - val_loss: 1.4623 - val_accuracy: 0.4828\n",
      "Epoch 78/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.4189 - accuracy: 0.5009 - val_loss: 1.4722 - val_accuracy: 0.4825\n",
      "Epoch 79/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.4120 - accuracy: 0.5015 - val_loss: 1.4725 - val_accuracy: 0.4817\n",
      "Epoch 80/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.4093 - accuracy: 0.5012 - val_loss: 1.4594 - val_accuracy: 0.4850\n",
      "Epoch 81/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.4077 - accuracy: 0.5030 - val_loss: 1.4807 - val_accuracy: 0.4867\n",
      "Epoch 82/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.4016 - accuracy: 0.5047 - val_loss: 1.4791 - val_accuracy: 0.4814\n",
      "Epoch 83/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.4035 - accuracy: 0.5045 - val_loss: 1.4485 - val_accuracy: 0.4912\n",
      "Epoch 84/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.3977 - accuracy: 0.5082 - val_loss: 1.4591 - val_accuracy: 0.4890\n",
      "Epoch 85/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.3883 - accuracy: 0.5111 - val_loss: 1.4605 - val_accuracy: 0.4910\n",
      "Epoch 86/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.3836 - accuracy: 0.5115 - val_loss: 1.4730 - val_accuracy: 0.4896\n",
      "Epoch 87/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.3827 - accuracy: 0.5132 - val_loss: 1.4776 - val_accuracy: 0.4885\n",
      "Epoch 88/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.3743 - accuracy: 0.5178 - val_loss: 1.4590 - val_accuracy: 0.4890\n",
      "Epoch 89/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.3854 - accuracy: 0.5148 - val_loss: 1.4605 - val_accuracy: 0.4860\n",
      "Epoch 90/100\n",
      "1172/1172 [==============================] - 9s 8ms/step - loss: 1.3772 - accuracy: 0.5195 - val_loss: 1.4644 - val_accuracy: 0.4909\n",
      "Epoch 91/100\n",
      "1172/1172 [==============================] - 9s 8ms/step - loss: 1.3691 - accuracy: 0.5187 - val_loss: 1.4580 - val_accuracy: 0.4878\n",
      "Epoch 92/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.3696 - accuracy: 0.5163 - val_loss: 1.4465 - val_accuracy: 0.4966\n",
      "Epoch 93/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.3688 - accuracy: 0.5176 - val_loss: 1.4481 - val_accuracy: 0.4969\n",
      "Epoch 94/100\n",
      "1172/1172 [==============================] - 9s 8ms/step - loss: 1.3612 - accuracy: 0.5240 - val_loss: 1.4523 - val_accuracy: 0.4915\n",
      "Epoch 95/100\n",
      "1172/1172 [==============================] - 9s 8ms/step - loss: 1.3641 - accuracy: 0.5195 - val_loss: 1.4315 - val_accuracy: 0.4978\n",
      "Epoch 96/100\n",
      "1172/1172 [==============================] - 9s 8ms/step - loss: 1.3590 - accuracy: 0.5195 - val_loss: 1.4390 - val_accuracy: 0.5034\n",
      "Epoch 97/100\n",
      "1172/1172 [==============================] - 9s 8ms/step - loss: 1.3522 - accuracy: 0.5241 - val_loss: 1.4422 - val_accuracy: 0.4928\n",
      "Epoch 98/100\n",
      "1172/1172 [==============================] - 9s 8ms/step - loss: 1.3529 - accuracy: 0.5226 - val_loss: 1.4505 - val_accuracy: 0.4914\n",
      "Epoch 99/100\n",
      "1172/1172 [==============================] - 9s 8ms/step - loss: 1.3522 - accuracy: 0.5218 - val_loss: 1.4362 - val_accuracy: 0.4981\n",
      "Epoch 100/100\n",
      "1172/1172 [==============================] - 9s 8ms/step - loss: 1.3388 - accuracy: 0.5292 - val_loss: 1.4486 - val_accuracy: 0.4946\n",
      "391/391 [==============================] - 1s 1ms/step - loss: 1.4486 - accuracy: 0.4946\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 1.4403 - accuracy: 0.4966\n",
      "fitting lr = 5e-05 Logged in: logs/fit/dropout_5e-05_20220704-140554 saved in models/dropout_5e-05_model.h5\n",
      "Epoch 1/100\n",
      "   2/1172 [..............................] - ETA: 23:05 - loss: 8.1156 - accuracy: 0.1406WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0070s vs `on_train_batch_end` time: 2.3610s). Check your callbacks.\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 3.3757 - accuracy: 0.1031 - val_loss: 2.3595 - val_accuracy: 0.1097\n",
      "Epoch 2/100\n",
      "1172/1172 [==============================] - 9s 8ms/step - loss: 2.5757 - accuracy: 0.1012 - val_loss: 2.3194 - val_accuracy: 0.1056\n",
      "Epoch 3/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.4498 - accuracy: 0.1005 - val_loss: 2.3141 - val_accuracy: 0.1012\n",
      "Epoch 4/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.4000 - accuracy: 0.0992 - val_loss: 2.3061 - val_accuracy: 0.0995\n",
      "Epoch 5/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.3745 - accuracy: 0.1047 - val_loss: 2.2855 - val_accuracy: 0.1110\n",
      "Epoch 6/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.2725 - accuracy: 0.1414 - val_loss: 2.0964 - val_accuracy: 0.1967\n",
      "Epoch 7/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.1971 - accuracy: 0.1571 - val_loss: 2.0780 - val_accuracy: 0.1698\n",
      "Epoch 8/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 2.1610 - accuracy: 0.1639 - val_loss: 2.0654 - val_accuracy: 0.1821\n",
      "Epoch 9/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 2.1390 - accuracy: 0.1695 - val_loss: 2.0535 - val_accuracy: 0.1780\n",
      "Epoch 10/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 2.1159 - accuracy: 0.1730 - val_loss: 2.0469 - val_accuracy: 0.1818\n",
      "Epoch 11/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 2.0956 - accuracy: 0.1751 - val_loss: 2.0257 - val_accuracy: 0.1916\n",
      "Epoch 12/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 2.0813 - accuracy: 0.1818 - val_loss: 2.0274 - val_accuracy: 0.1906\n",
      "Epoch 13/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 2.0608 - accuracy: 0.1930 - val_loss: 2.0070 - val_accuracy: 0.1948\n",
      "Epoch 14/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.0384 - accuracy: 0.2042 - val_loss: 1.9566 - val_accuracy: 0.2310\n",
      "Epoch 15/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.0197 - accuracy: 0.2125 - val_loss: 1.9472 - val_accuracy: 0.2450\n",
      "Epoch 16/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.0078 - accuracy: 0.2205 - val_loss: 1.9120 - val_accuracy: 0.2461\n",
      "Epoch 17/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 1.9814 - accuracy: 0.2303 - val_loss: 1.8984 - val_accuracy: 0.2599\n",
      "Epoch 18/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 1.9681 - accuracy: 0.2399 - val_loss: 1.8835 - val_accuracy: 0.2746\n",
      "Epoch 19/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.9520 - accuracy: 0.2455 - val_loss: 1.8962 - val_accuracy: 0.2700\n",
      "Epoch 20/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.9369 - accuracy: 0.2495 - val_loss: 1.8893 - val_accuracy: 0.2625\n",
      "Epoch 21/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 1.9259 - accuracy: 0.2542 - val_loss: 1.8670 - val_accuracy: 0.2804\n",
      "Epoch 22/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 1.9182 - accuracy: 0.2564 - val_loss: 1.8450 - val_accuracy: 0.2919\n",
      "Epoch 23/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.9050 - accuracy: 0.2657 - val_loss: 1.8507 - val_accuracy: 0.2895\n",
      "Epoch 24/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.8919 - accuracy: 0.2686 - val_loss: 1.8646 - val_accuracy: 0.2871\n",
      "Epoch 25/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.8802 - accuracy: 0.2721 - val_loss: 1.8253 - val_accuracy: 0.3048\n",
      "Epoch 26/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.8685 - accuracy: 0.2816 - val_loss: 1.8119 - val_accuracy: 0.3194\n",
      "Epoch 27/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.8581 - accuracy: 0.2849 - val_loss: 1.7968 - val_accuracy: 0.3306\n",
      "Epoch 28/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.8516 - accuracy: 0.2930 - val_loss: 1.7721 - val_accuracy: 0.3310\n",
      "Epoch 29/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.8393 - accuracy: 0.2967 - val_loss: 1.7953 - val_accuracy: 0.3296\n",
      "Epoch 30/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.8293 - accuracy: 0.3039 - val_loss: 1.8062 - val_accuracy: 0.3425\n",
      "Epoch 31/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.8188 - accuracy: 0.3094 - val_loss: 1.7563 - val_accuracy: 0.3478\n",
      "Epoch 32/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.8131 - accuracy: 0.3176 - val_loss: 1.7389 - val_accuracy: 0.3618\n",
      "Epoch 33/100\n",
      "1172/1172 [==============================] - 9s 8ms/step - loss: 1.7972 - accuracy: 0.3254 - val_loss: 1.7432 - val_accuracy: 0.3581\n",
      "Epoch 34/100\n",
      "1172/1172 [==============================] - 9s 8ms/step - loss: 1.7923 - accuracy: 0.3278 - val_loss: 1.8225 - val_accuracy: 0.3388\n",
      "Epoch 35/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.7818 - accuracy: 0.3362 - val_loss: 1.7097 - val_accuracy: 0.3731\n",
      "Epoch 36/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.7734 - accuracy: 0.3405 - val_loss: 1.7008 - val_accuracy: 0.3798\n",
      "Epoch 37/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 1.7640 - accuracy: 0.3482 - val_loss: 1.6865 - val_accuracy: 0.3861\n",
      "Epoch 38/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.7582 - accuracy: 0.3500 - val_loss: 1.6799 - val_accuracy: 0.3878\n",
      "Epoch 39/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.7479 - accuracy: 0.3587 - val_loss: 1.6917 - val_accuracy: 0.3870\n",
      "Epoch 40/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.7446 - accuracy: 0.3589 - val_loss: 1.6921 - val_accuracy: 0.3883\n",
      "Epoch 41/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 1.7361 - accuracy: 0.3625 - val_loss: 1.6854 - val_accuracy: 0.3906\n",
      "Epoch 42/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 1.7257 - accuracy: 0.3677 - val_loss: 1.6499 - val_accuracy: 0.3990\n",
      "Epoch 43/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.7251 - accuracy: 0.3715 - val_loss: 1.6740 - val_accuracy: 0.3977\n",
      "Epoch 44/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.7183 - accuracy: 0.3692 - val_loss: 1.6553 - val_accuracy: 0.3955\n",
      "Epoch 45/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.7046 - accuracy: 0.3787 - val_loss: 1.6675 - val_accuracy: 0.4026\n",
      "Epoch 46/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.6994 - accuracy: 0.3785 - val_loss: 1.6306 - val_accuracy: 0.4078\n",
      "Epoch 47/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6983 - accuracy: 0.3810 - val_loss: 1.6436 - val_accuracy: 0.4023\n",
      "Epoch 48/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.6899 - accuracy: 0.3812 - val_loss: 1.6414 - val_accuracy: 0.4101\n",
      "Epoch 49/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.6823 - accuracy: 0.3879 - val_loss: 1.6443 - val_accuracy: 0.4077\n",
      "Epoch 50/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 1.6719 - accuracy: 0.3894 - val_loss: 1.6255 - val_accuracy: 0.4126\n",
      "Epoch 51/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 1.6731 - accuracy: 0.3890 - val_loss: 1.6112 - val_accuracy: 0.4162\n",
      "Epoch 52/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.6705 - accuracy: 0.3924 - val_loss: 1.6084 - val_accuracy: 0.4211\n",
      "Epoch 53/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.6546 - accuracy: 0.3973 - val_loss: 1.6361 - val_accuracy: 0.4128\n",
      "Epoch 54/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.6559 - accuracy: 0.3949 - val_loss: 1.6086 - val_accuracy: 0.4157\n",
      "Epoch 55/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.6486 - accuracy: 0.3983 - val_loss: 1.6009 - val_accuracy: 0.4210\n",
      "Epoch 56/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.6442 - accuracy: 0.4021 - val_loss: 1.5929 - val_accuracy: 0.4234\n",
      "Epoch 57/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.6461 - accuracy: 0.4001 - val_loss: 1.6098 - val_accuracy: 0.4198\n",
      "Epoch 58/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.6350 - accuracy: 0.4076 - val_loss: 1.5917 - val_accuracy: 0.4244\n",
      "Epoch 59/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.6344 - accuracy: 0.4072 - val_loss: 1.5887 - val_accuracy: 0.4311\n",
      "Epoch 60/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6277 - accuracy: 0.4114 - val_loss: 1.5890 - val_accuracy: 0.4309\n",
      "Epoch 61/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.6248 - accuracy: 0.4100 - val_loss: 1.5954 - val_accuracy: 0.4290\n",
      "Epoch 62/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.6127 - accuracy: 0.4172 - val_loss: 1.5902 - val_accuracy: 0.4292\n",
      "Epoch 63/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.6123 - accuracy: 0.4173 - val_loss: 1.5796 - val_accuracy: 0.4288\n",
      "Epoch 64/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.6011 - accuracy: 0.4172 - val_loss: 1.5731 - val_accuracy: 0.4353\n",
      "Epoch 65/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.6024 - accuracy: 0.4223 - val_loss: 1.5676 - val_accuracy: 0.4368\n",
      "Epoch 66/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.6033 - accuracy: 0.4207 - val_loss: 1.5571 - val_accuracy: 0.4445\n",
      "Epoch 67/100\n",
      "1172/1172 [==============================] - 9s 8ms/step - loss: 1.5894 - accuracy: 0.4266 - val_loss: 1.5700 - val_accuracy: 0.4369\n",
      "Epoch 68/100\n",
      "1172/1172 [==============================] - 9s 8ms/step - loss: 1.5887 - accuracy: 0.4290 - val_loss: 1.5491 - val_accuracy: 0.4440\n",
      "Epoch 69/100\n",
      "1172/1172 [==============================] - 9s 8ms/step - loss: 1.5878 - accuracy: 0.4275 - val_loss: 1.5633 - val_accuracy: 0.4340\n",
      "Epoch 70/100\n",
      "1172/1172 [==============================] - 9s 8ms/step - loss: 1.5831 - accuracy: 0.4270 - val_loss: 1.5569 - val_accuracy: 0.4388\n",
      "Epoch 71/100\n",
      "1172/1172 [==============================] - 9s 8ms/step - loss: 1.5799 - accuracy: 0.4299 - val_loss: 1.5683 - val_accuracy: 0.4386\n",
      "Epoch 72/100\n",
      "1172/1172 [==============================] - 9s 8ms/step - loss: 1.5720 - accuracy: 0.4346 - val_loss: 1.5469 - val_accuracy: 0.4441\n",
      "Epoch 73/100\n",
      "1172/1172 [==============================] - 9s 8ms/step - loss: 1.5703 - accuracy: 0.4355 - val_loss: 1.5687 - val_accuracy: 0.4413\n",
      "Epoch 74/100\n",
      "1172/1172 [==============================] - 9s 8ms/step - loss: 1.5677 - accuracy: 0.4329 - val_loss: 1.5475 - val_accuracy: 0.4489\n",
      "Epoch 75/100\n",
      "1172/1172 [==============================] - 9s 8ms/step - loss: 1.5623 - accuracy: 0.4368 - val_loss: 1.5622 - val_accuracy: 0.4394\n",
      "Epoch 76/100\n",
      "1172/1172 [==============================] - 9s 8ms/step - loss: 1.5555 - accuracy: 0.4394 - val_loss: 1.5456 - val_accuracy: 0.4467\n",
      "Epoch 77/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 1.5575 - accuracy: 0.4415 - val_loss: 1.5221 - val_accuracy: 0.4510\n",
      "Epoch 78/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.5505 - accuracy: 0.4456 - val_loss: 1.5343 - val_accuracy: 0.4522\n",
      "Epoch 79/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.5498 - accuracy: 0.4428 - val_loss: 1.5355 - val_accuracy: 0.4546\n",
      "Epoch 80/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.5380 - accuracy: 0.4482 - val_loss: 1.5258 - val_accuracy: 0.4551\n",
      "Epoch 81/100\n",
      "1172/1172 [==============================] - 9s 8ms/step - loss: 1.5397 - accuracy: 0.4460 - val_loss: 1.5234 - val_accuracy: 0.4530\n",
      "Epoch 82/100\n",
      "1172/1172 [==============================] - 9s 8ms/step - loss: 1.5364 - accuracy: 0.4490 - val_loss: 1.5257 - val_accuracy: 0.4566\n",
      "Epoch 83/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.5357 - accuracy: 0.4516 - val_loss: 1.5153 - val_accuracy: 0.4581\n",
      "Epoch 84/100\n",
      "1172/1172 [==============================] - 9s 8ms/step - loss: 1.5256 - accuracy: 0.4553 - val_loss: 1.5174 - val_accuracy: 0.4549\n",
      "Epoch 85/100\n",
      "1172/1172 [==============================] - 9s 8ms/step - loss: 1.5272 - accuracy: 0.4558 - val_loss: 1.5257 - val_accuracy: 0.4582\n",
      "Epoch 86/100\n",
      "1172/1172 [==============================] - 9s 8ms/step - loss: 1.5217 - accuracy: 0.4594 - val_loss: 1.5258 - val_accuracy: 0.4536\n",
      "Epoch 87/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.5244 - accuracy: 0.4539 - val_loss: 1.5151 - val_accuracy: 0.4575\n",
      "Epoch 88/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.5136 - accuracy: 0.4579 - val_loss: 1.5180 - val_accuracy: 0.4579\n",
      "Epoch 89/100\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 1.5150 - accuracy: 0.4575 - val_loss: 1.5295 - val_accuracy: 0.4610\n",
      "Epoch 90/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.5099 - accuracy: 0.4615 - val_loss: 1.5292 - val_accuracy: 0.4586\n",
      "Epoch 91/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.5017 - accuracy: 0.4653 - val_loss: 1.5124 - val_accuracy: 0.4597\n",
      "Epoch 92/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.5094 - accuracy: 0.4603 - val_loss: 1.4992 - val_accuracy: 0.4664\n",
      "Epoch 93/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 1.5000 - accuracy: 0.4644 - val_loss: 1.5074 - val_accuracy: 0.4636\n",
      "Epoch 94/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.4963 - accuracy: 0.4651 - val_loss: 1.5295 - val_accuracy: 0.4588\n",
      "Epoch 95/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 1.4919 - accuracy: 0.4670 - val_loss: 1.4935 - val_accuracy: 0.4694\n",
      "Epoch 96/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.4948 - accuracy: 0.4660 - val_loss: 1.4999 - val_accuracy: 0.4652\n",
      "Epoch 97/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.4902 - accuracy: 0.4682 - val_loss: 1.5042 - val_accuracy: 0.4670\n",
      "Epoch 98/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.4843 - accuracy: 0.4703 - val_loss: 1.5017 - val_accuracy: 0.4652\n",
      "Epoch 99/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.4823 - accuracy: 0.4727 - val_loss: 1.5020 - val_accuracy: 0.4714\n",
      "Epoch 100/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 1.4738 - accuracy: 0.4730 - val_loss: 1.4952 - val_accuracy: 0.4688\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 1.4952 - accuracy: 0.4688\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 1.4845 - accuracy: 0.4698\n",
      "Best validation performance: 49.46% for lr: 0.0001\n",
      "Best test performance: 49.66% for lr: 0.0001\n",
      "validation accuracy: [0.2040800005197525, 0.4183200001716614, 0.4853599965572357, 0.4946399927139282, 0.46880000829696655]\n",
      "test accuracy: [0.19979999959468842, 0.4242999851703644, 0.4876999855041504, 0.4966000020503998, 0.4697999954223633]\n",
      "model: models/dropout_0.001_model.h5\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 1.9307 - accuracy: 0.2468\n",
      "validation accuracy: 24.68%\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 1.9178 - accuracy: 0.2556\n",
      "test accuracy 25.56%\n",
      "model: models/dropout_0.0005_model.h5\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 1.6080 - accuracy: 0.4210\n",
      "validation accuracy: 42.10%\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 1.5985 - accuracy: 0.4303\n",
      "test accuracy 43.03%\n",
      "model: models/dropout_0.00025_model.h5\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 1.4529 - accuracy: 0.4961\n",
      "validation accuracy: 49.61%\n",
      "313/313 [==============================] - 0s 2ms/step - loss: 1.4409 - accuracy: 0.4913\n",
      "test accuracy 49.13%\n",
      "model: models/dropout_0.0001_model.h5\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 1.4315 - accuracy: 0.4978\n",
      "validation accuracy: 49.78%\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 1.4230 - accuracy: 0.5012\n",
      "test accuracy 50.12%\n",
      "model: models/dropout_5e-05_model.h5\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 1.4935 - accuracy: 0.4694\n",
      "validation accuracy: 46.94%\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 1.4796 - accuracy: 0.4737\n",
      "test accuracy 47.37%\n",
      "Best test accuracy with 0.1 dropout: 50.12% with lr: 0.0001\n"
     ]
    }
   ],
   "source": [
    "#0.2 dropout rate was terrible so trying with 0.1\n",
    "def dropout_model_new():\n",
    "    return make_DNN_dropout(elu_layer,20,False,0.1)\n",
    "\n",
    "lrs=[0.001, 0.0005, 0.00025, 0.0001, 0.00005] # back to original he values\n",
    "\n",
    "valid_accuracy,test_accuracy,logs, best_models = tune_lrs(dropout_model_new, lrs, \"dropout\")\n",
    "\n",
    "print(f\"validation accuracy: {valid_accuracy}\")\n",
    "print(f\"test accuracy: {test_accuracy}\")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "test_acc = []\n",
    "for _,model_file in enumerate(best_models):\n",
    "    model = dropout_model_new()\n",
    "    model.compile(loss='sparse_categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "    model.load_weights(model_file)\n",
    "    print(f\"model: {model_file}\")\n",
    "    print(f\"validation accuracy: {model.evaluate(X_valid,y_valid)[1]:.2%}\")\n",
    "    test_acc.append(model.evaluate(X_test,y_test)[1])\n",
    "    print(f\"test accuracy {test_acc[-1]:.2%}\")\n",
    "\n",
    "print(f\"Best test accuracy with 0.1 dropout: {np.max(test_acc):.2%} with lr: {lrs[np.argmax(test_acc)]}\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Choice of dropout percentage is critical 20% failed completely, 10% worked reasonably well\n",
    "* Not full tuned because of time but roughly equivalent performance to Nadam with batch normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2556000053882599, 0.4302999973297119, 0.49129998683929443, 0.5012000203132629, 0.47369998693466187]\n",
      "['models/dropout_0.001_model.h5', 'models/dropout_0.0005_model.h5', 'models/dropout_0.00025_model.h5', 'models/dropout_0.0001_model.h5', 'models/dropout_5e-05_model.h5']\n"
     ]
    }
   ],
   "source": [
    "print(test_acc)\n",
    "print(best_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 1.4230 - accuracy: 0.5012\n",
      "test accuracy: 50.12%\n"
     ]
    }
   ],
   "source": [
    "model = dropout_model_new()\n",
    "model.compile(loss='sparse_categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "model.load_weights(\"models/dropout_0.0001_model.h5\")\n",
    "print(f\"test accuracy: {model.evaluate(X_test,y_test)[1]:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5015\n"
     ]
    }
   ],
   "source": [
    "#Monte-carlo dropout on best performing on validation set\n",
    "import numpy as np\n",
    "\n",
    "model = dropout_model_new()\n",
    "model.compile(loss='sparse_categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "model.load_weights(\"models/dropout_0.0001_model.h5\")\n",
    "\n",
    "\n",
    "#This is softmax classification so prediction will be probabilities for the 10 types\n",
    "y_preds = np.stack([model(X_test,training=True) for sample in range(2000)])\n",
    "y_pred = y_preds.mean(axis=0)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "accuracy = np.sum(y_pred == y_test)/len(y_test)\n",
    "print(accuracy)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5015\n"
     ]
    }
   ],
   "source": [
    "y_pred_1 = y_preds.mean(axis=0)\n",
    "y_pred_2 = np.argmax(y_pred_1, axis=1)\n",
    "accuracy = np.sum(y_pred_2 == y_test)/len(y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analysis of y_preds\n",
    "\n",
    "#so I don't accidentally mess up 12 minute of predictions\n",
    "my_copy = y_preds.copy()\n",
    "acc=[]\n",
    "\n",
    "for _ in range(2,2000):\n",
    "    preds = my_copy[:_,:,:]\n",
    "    pred_1 = preds.mean(axis=0)\n",
    "    y_pred = np.argmax(pred_1, axis=1)\n",
    "    accuracy = np.sum(y_pred == y_test)/len(y_test)\n",
    "    acc.append(accuracy)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 1.4796 - accuracy: 0.4737\n",
      "47.37%\n",
      "0.4723\n"
     ]
    }
   ],
   "source": [
    "#Trying on a sub-optimal model\n",
    "import numpy as np\n",
    "\n",
    "model = dropout_model_new()\n",
    "model.compile(loss='sparse_categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "model.load_weights('models/dropout_5e-05_model.h5')\n",
    "print(f\"{model.evaluate(X_test,y_test)[1]:.2%}\")\n",
    "\n",
    "\n",
    "#This is softmax classification so prediction will be probabilities for the 10 types\n",
    "y_preds_sub = np.stack([model(X_test,training=True) for sample in range(2000)])\n",
    "y_pred_sub_2 = y_preds_sub.mean(axis=0)\n",
    "y_pred_sub = np.argmax(y_pred_sub_2, axis=1)\n",
    "accuracy_sub = np.sum(y_pred_sub == y_test)/len(y_test)\n",
    "print(accuracy_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analysis of y_preds\n",
    "\n",
    "#so I don't accidentally mess up 12 minute of predictions\n",
    "my_copy = y_preds_sub.copy()\n",
    "acc_sub=[]\n",
    "\n",
    "for _ in range(2,2000):\n",
    "    preds = my_copy[:_,:,:]\n",
    "    pred_1 = preds.mean(axis=0)\n",
    "    y_pred = np.argmax(pred_1, axis=1)\n",
    "    accuracy = np.sum(y_pred == y_test)/len(y_test)\n",
    "    acc_sub.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEWCAYAAABbgYH9AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAABViElEQVR4nO2dd5gUVda43zMBhiHnDEMyoCgqKooBMKxiwLAKpsXVXVe/9VNXdxWzrmvYXfXnun7qomvEgIprQkUXBbOSEQQlDTkHyTDh/P6oWz3V3dU9Pcx0zwyc93n66aqb6lR1dZ265957jqgqhmEYhlHVZFW3AIZhGMaeiSkYwzAMIy2YgjEMwzDSgikYwzAMIy2YgjEMwzDSgikYwzAMIy2YgjEQkf4isrSK2rpLREZWRVtGzUU8nhWRDSLyXQrlnxORv7jtKrvfqpPgOaVY/nci8kglj1kgIioiOZVpZzePXVdE5ohIq1TrmIIJQUQKRWSXiLSISZ/mftyCKjjGeBH5TWXbMVKjog8Do1yOAU4COqjqEdUtjE9NfcERkTrAbcDfq1uWVIl9RqnqTuAZ4KZU2zAFk5iFwAX+joj0AupVnzjpIZNvQtXx1lVbqIXXpjNQqKpbq1uQ6kBEsitYZTAwR1WXpUOeDPIyMExE6qZS2BRMYl4EfhXYHwa8ECwgIo1F5AURWSMii0TkNhHJcnmXisgXIvKgMyMsFJFTXd69wLHAYyKyRUQec+n7icjHIrJeRH4UkfOTCSgig12vapOIzBeRU1z6r0VktohsFpEFIvK7QJ3+IrJURG4SkZXAsyHt7u/eXjaKyCwROTOJDF1EZII71sdAi0Ce352/XEQWA5+ISJa7TotEZLW7fo1jyl8hIstFZIWI3BBor66IPOLylrvtusHrHSObikh3EbkCuAi40V3vdxOcyz9EZIm7npNF5NhAXraI3OKu82aX39HlHRD43VaJyC0uParXJDGmIddTvklEZgBbRSRHRIYHjvGDiJwdI+NvA7/tDyJyqIj8SURGx5T7p4SYY1z7b4Sc96OB67jAtb9QRC4KaeNy4GngKHc97052/cOudTLE64W85u6Nze4e7BPIbycio8X73y0UkWtc+inALcAQJ9d0ERkgIt8H6v5XAiY98f6jZ7nthPe9+y2fEJH3RWQrMCBG5oYi8qmIPCoiEnJapwITAuXzRGSkiKxzx5soIq1dXqGInBhzPWJ7ZZeF/UdCrmV5z6gv3b3ys3jmrxNcXugzSlWXAhuAvomOGYWq2ifmAxQCJwI/AvsD2cASvLc2BQpcuReAt4GGQAHwE3C5y7sUKAJ+6+pfBSwHxOWPB34TOGZ9d4xfAznAocBa4IAEMh4B/IxnpsgC2gP7ubzTgG6AAMcD24BDXV5/oBj4K1AXr1fWH1jq8nOBeXh/1DrAQGAzsG8COb4GHnZtHefKjnR5Be56veDOrx5wmWu/K9AAeBN4Mab8K658L2ANcKLL/zPwDdAKaAl8BdwTuN5fxMimQHe3/Rzwl3J+94uB5u763wCsBPJc3p+A74F93XU92JVtCKxw5fPc/pFhxwxe58B9Ng3oCNRzaecB7dxvOgTYCrQN5C0DDncydMe7J9u6ck1cuRxgNXBYyDl2dvdDI7ef7eTv6675Jv+3du0muv+irndFrn/sdQhp+y5gBzDIyXc/8I3LywImA3fg3Z9dgQXALwJ1RwbaygO247345LjfdLn7neq5vOaUc987+X8G+jkZ8vxzcvW/I8n9BUwEzgvs/w54F8h353hY4DcpxN3zsedEOf+RkOOW94wqBv7gzn+IO8dmYc+oQJvvANek8iy1Hkxy/F7MScAcvD83EOkiDwFuVtXNqloIPARcEqi/SFWfUtUS4Hm8P2zrBMc6Hc/k8KyqFqvqFGA08MsE5S8HnlHVj1W1VFWXqeocAFUdo6rz1WMC8BHe24hPKXCnqu5U1e0x7fbFe/A/oKq7VPUT4D0C5sLANeiE97C73bX1Gd6fJpa7VHWrO9ZFwMOqukBVtwA3A0Ml2kR0tyv/PV4Pyz/2RcCfVXW1qq4B7ib6elcKVR2pquvc9X8IT2nu67J/A9ymqj+66zpdVdfh/W4rVfUhVd3h7oVvK3DYR1V1if87qOrrqrrc/aajgLl4LxO+DH9T1YlOhnmqukhVVwCf4SkggFOAtao6OeQcFwFTgLNc0kBgm6p+4/ZLgQNFpJ6qrlDVWRU4l6rkC1V93/13XsRT6ODdby1V9c/u/lwAPAUMDWtEVXcAk/BefvoAM4Av8BRFX2Cu+x1Tue/fVtUv3W+zw6W1w+uZvK6qtyU5nyZ4CsunCE8xdVfVElWdrKqbUrguPon+IxFSfEatBh5R1SJ3v/2I94KajM3ufMrFFExyXgQuxNP0L8TktcB701kUSFuE15PwWelvqOo2t9kgwbE6A0e67vJGEdmI90BtIyKdXDd1i4hsceU7AvPDGhKRU0XkG2ey2Yj3JhicsLAm8AeJpR2wRFVLk5xXsOwGjbbDLwoptySmTuw1yyFa8S6JyW+XpG47qggRucGZn352160xZdct0fVO+DukSPBcEZFfiWf29O+BA1OQAbwXmIvd9sV4924iXqbsgXSh28f9jkOAK4EVIjJGRPar2OlUDBG5KHBvfxDIWhnY3gbkuZeQzkC7mP/JLSR+cQNPAfTHUzIT8N7Mj3cf32yVyn0f9Vs5TsPrCT2Z7DzxzEoNA/svAmOBV52p628ikltOG0ES/UeCpPKMWqauW1JOW0EaAhtTEdIUTBLc295CvAf0mzHZa/HeQjoH0joR6OWU13zM/hJggqo2CXwaqOpVqrrYbTdQ1QaB8t1iGxVvTGI08CDQWlWbAO/jmVQSHTvIcqCjb6ct57xWAE1FpH5M2ViCx1tO/DUrBlYF0jrG5C9PUtfP24pnbgBARNokkSEO8cZbbgLOB5q66/YzZdct9HonSY+TCYiVKUouEemM9zZ+NdDcyTAzBRkA3gIOEpED8XpVLyUoB/A60F9EOgBn4xQMgKqOVdWT8Hrbc5w8qVDe9Q9FVV8K3NunplBlCbAw5n/SUFUH+U2G1IlVMBOIVzCp3PdhbT8FfAi8H/M/iGUGsE+kIa/HcLeq9gSOxvvN/DHfVO6bRP+RIKk8o9rHjBkF20r0n9kfmJ4gLwpTMOVzOTAw5i0d13V/DbjXDfB1Bq4HUp0iuQrPfuzzHrCPiFwiIrnuc7iI7J+g/r+BX4vICeINnLd3b5t18Ew7a4Bi8SYWnJzqyQLf4t3gNzoZ+gNnAK/GFnQKeBJwt4jUEZFjXNlkvAL8QbzJAQ2A+4BRqlocKHO7iOSLyAF4Y1KjAnVvE5GW4k0hv4Oy6z0dOEBEeotIHp7dOkjs9Y6lIZ6iWwPkiMgdQKNA/tPAPSLSQzwOEpHmeL9bGxG5TrxJCA1F5EhXZxowSESauQfudeVcm/p4f+o14E3WwOvBBGX4o4gc5mTo7u473xT0Bp6y+E5VFyc6iDMvjsczrSxU1dnueK1F5Ez3oNwJbAFKypHZp7zrX1V8B2wSb3JEPfEmXxwoIoe7/FVAQYyi+ArP1HkE3rWZhbMY4JkWoQL3fQhX45mW3hORRDNN38dTaACIN/mglzNjbcJTBP61noZnNs4Vb3JDmJk80X8kQorPqFbANe5Y5+Epj/ddXtx/RkTaA83wxkLLxRRMObixjEkJsv8X76ZcgGfXfRlvnngq/AP4pXgzzB5V1c14imAo3hvESsoG4sPk+g7vxvp/eG/aE4DOrp1r8G6sDXgmkHdSlAlV3QWciTfrZS3wOPArdeM7IVyI90ddD9xJvCkxlmfwzAOf4fUOd+BdxyAT8AZcxwEPqupHLv0veAptBt6A+xSXhqr+hDcJ4L944xZfxLT5b6CnM6u8FSLXWOADvEHQRU6uoBniYbxr+hHeA+HfeAPzm/HG6M7A+83mUjbD6EW8B2+hqxf3EAiiqj/g2ci/xvtz9wK+DOS/DtyLd59txuu1NAs08byrk8w85vMy3kSWlwNpWXiTFZbj/Z7HA/+TQlupXP8qwT00zwB6490/a/EUb2NX5HX3vU5Eprg6W/HulVnu/gbvGi9S1dWuTEXv+6BMClyBd7+87RRsLO8C+4mIb35qg/dCsAmYjXfP+w/+2/F6qhvwxhlfJp5E/5FYyntGfQv0wDvne4FfujEpiHlGubQLgefVWxNTLv6MJsOodsRbwLoQyI3p0RgpIN6kizlAmwoOGBsZQLzp8j1V9brqlgW8acp4s8SOSbF8XbwXpuN8xVwetW1xl2EYITiT0PXAq6ZcaiaqOqK6ZagMrtdSoUkfpmAMo5bjxkxW4Zn2TqlmcQwjgpnIDMMwjLRgg/yGYRhGWjATmaNFixZaUFBQ3WIYhmHUKiZPnrxWVVuG5ZmCcRQUFDBpUqLZyIZhGEYYIhLmvQMwE5lhGIaRJkzBGIZhGGkh7QrGuXKYKiLvuf1R4jnzmyZe3INpCeo1EZE3xItRMFtEjnLpzcSLvTHXfTd16f1EZIZ4cRW6B9oYG+NrxzAMw8gAmejBXIvnCgEAVR2iqr1VtTeeU8ZYJ5I+/wA+VNX98Fx1+20MB8apag88NwnDXfoNwLl4nlWvcmm3A/epzcU2DMPIOGlVMM5b62l4voJi8wTPc+0rIXmN8Dyf/hs8P0GqutFlD8bzuYT7PsttF+G5zc4HikSkG9BevXgohmEYRoZJ9yyyR4AbiY6D4HMssEpV54bkdcXzKPusiByMF8HuWue0rrULsISqrhCRVq7O/cAIvAh1l+C5q7+9Cs/FMAzDqABp68GIyOnA6rCoeo4LCOm9OPyQwU+o6iF43kCHJygLgKpOU9W+qjoAT0Et98SQUeLFvo4LSCRe7PdJIjJpzZo1KZ6ZYRiGkQrpNJH1A84UkUK8mAoDRWQkgHiR6c4hsfvypXgxu/3Qs2/gKRyAVSLS1rXTFi/kZwRnersNuAfPffydeG6wr4k9iKqOUNU+qtqnZcvQdUI1knVbdvLhzBXVLYZhGEZS0qZgVPVmVe2gqgV4MU4+UVU/pOuJwBxVXZqg7kpgiYj48dBPAH5w2+8Aw9z2MODtmOrDgDGqugFvPKbUffLZQ7jsuYlcOXIKP28vqm5RDMMwElJdK/mHEmMec4F4ng6EPv1f4CURqYMXLOfXLv0B4DURuRxYDJwXaCMfT8H4ERwfxpuptouyGOS1nsJ12wCwyXGGYdRkMqJgVHU8XohWf//SkDLLgUGB/WlAn5By6/B6NGHH2UZZNEFU9XO8CH97DGu37Iz0XEy/GIZRk7GV/LWM0ZPLrIrFpaZhDMOouZiCqWVkBZwSlJiCMQyjBmMKppaRlRVQMGYjMwyjBmMKppYRdKpWUmIKxjCMmospmFrG7BWbItubd9o0ZcMwai6mYGoZrwcG+e8dMztJScMwjOrFFEwtZu2WndUtgmEYRkJMwdRi6uVmV7cIhmEYCTEFU4s5+5D21S2CYRhGQkzB1CKKS0qj9rOzam6gzh1FJbZOxzD2ckzB1CKGjvgmar8mr+Tf7/YPuW7UtOoWwzCMasQUTC1i0qINUfs1vYfw7vTl1S2CYRjViCmYWsz4H2tmkLSlG7ZVtwiGYdQATMHUYr6Yt7a6RQjlxIcnVLcIhmHUAEzBGFXOjqLS8gsZhrHHYwqmFjCxcD297hwbmleTgo7tLC7hN89PCs3btquYS5/9jkXrtmZYKsMwqgtTMLWAXz87kc07i0PzatJA/7TFG/nv7FWheZ/OWcP4H9fw1w/nZFgqwzCqC1MwtQBJstylJk1VTrYuZ1dJSQYlMQyjJpB2BSMi2SIyVUTec/ujRGSa+xSKyLQE9QpF5HtXblIgvZmIfCwic913U5feT0RmiMhEEenu0pqIyFiRZI/omk9WEvE/mbM6g5IkJ9ll/sOo6RmUxDCMmkAmejDXAhG3v6o6RFV7q2pvYDTwZpK6A1zZPoG04cA4Ve0BjHP7ADcA5wK3AFe5tNuB+7QmDVTsBsl6BuN/rDkKJpXLLNRqXW8YRgXISWfjItIBOA24F7g+Jk+A84GBFWx2MNDfbT8PjAduAoqAekA+UCQi3YD2qlrr58wm8whTUoMmbH2/7Oe4tCfGz2eJrYsxjL2StCoY4BHgRqBhSN6xwCpVnZugrgIfiYgC/1LVES69taquAFDVFSLSyqXfD4wAtgOXAA/i9WASIiJXAFcAdOrUKdVzqgYSa5ia1Dm7+90f4tLiBvWtA2MYew1pM5GJyOnAalWdnKDIBcArSZrop6qHAqcCvxeR45IdT1WnqWpfVR0AdAWWe2LIKBEZKSKtQ+qMUNU+qtqnZcuWKZ1XdXNg+0YUPnBaZL+0BimYVDD9Yhh7D+kcg+kHnCkihcCrwEARGQkgIjnAOcCoRJVVdbn7Xg38BzjCZa0SkbaunbZA1CCEM73dBtwD3Ok+I4FrqurEMslL3y6KCiyWHTOQXrvUS/nMXPYz17wyNW769YSf1nDXO7OqSSrDMHaHtCkYVb1ZVTuoagEwFPhEVS922ScCc1R1aVhdEakvIg39beBkYKbLfgcY5raHAW/HVB8GjFHVDXjjMaXuk18lJ5Zhbv3PzKj9WIVySMcmGZOlKihvQt/vX57CO9OXs3h99LjNsGe+47mvCtMomWEYVU26x2ASMZQY85iItAOeVtVBQGvgP+5hlAO8rKofuqIPAK+JyOXAYuC8QBv5eArmZJf0MN5MtV14JrlaT6xFrMTtl5YqparkZFf+ncHvPaQj3syOouTrYYrdCSU6dEmp1ug4OIZhlJERBaOq4/Fme/n7l4aUWQ4MctsLgIMTtLUOOCFB3jZgQGD/c6DXbgtezcxdtTkuTWP6MEVuGtlZj3/JjKU/R43P7C773/EhnZrl89/rj690W7F8/MMq1mzeScuGdUPzl23cnrT+jqIS6tetrvciwzAqgq3kr8GETfv1+egP3pyHomJPwcxYmrhsRdlVXMq81VuqrL1Y1mzeWW6ZopLw0aXt5fSADMOoOZiCqcGE+RnzTWQ9WjUA4JuF66LyZyzdmG6xKs3EwvXllilKsMBn+y5TMIZRWzAFU4MJm4Ls6xx/sPzLedEK5uzHv0q7XJXlzndmsTWB806f4gQ9mPLGcAzDqDmYgqnBhPmxLG9hZXV6V66Tk8V+bcLW1MZT3vqdXQl6MBZrxjBqD6ZgajBhymJuyNjIzuL0vNWn2lvYUVRC7z9/xK7iUurmZgPJ3dtAuPIMsnzjdgqGj+GBD6I9AcxZuSklmQzDqH5MwdRgwt7yw5ROut7qx/+4JqVyS9ZvY+O2Im/HyRwU89f9CuLqlJajYca5uDJPTpgflb7NxmAMo9ZgCqYGU95DOFG5VP2TlZRq0kHzDdt2sW1X8rESgE07yspsjWnv1APbcOcZB8QfuxwZE80W21FUgqqmfG0Mw6g+TMHUYFJ9hsY+rF/5bklK9a4bNY0j7vtvVFpwCvHNb35PzzvGljuu821gJpuvsBq4tSr7t20ExJvMylMQY2eVRcYMHv/+D+Zwyb+/o+st7yetbxhG9WMKpgYTZiKbdNuJke3fHdcViDebvTYpNQXz7vTlbN4R3UNZHrLQcUs5M76CYvqD88d0b8FLvzmS3w/oDsDXN5/AmGuOiZQL68Ek6nnFHv+LeWuTymMYRs3AFEwNJqzn0KJB2Qr4Li3qA/ELF4tLKzYmE3yw182NvyU27yiKS5u57GcWr/P8hQUnAxQ7BZOdLfTr3iLi1qV1ozx6ut4MhJ/bzuJwud+etiyV06CkVBk7a2WNCWEwY+lGnv58QXWLYRjVhimYWkyWe3if80T02pdEa0gSEXweh+mmrTvjx0NO/+cXHPf3T4HoxY/Fvh+zEKeWQUeXYceZszLeNQ7AHW+n5kX5ua8K+d2Lk3ln+vKUyqebMx/7kr+Mmc2S9RZwzdg7MQVTgymOecvPi+ld5DgFsyvmzT/RKvhEBM1VYb2f8sZgggPyftmccuYph5nINm33ekq52bvnzHKFM++t2rRjt+qnC5v5ZuytmIKpAUxetJ57x0RHg9y4bRd/H/tj0nqJvArPX7OV1RV4yAYVSJhy8vNXb9pBwfAxfD2/bFB//dZdvPTt4pRl8xnw4Pi4NH+spWl+nZTkTnTMmhBGOugO5y9jfgg1223eUcS1r05l/dZdmRTNMDKGKZgawLlPfM1Tny+MSnti/Py4cm9ceXTKbT4QG6o4CcHJBLuK4x+Efq/mrnc9U9UFT30TyXv803lRZT+5oT9DD+/I8FP3S/n4Pr77mNUpOMMMwzfB1YQon898UfZ7fj53bajzztcmLeXtact5dFyiqOGGUbsxBVOD8KfuqmrczKkrj+/Gge0bR6UlC94VNmCuqqFv0iWB44a5aPEf2Bu2xg/2B814hQ+cRpvGeTxw7kE0bxDujv/sQ9qHygWww8ncKC/cHf9FR3YKTffxQ+FU9yC/qsat4wlTer4pMJW1RoZRGzEFU4PwxyW63Px+nNkpbEwjmREqdlymtFTpcvP7ca5XAHrd9RGjJy+l110fMeyZ7+Ly/UkDXy9YF5f3VoozvHy6Oy/QAK98t5gVP2+ny83vUzB8DLe/5UXvPKxz09C6+XWyo/bnrY6eFOBPLHjwo58ArxdRMHwMP2+LV4zp4F8T5lMwfAxdbn4/zgtC2DhWrtOIr00KDexqGLUeUzA1iGSD6VlhCiaJhokdS/F7Gv/6LHza7OgpSxOud0kmV/sm9QD45WEdEgsT4Mrju0W2/zN1GQvXbo0rc0fIyn+AwwuaRe1PWxIdAye2Rzfy20UArNmSmUH/F75elDAvdsKGYewNmIKpQVTUE7Ik6cOM/3FNlKkotu0v5kYvVoy14ASf1ZMWbWD15vCH9KzlnvPJVL0oZ2cJ/bo3BzzT0OTCDVH5WVLmBaBZ/ejB/lKFOoGQ0B98v4K3py3jmwXrWPnzDv4RM5axYM1Wdy7CtwvW8c9xc6vUfLZpRxFfzU9t0WfYb1ua5PcxjD2BtCsYEckWkaki8p7bHyUi09ynUESmpVrXpTUTkY9FZK77burS+4nIDBGZKCLdXVoTERkryQYrahDJ/HO9HrI6vzyPxW9MLjO9FMVMP774399G7ceavy7v1yWy/fDHP3HEveOSHisvNztpfhD/Ybp0w3Ye+vinqLycrCwa1fMUzOXHdOHCwLjL/m0bkhW4Y8fNWc21r05j6Ihv6Ht/tHzBxZ9ZIgwZ8Q0PffwTP62qukidV744mQuf+pZNIQtRYwlTIMG08rwlGEZtJBM9mGuB2f6Oqg5R1d6q2hsYDbyZal3HcGCcqvYAxrl9gBuAc4FbgKtc2u3AfVrdo74pksw/14qf43sQsWrzV0d15sguZWakRevKFviVVHDx5WXHdOGdq/ulXL4iCsY/zbBwAKWq1M3JpvCB0/j9gO5c0rdzJK9z8/op+2cLmqRKAsq1KgfUpy7eCCT+3fyemidDmBfssvM3BWPsiaRVwYhIB+A04OmQPAHOB16pYN3BwPNu+3ngLLddBNQD8oEiEekGtFfVCZU7i8xRcTt9tIbJycqKWn/y2KfzIqatYNs3vjG93JZzs7PKXcsSJHYRaFIiCiZ+xlrsNYhddBk7eSERQYU6bvbqpPX/+uEcJi8qM9WpKne/O4tZy8vGeEpKlQuf+oaC4WM4/8mveXz8vMhMMX8K8rIYP25BE+ZNo2fEKZng+d/3/uyUfcgZRm0h3T2YR4AbgbCnwrHAKlVNtAggUd3WqroCwH23cun3AyOA64DHgHvxejAJEZErRGSSiExasya12CfppLQ0fBoxwN9+eVBcWmwP5uK+nciKSfzrB95izeDDLZVZS83q1yEnK/XbIy8n9R7MQ+cfnHLZ3SVobrw/MHMudhq2qvLE+PmcG3C3s2l7Mc9+WciFT5WZEVdv3sFXboHpd4Xr+duHZYtg/XVCXVvWj2r7m4DZccJPa+KCpQV7MGNmrODGN2akfoKGUQtIm4IRkdOB1ao6OUGRC0jceymvbhyqOk1V+6rqAKArsNxrSkaJyEgRaR1SZ4Sq9lHVPi1btkz1UGmjRDVusN2nR2B6r09QlTTMy6FrywZxSsef3hy2Qr+geX7osfrv25LsLCG7nLvDn0EGFTORdWwWftww/N7Bvq1Tm0TQvkk9CprnJ3T4uTOm1xRmuvLrBp18Jus5+dO4m+XX4dBOTSLpDfNy+MfQ3pF91bJ4NpA4UFxxSWmF3f0YRk0knT2YfsCZIlIIvAoMFJGRACKSA5wDjKpoXWCViLR17bQFVgcrOtPbbcA9wJ3uMxK4psrOrBI89slcCoaPCR1/KCnVhKvQm4S4TwnOXWiUlwsQ14PJzRFe/nYxx/7t07j6dRP0Ovz1JNnl9GD2Dcwcq5CJrAL4a1/2b5uagsnLzaKoRBNOSvjNC5N48ZtFHPe3Tznp4QmhZsnz/vU1EB2PJ9kU5KKSUt6auoxJizZEjaUc0qlplJlx7Zad9LzjQx7++CceHz+PZ75cSP2YtT2f/bSG7rd+wAF3jE3pfA2jJpM2BaOqN6tqB1UtAIYCn6jqxS77RGCOqobaasqp+w4wzG0PA96OqT4MGKOqG/DGY0rdJ/XX5jTytHMhsjVkUNdTMNFpHZvV46XfHBlxzR8kqEr8qb2xwyY5WVm8/F34wzHMNT+UKa5EDiv//suD+PSP/aPezhMpq8rSuXl9XvrNkdx/jmcibFg3fpX/eYE1OHVyshO6/fd5fdISFq/fxtzVW0J7Cv705iAjv0m+xuUlt+Zm+cayyRiPX3Qo9euUybtuyy5KFR4fP59nvigEPJNdg8A5+QtXwzwqGEZto7rWwQwlxjwmIu1EJJUwhQ8AJ4nIXOAkt++3kY+nYB53SQ/jzVS7H3iiCuTeLdZu2cn0JRuBsvUmEb9ZUbOd4nswDerm0q97i9B2gx2Mhnm+gonpwSTxTFw3J/rnb9XQc+/im8bCFncCnNenI11a1KdhXi7N3VqVdPVgAPp1b0G9SE+mUVRe60Z1+eMv9o3sZ2d51zsZM5aWDd4vWR8fYC3IJOe0MpnSWrBmCxPdep76dcsUbV5uduR3AXhknDclu6RUA6ZLjerlvDmlzDNCWBwew6hNZETBqOp4VT09sH+pqj4ZU2a5qg5Koe46VT1BVXu47/WBvG2qOkBVi9z+56raS1UPU9WfYtvOFIMf+5LB//elLyNQ1vt45ssyp4ilIWMwyfxvBWcpnXOo9xZ/9qHRvr5ys7MigcGS1YcyJ5P+Ay+sB3PaQW2j9i87xlsv0yKB77GqJtZjQLZIlJxFIc46kzHo0c+TH+/JryNB1BJx5cgpke06Tmn/4gBvyK9D07KOc1CZBXuPP28PVyRXjkx5CNIwaiS2kj8DBKev+o8/v6OxOBCMqjimB9O+ST0uDqwDiSPw/PcXJJ5+UDsKHzgtkp6TncWmHeFrLPp2ax6anhUZg4lXMMNPifaS/PsB3Zlzzyk0rb97LvaDFD5wWpTsYZx/eEfm3HNKZF9EyAnMRogd3D/j4HZMuu1EFt4f9+6SMkFz1cPlzIBT9c7jX5f0AaBN4zzeuPKouHJhkzZi8dfZGEZtRWrJGsS006zz/nrSLc+kpe1vF3qdrCO7NGNS4XpK1HPomJMlFK7dyirXc+jVvhF1c7KZFFiTEVw4GcvG7UX86KJAxpbzj9mhaT2Wbgg3A3Vuls+ikGiLzevXoXurBhSXatT6EIBDOjaJvKVXBl++IMnONVH9ujlZHNShccRElZeTFfHKDNCiQR26tWyQ8JhBmuTn0rlZPtMDJjSADk3qsdS9JHRv1YB5q5N7A4g9j+1FJVFmuYrgt7V5RzGbdhRFzdwzjJrAa1cePVlV+4TlWQ8mw0TUuVPsQfVeUV3fuJ43c6xTkmm/IpBoGCZ23KSxc9Hi965iqzWrX4fcKlAusfRo1YDOCaZMJ6JNozzq18lm3zYNo0x9jdw18Ul2bWLZuK0o1Fy1NNAD3VlcNvuvU7PUHvbJlqse0K4R3VrGT+CI5YcVmxK+KBhGTSU88MZeSNeW9Rn1u3hTRlVQMHwMAK/8ti+97hrL1l0lPHlJH1o2rMut//k+4pr/L2f3okvz+hz8548idXdXJv+Yw44q4NkvC+NWmYPntfiip8sWE17ctzP/9+l8jt+nFQ+dfzBbdxZzwJ1l02Wn3H7SbsmSTD6Aj68/vlJtqXqhCAAG9WrLc18VAnBW73Y8MvSQ0GMm4rJjuvDyt4uZs3JzeH6/Ltz9rhd99LMbB4a2GfubLVm/LXSaOMCYa44FvLUv3W/9IGFb/nGev+yICq05Mox089qVifOsB5NBiks10mPRkB7ME+PnVVk0xtaNvEH3L+atDVUuED/jzB+Q9hdgVsRVTHXiz8iLNR99MS81T8dB7nh7VkLlAhXvZQKRGXDJyEmwqrVg+Bh++8KkyP7mBONphlETMQWTQUpKy2aJ+bOTgw+ssbNWVZmCGXn5kQBxga/+cOI+ke1Y/XHaQW156ld9uKq/F7MlLzc7sjL93ENTi/dSXTz368N54fIjopTm2i3Rse4//WP/lNsb3Ltd6Nojf+X/6TGz6QCuHtCdL24aEJfeokFdLjgifjbgI0N6pyTLxz+simwHTXSGUdMxBZNBSlTx+zBliiRaoXwwc2WVHCs2lorPCfu3imzH9lDq5WZzUs/WUW/TR3fz1uBUdIwk0/TftxXdQlzlBAlTGIkoLlHuOKNnfLpTMGGD7X/8xb5R05KDDNg33hXRPim6vwmSqrNPw6gJmILJICUlZSv1y0xl0WVuc2GDK0si81Z2lnBg+0bUycmKhOz1id0HON49GIOu56uCgft5ii5ReOTdpW/XMjkvTLKGKEiY4hnz/QrqxlyPXu0bc5Sb2t1/X0/+i/umdox9QwKyxS6CPaBdo7gysZTnpcAwahI2yJ9BiktLI5olWeyXqiBRjLW6OVm8e/UxACwIhCtecF/4OpHDC5qx4L5BCVf17y7/HhY6q7HSnNSzNbPu/gX1crPLlfmnv5xKUUkp970/OzR0c6wrnRtO3ofeHZtEXY97Bh/In888MGnPCTyXN/41PvqBT1i5aUecW6B7z+7FWf/3Jb3aN+b7ZeHTmq0HY9QmrAeTQdZv3RVZtLdyk+ezKl3LkBI9W+vkZCEiiEiUG5NkD+OqVi5ARIZ0BButXzcnJZnr5GRRv25OwnU9dbKjB+d9s2awbREhKyu188jK8sq2dC55Yh2e+q57knlSHvH5Aj6cuYLrXp3K1MUbEpYzjJqAKZgM4rtiAbj1P98DJB3Uf/SCQxLmlUfsDDGfoFPK/Dp7Xwf2nrMOpFFeDqOvOjqSdnHfzgzcr1XUivsT929FQYvo8ZQEEQAqzD+G9uaXh3WIM4n5ii6Zo8sxM1Zw5cgpvDVtOQ8E4twYRk1k73vCVAP+avqgMtGYsZhYFt4/qFJv94kUTNDuH+vscm/gkr6do8IwA3Rr2YBnLj0cgN8c04Wnv1jIkV2a0zAvetFmVc3w69qyAQ+eF+9ypk52+T2YIDvMXGbUcPa+J0w1ErS5+8//RM+sypqOElUPmoMSuePfm2nrZof5ZqwgaR42o75z29+lRQOa5OeWUxqmL9nI8NEWBdOouZiCyQD+wz74Buy7N6mqt+JY6uZkxc0kG3n5kVFmsXSMf9R2Lj26gCcvPpTBvdsB8MRFh0by0vVb+TSrX4cXLz+Cf15wCB9ce2xKdV6duCStMhlGZTAFk0GCjkX9Z3u6QuPGDuI3rJvDMT3C48oYZWRnCacc2DaifE/t1ZZBvdoA6VcwAMf2aEnjerm0bVy2zqZeLXENM3/NFpt4YERhCiaDhA0S+/Hc00EwUmKyo5zUs3XaZNgTGNTLW7UfG+ws3dTNyWK/Ng351VFJQjbUIM55/CvOfvyr6hbDqEHYIH8GiTKRiR/RMH0DtcFB/ERhGebfNyipt1/Di7FzygFtEvoLSxc//PkUBK+3e80JPSKOR+fde2qoY8zqJlHgNGPvxRRMBnn/+xWR7dkrNgFQlMaR4zqBKcmJjlJbHFpWN5lWLhD92+QHHGbGylIwfAzT7zw5Er4hk/z7i4U888VChhzeMSPH+3lbEbe+9T3zVm9hzsrNnHdYB7btKmHrrmL679OSS/t1yYgcRmqk/V8jItkiMlVE3nP7o0RkmvsUisi0kDp5IvKdiEwXkVkicncgr5mIfCwic913U5feT0RmiMhEEenu0pqIyFip5tFsf0D/rWnLI3HvfcoLx1sZonswaTuMkQFEhILm+fRJ4FrnifHzMyyRxz3v/cCyjdt5+OPMRCSfvnQj781YEfF4/frkpYz5fgXjf1zDXS6MglFzyMRr2bXAbH9HVYeoam9V7Q2MBt4MqbMTGKiqBwO9gVNEpK/LGw6MU9UewDi3D3ADcC5wC3CVS7sduE+rOWxnUL21ahSrYNLZgwkomKSjMEZtYPyfBvCGWyD66hV9o/LSaWqNpbiklOKS0ohn6Uxi4QpqF2k1kYlIB+A04F7g+pg8Ac4HBsbWcwrBj0ub6z7+3TwY6O+2nwfGAzcBRUA9IB8oEpFuQHtVnVBlJ1QFzFy2KWo/lVghu0uw29Y9hRjwRu0h1jFpfhrvI5/Y4GoViRZaVXw+d035hVLgq3lrufDpbxn/x/4UVMDLtlEx0t2DeQS4EQh7vToWWKWqc8MqOtPaNGA18LGq+qEXW6vqCgD37fufvx8YAVwHPIan1G5PJpyIXCEik0Rk0po1VXPjhpHsTS9sZtK0O6omcqS/cO/QTk144bIjq6RNo2ZQJ0bB7NcmvTPcwowAi9dvS+sxw/CtAb/uVxCan6qxYvSUZQBMLFxfFWIZCShXwYjI6SJSYUUkIqcDq1V1coIiFwCvJKqvqiXOjNYBOEJEDkx2PFWdpqp9VXUA0BVY7okho0RkpIjEzcVV1RGq2kdV+7RsGR+vo6pIpmB+WhUfPbFJfngsl4riu6E/fp9WCePDGLWT3JzoYcWSNFuBt+xM3TT1w/LoXvq81ZspXLuVCT+t4Yu54VFGl6zfxpKAwlJVvlmwjh1FJZG1NR/NWsnoKcvo2qI+d55xANeftE9cO5+59n9atZk5KzfF5UfadwaRhz/+ifdmLE/53IyKkYqJbCjwDxEZDTyrqrPLq+DoB5wpIoOAPKCRiIxU1YtFJAc4BzisvEZUdaOIjAdOAWYCq0SkraquEJG2eD2cCM70dhswBK8ncydQAFwD3Jqi7FVKcRIF88mcKPGrNHKk/8zJybaZYnsabRtFBzwrqSpPnAmYsTQ8fEAYgx79nMIHTovsn/jwZ1H5E289Mc4Vz7F/+xQgUu/l7xZz639m0rBuDpt3FvPa747iihe9d1XfC3XY/2rYM9/x9K/68PuXp7CzuDRKjiCNnJ+5FT/v4OqXpzJwv1Z7pfPXdFNuz0RVLwYOAeYDz4rI1860lDQcn6rerKodVLUAT0l94toCOBGYo6pLw+qKSEsRaeK26/nlXfY7wDC3PQx4O6b6MGCMqm7AG48pdZ9qC8lYkcHQv//yoCo7rv+WZlOR9zwa5+fyeMCNTToniwBs3lG2xuX9a47lhcuO2O22Nm7bVW6ZH90ssc2u57R0Q1nvxg+Cd3S38CB4SzdsKzcwW9DLBdgannSRkspW1U2uB1MPb4zjbOBPIvKoqv5zN447lBjzmIi0A55W1UFAW+B5EcnGU4Kvqep7rugDwGsicjmwGDgv0EY+noI52SU9jDdTbReeSa5aCCqY7CxJqnCqMvaK34PJNp9jeyTBcZhvFqznvD4dmbnsZ8b/uJqrB/aocHuvTVpCywZ1GbBfq7i8qYs3RrY7NKtHnZzU7qmwAGkn/b/PaJiXQ6/2jflq/jpuOmW/SF7B8DHs16ZhZBpyGE2dCTlWSfjMXxMfPA7gpW8XUdC8Pv26t2D7ruhYPJt3FNO2cbmnY1SQchWMiJwBXAZ0A14EjlDV1e5hPhsoV8Go6ni82V7+/qUhZZYDg9z2DLxeU1hb64ATEuRtAwYE9j8HepUnX7oJKpTqeNSnI2CYUf0c06MFh3VuyuRFGxg9ZSkPnX8wp//zCwD+p3/3Cv/uN77heWYOMyv5PYJfHNCahnVzyG+e2syrMd+Hj29s3lHMV/PXAfDXD6Pj2oQpl+A72XH7eD2Yri0acPw+LenbtTnvTl/OD27xcljob4Bb/+OFIy984DS2F8UqGOvBpINUBu/PA/6fqh6kqn9X1dUQeZhfllbp9hCKE9jHM7WOwNTLnklebjZPXhw+jJksaNnuUFKqNKtfh39d0gcRISc7i31bx1vJLz26ICrUQFWFeN5ZXKYQ+nb1TGP16mTz/GVHcFX/boy55phI/tbAhIREs8q27SqhQ9N6kcBz/voaVY2EMy8t1cgn2E5JqUYWSFfXeqDaQiomsjuBiI8TNybSWlULVXVc2iTbQ3jqswXsKCr7kwVvRT9scrrw/xRmIdtzCf62wXGK0x79nDeuPJpD7vmYfwztzeDe7RO28cH3K7jqpSmR/benLYsrX1yqcUHsikJenHKzhSKnVDbvKOKm0d9X6HwS4fc+EhF01jFqUlkIg53FpeTFeKMOrufxleFX89fRt2tz9rv9Q8Dr5XS95f1IuT6dm/LGVUcz5F9f8+3C+KnNt5/ek8uPMTc1saTSg3md6HUsJS7NSIFH/lvmQqNpfm7Um1BwOucH1x4b9RZWFfiKLfYPZuw5tGhQNhvrmwVlD775a7aycJ03FvHMl4VJ23j6i4VR+69+Fx9jpqS0NC5AnR/x8/qT9uEXB7Tmm5tPIDc7iyI34WBVml+gYhl1Rd+4cZnYsZZYurqp/KWlyqbAQH+sZ4RJi7yp0mHKBTyXOUY8qSiYHFWNTPtw27aoohy27Czmh+WbouzgRSUaZUsOmg/2b9uIA9pV7Sjj1l1etz8Tq7yNmsfMZd7U4kRmoqKSUqYt2cjkRdExXOaujh4Dmb5kI29NXU7skE4393Bu36Qe/7qkD20a55GTncWuklJUlXGzo6fgp5sjuzaPCz2xvaiEsbNW8u8YJeojIrRqWJfRU5ayevPOSPoNr02PK/vmlNBJrxH+79N5rAm0sbssWreV1Zt2UFxSypgZK/hq/lpW/pxZZV1VpGIiWyMiZ6rqOwAiMhgIXy1lRLjihUl8NX8dDQMxWWIXq6Xbf9Q29/ZW3+b37xVs2xV9f93x9iwg8VjfvWNm89xXhXHpa7dETyMe/H9fArA85iF38gGteXPqsihvFHXcmqvNO4u5/4PowXsvP6tS40MtGiR/t523ekvU/oZtu/jdi4nWent0apbPpEUbePCjHyNp70yPn5xwfYjSCfL3sT/y97E/Jlx7kyrH/3084IVoeHRcmaOTyrZbHaTSg7kSuEVEFovIEjy/X79Lr1i1n+9cVzrZ8F9VDYAmwn/g5Ne1HszeQKLV9okUzPSlG0PTgxE0k70EnXJgW2b/+RR6titTMP4MrrXuTf66E3twx+k9AejbtRk//PkXCdsraF62VG3irSdGtoM98CcSTGrwiTWJxSpLnwuP7MTce08F4IXLjyAvNyvlXsKD5x0cmRyQmy1MvPVEnq/EuqBEzA3x8lHbSGWh5XxV7Qv0BHqq6tGqOi/9otVu/DHHZC42dloPxqhC/vbhj6Hpc1ZuZvaKTSxat5WC4WM44I4P2VVcSqLJT9uLSiJuW5Zt2J70mLHOWn0F47vO79ayQWQgvUPT/KRxdTo0LVMwwZX+wRAXsT7YYomdojzsme9Cy7VsUDdSNr9ODvu0Tr72JkiPVtHn1LJhXbqkOG27Inwwc2XUfs87PuSqkZNT9rdWE0jJx5iInAb8D/AHEblDRO5Ir1h7B+nuwTxwzkGcuH/rjIf6NTLLsBRCKv/1wzmM+GwBAFt3lfDBzBWR6bhhjPxmEQAL14YvWkxErgsR8dlPnvPYgzs04ZjuLTiqa3MuPboAgL+cdSDXn7QPVx7fLUqRJJrteFX/bpHt8twe3Txov6T5PoN7t4va98NiA5x2UNuovD6dm7JP6zJv5N1aNaBzs3xOOaANjw71luu1a5IHQJtGeSkdf3fYtquED2aujFvDU5NJZaHlk3huVgYATwO/BMJfC4wIXpCx5G8a6VYwPds14ulhfdJ6DKP6Oe2gdjz/9aKkZTbvKI70aH2Srd/YWewN1G+q4ALEOgEFcMug/ejkzF6vBOLXXNy3TCEe2bUZv352YtI2T+7ZhsfHz2fRum0JF1H6HNsjNae1XVtGh6+48vhuPPvlQlZt2skVx3ZlzAxvZcbC+wdFTYEO8uQlZea6nOwsDurQmCb5dSgt1bQubt68o7hK/aapasJzrCyp9GCOVtVfARtU9W7gKCAz8VH3cGJXMBvG7pDKQr9sEf4zdVlk/9pXp0VWvgfp7BTCc18V0uXm97n21WkVkiU4Jb5B3fJDOAcnwcTGl+nhYhg1yMuJuDuKXYsThl+kaX7FQkj7szgb18sl1ynKijx483Kz+eynNXS95X1GfFZ+hNExM1ZQMHxMSr7ZgiQy+6XKnJWbKBg+hlnLf2b91l30uusjRk1cXKk2E5GKGvRHvrY5f2HrAFtRVB4x9+WAfVvy6Y/pizlj7L0EbfL7tm7IjyGDw6k6PH3u10cw4MHxcenn9+nAeX3Kf68M+jEbGOLTLJY+Bc14+PyDWbdlF2cf2p7TD2pH+yaep+iXfnskM5f9TG52VkRplKYw/vDZnwawZMM26uZkc+4TXwHw4XXHsml7MXVzsqIivQa556wDOemnNXRuns8nN/SvcLyb4afuxzmPe8e77/05XHFct6TlR3zumSwXrN3KoZ282XFBjwXgBQqMnRmX6lhRIj50YzsfzlzJST1bs2VnMf/6bAFDDu9UqXbDSEXBvOs8G/8dmIJn93mqyiXZw4g1f9XNsZlcRnoIxoI5s3c7npwwPy608NcL1iWsf3S35hG/YK1jQnr7/PXcg1J6m/fd4EP04HwyzgmEqAguHG3VMI+B+3ljGr6CTKW31rFZPh1jekOpBGRr36QeFxzRKWEb5XFop6ZR+5MXrUdE4tJ3FJXwzrTlTF+yESg7px+Wb4rr+fz3+uPjIolWlrmrPIVVuG5bRImma95AUhOZCzQ2TlU3qupooDOwn6raIH8FSeSPzDAqS0FgBlPdnCwuPKJib6In7t+aY3u0ACAnK/yRsDs2+qoch/Bd18TGkSmPernZDNg3fcEEk3HuE19zzuNf8dGs6Nlgr09eyo2jZ0T2b3NucAY9+jlvTYtffzPE9Rwreu6JGPO9N7707vTlXP3y1CppMxFJezCqWioiD+GNu6CqO4HKL1XdC0kWdMwwKkPHZvn0at+Y75f9TKO8XC49uoB/uRljdXKykk4mGX3V0RzaqQm/OqozRSUaGXuoafxP/278ul9BhQe3v7n5BPLqpDsyfBlz7jkl4s/MZ/nG6KneG7ZGj7mEmTSfuOhQ+u/rmRjvO6cX15zYg2b5dbjixUksWlf1oaofPK/q4lAFSeXKfyQi50q6phnsgYTNU093QChj76auG1domJeTdK1JLJ2b50e8I9erk5222USVRUR2a+ZU4/zcjJqn83Kzo6Y0g7cm6ISHxkfGPsJ46KPoNUxN8utE1hhlZwntm9SjXp1sdhaVsnj9Nm56YwY3vjGd0x79vELrYhKV7ZyGdTyQ2hjM9UB9oFhEduANX6uq2uKKBFTUfffVA7qnSRJjb2Grm4LcwDl7HNy7Hacf1I76dbK58OlvI+Ua5eWwyY3P9OnclGb58a5XBvdux9sBU42/Et9IjXohinD+mq2M/GYRpxzYJnS845+fRK9d37dNeMDg7wo9DyFBj9Hrt+6ieYPUzGexU9V9wu6DqiCVlfwNVTVLVeuoaiO3b8olCWHmsLzcxJf6j7/YN53iGHsBfgwU38PxP4Yewkk9W3N09xZR5WbcVeaqZeRvjgwdJ/nH0EOi/F5dUMExnb2duglmqfnhn3eVJF4o+fxlR1D4wGk0q5/6A78iCy9jJ3+AtwYoXet2UlloeVxYuqp+lsoBXNjjScAyVT1dREYB/hO1CbBRVXvH1OkIvAC0wQsVMEJV/+HymgGjgAKgEDhfVTeISD/gCbwxogtUdZ6b/TYKOEUz6F8hTMHYLDIjnfh+5xKFEQ7SokEd1m7ZVW4Yh64t6rNg7daE03qNcLq1rB/xRRhk+pKN5c4IK+/3a5Kfy8Zt0Ytft+ws5obXpjN6ylJ6tGpAbnYW7197LGNnreR/X5nKs5cezsvfLmbGso10bhZvCivPgWhlSMVE9qfAdh5wBDAZGJjiMa7FC63cCEBVh/gZbgLBzyF1ioEbVHWKiDQEJovIx6r6AzAcb2bbAyIy3O3fBNwAnIuneK5y+7cD92VSuQCRaHdBgm8I7159DGc89kUmRTL2cHzTR7IH1PvXHAvA21cfw08pOFJ89Yq+TF/6c8praHze+99jUlqvsqdy/Un7sm/rhnRt2YD3v1/Bip930KZRXpRZC+BflxwW5+n5wHJCdrx79TFc/crUyBRn8Bx8jnahBOYG1sx8NW8tu4pL+WnV5sjMsaJi73d5/KJD2bqzmHlrtkRc+KSDchWMqp4R3He9i7+l0riIdABOA+7FG8sJ5glwPiGKSlVX4KJoqupmEZkNtAd+AAYD/V3R54HxeAqmCKiH59amSES6Ae1VdUIqslYlYbN2gn/RTs0rNr/eMMrDVzDBdSix+F6P2zepF1nMmIxWjfI4qWfFfWsd2L5q4xrVNlo2rMul/by16Mft402Rnli4Pk7B/OKANtxz1oHc/lZZtM7yeosdm+Uz4pLDOPK+smDCiUxkS5yj0pe+LVulv3LTDob06Rjley2d7E7fdylwYIplHwFuJDoips+xwCpVnRuSF0FECoBDAH+ksrVTQL4i8pcL3w+MAK4DHsNTareX0/YVIjJJRCatWVN1q+x/WrUlLi34EpjIRmsYu8u5brFi2L3VsmHduGBhRmZp2zhaUfdySrhuBWb8+cSaNnckUDB+CO1YTwDlOQytSlIZg/knZV4bs4DeQPLIO16904HVqjpZRPqHFLkAeKWcNhoAo4HrVDXecVIAVZ0G9HX1jgOWe5syCq93c4OqroqpMwJPKdGnT58q69NriJPL4PTP8lyOG0ZF+dsvD+LuwQeETjP+/MYB1SCREaRD03y+Gj6QnCyhYV5uxOy4O+Nb9WIUzPZd8e/vxSWlUX7brjy+G09O8LwEZHLSRipjMJMC28XAK6r6ZQr1+gFnisggvLGbRiIyUlUvFpEc4BwgYfQgEcnFUy4vqeqbgaxVItJWVVeISFtgdUw9AW4DhuD1ZO7EG5e5Brg1BbkrzeuT4kOrBv/26fS0auydZGcJDeqG/53LG8w3MkO7ELPk7lgzYhfD/v7lKXFlTnx4AoWBBZndW5WtzWlcr2JOQCtDKmf3BjBSVZ9X1ZeAb0Sk3EEEVb1ZVTuoagEwFPhEVS922ScCc1Q1NMi1UxL/Bmar6sMx2e8Aw9z2MODtmPxhwBhV3YA3HlPqPhkb+AgLt3rNCT0A6Ne9OYCZLAzDoGe7RhzQrhFtGuVxw0n7pFQnlcWwQeXSvkk9TurZGoB2jfNoncaYNbGk0oMZh6cQfENePeAj4OhKHHcoMeYx56n5aVUdhNf7uQT4XkSmuSK3qOr7wAPAayJyObAYOC/QRj6egjnZJT2M1wvahWeSqzYKWtSPWlsgIunzMGcYRq2gc/P6jHGz+ypDm0Z5jP3DcRx890dR6Vf178ZNp3hB2ILPn0yRioLJU9XIKJGqbkmlBxNEVcfjzfby9y8NKbMcGOS2vyDO4X2k3DrghAR52/ACo/n7nwO9KiJrprAOjGEYVUW3VvWjYuv4JJtVmAlSUTBbReRQVZ0CICKHAckDdRvlsjevEzAMo3I8e+nh1MnJ4tuF63n2i4X8efCBZGUJjwzpzdTFG9hRVEqpKucc2r5a5UxFwVwHvC4i/sBCW7wBdCOEZHHOo8qZfjEMYzfxA7v1696C6wNjN2cd0p6zDqlepRIklYWWE0VkPzz3LoI3OF+xQN17EY99Oq/8QoZhGHsB5c4iE5HfA/VVdaaqfg80EJH/Sb9otZOJhfE+iMKoqXE3DMMwqopUpin/VlU3+jtu+u9v0yZRLSeRO+xYbG2CYRh7OqkomKxgsDHnHTl97jdrOb7b9PJ44qKEa0wNwzD2CFJRMGPx1p2cICID8davfJBesWovQb9AH16XeH77MT1aJMwzDMPYE0hlFtlNwBV4LvAFmIo3k8wI8MwXC/n72B9p37TMHURWCitum1cgsJBhGEZtIpVZZKUi8g3QFW96cjO81fFGgD+/9wMAA/drFfFeWp56efLiQzmoQ5P0CmYYhlFNJFQwIrIPnkuXC4B1eJEhUVVzzZqE4pKyBS7ldWBOOdA6goZh7Lkk68HMAT4HzlDVeQAi8oeMSFWLKS4Nus72NIwfdMgwDGNvIpmCORevB/OpiHwIvIq50CqXokC45CyB6XeeHBe/wTAMY28g4SwyVf2Pqg4B9sNzVPkHoLWIPCEiJyeqt7dTFGUiExrXy92toEKGYRi1nXKffKq6VVVfUtXTgQ7ANGB4ugWrrQR7MNbdMwxjb6ZCr9aqul5V/6WqA9MlUG0nOMifyjRlwzCMPRWz3VQxu4I9GNMvhmHsxZiCqWKKS0rLL2QYhrEXYAqmipm7OhL8k6ws68IYhrH3knYFIyLZIjJVRN5z+6NEZJr7FIrItAT1nhGR1SIyMya9mYh8LCJz3XdTl95PRGaIyEQR6e7SmojI2KCzznSzdENZsE9TL4Zh7M1kogdzLTDb31HVIaraW1V747mceTNBveeAU0LShwPjVLUHMI6yGW034K3duQXPbxrA7cB9qhaf2DAMI9OkVcGISAfgNODpkDwBzsfzzhyHqn4GhEXvGgw877afB85y20VAPSAfKBKRbkB7VZ1QiVOoFCUWF9kwjL2YVLwpV4ZHgBuBhiF5xwKrVHVuBdtsraorAFR1hYi0cun3AyOA7cAlwIN4PZiEiMgVeJ6i6dSpUwXFKJ9S6zgZhrEXk7YejIicDqxW1ckJilxAgt7L7qCq01S1r3PG2RVY7okho0RkpIi0DqkzQlX7qGqfli2r3l9YsfVgDMPYi0mniawfcKaIFOL5MRsoIiMBRCQHOAfnobmCrBKRtq6dtsDqYKYzvd0G3APc6T4jgWt27zR2n2xbCGMYxl5M2hSMqt6sqh1UtQDPaeYnqnqxyz4RmKOqS3ej6XeAYW57GPB2TP4wYIyqbsAbjyl1n/zdOFbK5ASmJDfMy+Gv5/aioEX9dB7SMAyjRlNd62CGEmMeE5F2IvJ+YP8V4GtgXxFZKiKXu6wHgJNEZC5wktv36+TjKZjHXdLDeDPV7geeSNO5ANAgr2w4q0l+LkMOr/oxHcMwjNpEugf5AVDV8Xgemf39S0PKLAcGBfYvSNDWOuCEBHnbgAGB/c+BXrsndcWoXyeHjduKACi1xfyGYRi2kr+qyApcSZs9ZhiGYQqmypDAun1TMIZhGKZgqozghDGbnWwYhmEKpsoITkguNQ1jGIZhCqaqCPrTNBOZYRiGKZgqI6oHY/rFMAzDFExVETUGYxrGMAzDFExVETSRlZiJzDAMwxRMVRFtIjMFYxiGYQqmiog2kVWfHIZhGDUFUzBVhC20NAzDiMYUTBWhlCkVUzCGYRimYKqE85/8mp9WbYns2yQywzAMUzBVwneF66tbBMMwjBqHKRjDMAwjLZiCMQzDMNKCKZgqpG3jvOoWwTAMo8ZgCqYK6dgsv7pFMAzDqDGkXcGISLaITBWR99z+KBGZ5j6FIjItQb1TRORHEZknIsMD6c1E5GMRmeu+m7r0fiIyQ0Qmikh3l9ZERMZK0I9LGsnNzshhDMMwagWZ6MFcC8z2d1R1iKr2VtXewGjgzdgKIpIN/B9wKtATuEBEerrs4cA4Ve0BjHP7ADcA5wK3AFe5tNuB+1QzszBFMAVjGIbhk1YFIyIdgNOAp0PyBDgfeCWk6hHAPFVdoKq7gFeBwS5vMPC8234eOMttFwH1gHygSES6Ae1VdULVnE35+P2knCxTNIZhGDlpbv8R4EagYUjescAqVZ0bktceWBLYXwoc6bZbq+oKAFVdISKtXPr9wAhgO3AJ8CBeDyYhInIFcAVAp06dUjid5IgID513ML07Nal0W4ZhGLWdtPVgROR0YLWqTk5Q5ALCey9AqK0pqZlLVaepal9VHQB0BZZ7YsgoERkpIq1D6oxQ1T6q2qdly5bJmk8JAc49rAPdWjaodFuGYRi1nXSayPoBZ4pIIZ6Ja6CIjAQQkRzgHGBUgrpLgY6B/Q54CgNglYi0de20BVYHKzrT223APcCd7jMSuKbyp5SczEwlMAzDqB2kTcGo6s2q2kFVC4ChwCeqerHLPhGYo6pLE1SfCPQQkS4iUsfVf8flvQMMc9vDgLdj6g4DxqjqBrzxmFL3SfscYtMvhmEYZaR7DCYRQ4kxj4lIO+BpVR2kqsUicjUwFsgGnlHVWa7oA8BrInI5sBg4L9BGPp6COdklPYw3U20XnkkurWzeUZzuQxiGYdQaMqJgVHU8MD6wf2lImeXAoMD++8D7IeXWASckOM42YEBg/3Og124LngLBGdCTFm1I56EMwzBqFbaSv5KYa37DMIxwTMFUEgsuZhiGEY4pmEpiCsYwDCMcUzCVxPSLYRhGOKZgKon1YAzDMMIxBVNJbJDfMAwjHFMwlcR6MIZhGOGYgqkkWlrdEhiGYdRMTMFUEuvBGIZhhGMKppKYgjEMwwjHFEwlsUF+wzCMcEzBVJIMRWM2DMOodZiCqSTWgzEMwwjHFEwlsTEYwzCMcEzBVBJTMIZhGOGYgqkkpl8MwzDCMQVTSawHYxiGEU7aFYyIZIvIVBF5L5D2vyLyo4jMEpG/Jah3rYjMdGWuC6Q3E5GPRWSu+27q0vuJyAwRmSgi3V1aExEZKyKSrvOzQX7DMIxwMtGDuRaY7e+IyABgMHCQqh4APBhbQUQOBH4LHAEcDJwuIj1c9nBgnKr2AMa5fYAbgHOBW4CrXNrtwH2axrnE1oMxDMMIJ60KRkQ6AKcBTweSrwIeUNWdAKq6OqTq/sA3qrpNVYuBCcDZLm8w8Lzbfh44y20XAfWAfKBIRLoB7VV1QtWdUTy2DsYwDCOcdPdgHgFuBIIuIfcBjhWRb0VkgogcHlJvJnCciDQXkXxgENDR5bVW1RUA7ruVS78fGAFcBzwG3IvXg0krpl8MwzDCyUlXwyJyOrBaVSeLSP+YYzYF+gKHA6+JSNegGUtVZ4vIX4GPgS3AdKA42fFUdZprExE5DljubcoovN7NDaq6KkbGK4ArADp16rRb52ljMIZhGOGkswfTDzhTRAqBV4GBIjISWAq8qR7f4fVuWsRWVtV/q+qhqnocsB6Y67JWiUhbAPcdZWJzA/q3AfcAd7rPSOCakGOMUNU+qtqnZcuWu3WSNgZjGIYRTtoUjKrerKodVLUAGAp8oqoXA28BAwFEZB+gDrA2tr6ItHLfnYBzgFdc1jvAMLc9DHg7puowYIyqbsAbjyl1n/yqOrcgpmAMwzDCSZuJLAnPAM+IyExgFzBMVVVE2gFPq+ogV260iDTHM2/93ikMgAfwzGqXA4uB8/yG3XjNMOBkl/QwMNod54J0nIzpF8MwjHAyomBUdTww3m3vAi4OKbMcbzDf3z82QVvrgBMS5G0DBgT2Pwd67b7k5WM9GMMwjHBsJX8laVwvl9MPalvdYhiGYdQ4TMFUks7N6/PYhYdWtxiGYRg1DlMwhmEYRlowBWMYhmGkBVMwhmEYRlowBWMYhmGkBVMwhmEYRlowBWMYhmGkhepYyb9HcucZPTmyS/PqFsMwDKPGYAqmivh1vy7VLYJhGEaNwkxkhmEYRlowBWMYhmGkBVMwhmEYRlowBWMYhmGkBVMwhmEYRlowBWMYhmGkBVMwhmEYRlowBWMYhmGkBVEL+QuAiKwBFqVYvAWwNo3iVIaaKltNlQtqrmw1VS4w2XaHmioXVE62zqraMizDFMxuICKTVLVPdcsRRk2VrabKBTVXtpoqF5hsu0NNlQvSJ5uZyAzDMIy0YArGMAzDSAumYHaPEdUtQBJqqmw1VS6oubLVVLnAZNsdaqpckCbZbAzGMAzDSAvWgzEMwzDSgikYwzAMIy2YgqkAInKKiPwoIvNEZHg1HL+jiHwqIrNFZJaIXOvS7xKRZSIyzX0GBerc7OT9UUR+kUbZCkXke3f8SS6tmYh8LCJz3XfTapBr38B1mSYim0Tkuuq6ZiLyjIisFpGZgbQKXycROcxd73ki8qiISJpk+7uIzBGRGSLyHxFp4tILRGR74Po9mS7ZEshV4d8vg9dsVECuQhGZ5tIzec0SPSsye6+pqn1S+ADZwHygK1AHmA70zLAMbYFD3XZD4CegJ3AX8MeQ8j2dnHWBLk7+7DTJVgi0iEn7GzDcbQ8H/pppuUJ+w5VA5+q6ZsBxwKHAzMpcJ+A74ChAgA+AU9Mk28lAjtv+a0C2gmC5mHaqVLYEclX498vUNYvJfwi4oxquWaJnRUbvNevBpM4RwDxVXaCqu4BXgcGZFEBVV6jqFLe9GZgNtE9SZTDwqqruVNWFwDy888gUg4Hn3fbzwFnVLNcJwHxVTeaxIa2yqepnwPqQY6Z8nUSkLdBIVb9W7wnwQqBOlcqmqh+parHb/QbokKyNdMiW4JolotqvmY970z8feCVZG2m6ZomeFRm910zBpE57YElgfynJH+5pRUQKgEOAb13S1c6M8Uyg25tJmRX4SEQmi8gVLq21qq4A74YHWlWDXEGGEv1nr+5r5lPR69TebWdSRoDL8N5gfbqIyFQRmSAix7q0TMpWkd+vOq7ZscAqVZ0bSMv4NYt5VmT0XjMFkzphdsdqmeMtIg2A0cB1qroJeALoBvQGVuB1yyGzMvdT1UOBU4Hfi8hxScpm/FqKSB3gTOB1l1QTrll5JJKlOq7frUAx8JJLWgF0UtVDgOuBl0WkUQZlq+jvVx2/6wVEv9Bk/JqFPCsSFk0gQ6VkMwWTOkuBjoH9DsDyTAshIrl4N8xLqvomgKquUtUSVS0FnqLMpJMxmVV1ufteDfzHybDKdbF9M8DqTMsV4FRgiqqucnJW+zULUNHrtJRoU1VaZRSRYcDpwEXOTIIzpaxz25PxbPb7ZEq23fj9Mn3NcoBzgFEBmTN6zcKeFWT4XjMFkzoTgR4i0sW9DQ8F3smkAM6m+29gtqo+HEhvGyh2NuDPaHkHGCoidUWkC9ADb8CuquWqLyIN/W28geGZ7vjDXLFhwNuZlCuGqLfJ6r5mMVToOjnTxmYR6evuiV8F6lQpInIKcBNwpqpuC6S3FJFst93VybYgU7JV9PfL5DVznAjMUdWIeSmT1yzRs4JM32uVmamwt32AQXizMeYDt1bD8Y/B657OAKa5zyDgReB7l/4O0DZQ51Yn749UwayZBHJ1xZuBMh2Y5V8boDkwDpjrvptlUq7AsfKBdUDjQFq1XDM8JbcCKMJ7O7x8d64T0AfvoTofeAznlSMNss3Ds83799uTruy57reeDkwBzkiXbAnkqvDvl6lr5tKfA66MKZvJa5boWZHRe81cxRiGYRhpwUxkhmEYRlowBWMYhmGkBVMwhmEYRlowBWMYhmGkBVMwhmEYRlowBWPUaEREReShwP4fReSuKmi3roj813m1HVLZ9lI85qUi0i4Txwocs7+IvJfJY1YEEdlS3TIY6cMUjFHT2QmcIyItqrjdQ4BcVe2tqqPKLV01XApkTMG41eSGUW2YgjFqOsV48cL/EJshIp1FZJxzeDhORDqFlGkmIm+5Mt+IyEEi0goYCfR2PZhuMXXGi8j/E5HPxIuncbiIvCleDI2/BMpdLyIz3ec6l1bg6jwlXhyOj0Sknoj8Em/B2kvumPXEi7MxQTwHoWNjVqcH5fmVk3+6iLzo0s4QkW/Fc5z4XxFp7dLvEpERIvIRnufbpNci5FgHiMh3TsYZItLDpb/l5JwlZc5MEZEtIvJXl/dfETnCXb8FInKmK3OpiLwtIh+KF2vkzgTn+ScRmeiOe7dLqy8iY9y5z8xUb9OoIqpylbJ97FPVH2AL0Agv3kxj4I/AXS7vXWCY274MeCuk/j+BO932QGCa2+4PvJfgmOMpi5NxLZ7vpbZ4sTKW4q2GPgxvJXl9oAHeCu1D8GJ+FAO9Xf3XgIsD7fZx27nAV0BLtz8EeCZElgPwVla3cPvN3HdTiCyU/g3wkNu+C5gM1Is9z0TXIuR6XeS26wTa8Y9bD29Vd3O3r7hV33g+6D5y53Zw4FpfirfavXmgvn8dtrjvk/FeJATvxfc9vFgr5wJPBeRrXN33pH1S/1gX2qjxqOomEXkBuAbYHsg6Cs+hIHiuQ/4WUv0YvIcUqvqJiDQXkcYpHNb3M/c9MEudi3MRWYDnFPAY4D+qutWlv4nnnv0dYKGqTnP1J+MpnVj2BQ4EPvZcPJGN9xCOZSDwhqqudefgxx7pAIxyvZ46wMKg7Kq6nXhCr4Wq/hwo8zVwq4h0AN7UMlfz14jI2W67I56vqnXALuDDwLXaqapFIvJ9zHl/rM7Ro7tWxwCTAvknu89Ut9/AHeNz4EER+Sueovw85LyMGoopGKO28Aie/6Znk5QJ83u0u+7Gd7rv0sC2v5+ToN3YugAleG/tYXLNUtWjohJFOuL1zACedOXC5P0n8LCqviMi/fF6Lj5bE8hV7rVQ1ZdF5FvgNGCsiPwG75xPBI5S1W0iMh7Ic1WKVNVvI3KtVLU0Zgwo9hxi9wW4X1X/FSe0yGF4frTuF5GPVPXPCc7PqGHYGIxRK3Bv7q/hOTr0+QrPqzXARcAXIVU/c3m4B/FaTR4XI1U+A84SkXzxPEifjfe2nYzNeOFrwTN7tRSRo5xsuSJygKouUW/iQW9VfRLPIeH5ItLclWvm6jcGlrntYaRGuddCPC+/C1T1Ubze2EHuWBucctkP6Jvi8YKc5MaA6uFFRPwyJn8scJl48UsQkfYi0kq8WXfbVHUk8CBeeGKjlmA9GKM28RBwdWD/GuAZEfkTsAb4dUidu4BnRWQGsI3UH8ZJUdUpIvIcZa78n1bVqeJFD0zEc8CTIrIdz7z3S+BRZ7LLweulzYo5ziwRuReYICIleCakS915vS4iy/BCGXdJQey7KP9aDAEuFpEiYCXwZ7we0ZWu3o/ueBXlCzwzZnfgZVUNmsdQ1Y9EZH/ga2cy3AJc7Mr/XURK8TwWX7UbxzaqCfOmbBhGWhGRS/EG9a8ur6yxZ2EmMsMwDCMtWA/GMAzDSAvWgzEMwzDSgikYwzAMIy2YgjEMwzDSgikYwzAMIy2YgjEMwzDSwv8HyI6DO7oEMagAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "\n",
    "plt.plot(range(30,2000),acc_sub[28:])\n",
    "plt.title(\"Monte-carlo dropout accuracy vs full-network (sub opt)\")\n",
    "plt.xlabel(\"No of monte-carlo samples\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1.0))\n",
    "plt.axhline(y=0.4737)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Above graph shows full network performance as horizontal line\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alpha dropout\n",
    "\n",
    "* Alpha dropout is used with self-regularizing networks (dense and using SELU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting lr = 0.01 Logged in: logs/fit/selu_alpha_0.01_20220704-162735 saved in models/selu_alpha_0.01_model.h5\n",
      "Epoch 1/100\n",
      "   2/1172 [..............................] - ETA: 43:49 - loss: 3.1368 - accuracy: 0.0469WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0093s vs `on_train_batch_end` time: 4.4851s). Check your callbacks.\n",
      "1172/1172 [==============================] - 17s 15ms/step - loss: 3.0762 - accuracy: 0.1007 - val_loss: 2.4185 - val_accuracy: 0.0970\n",
      "Epoch 2/100\n",
      "1172/1172 [==============================] - 15s 13ms/step - loss: 1008.3713 - accuracy: 0.1015 - val_loss: 2.3687 - val_accuracy: 0.1010\n",
      "Epoch 3/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 689.1571 - accuracy: 0.0980 - val_loss: 2.3882 - val_accuracy: 0.0988\n",
      "Epoch 4/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 64.6981 - accuracy: 0.0982 - val_loss: 2.5004 - val_accuracy: 0.1031\n",
      "Epoch 5/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.3926 - accuracy: 0.1011 - val_loss: 2.4071 - val_accuracy: 0.0970\n",
      "Epoch 6/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.3882 - accuracy: 0.1006 - val_loss: 2.3693 - val_accuracy: 0.0988\n",
      "Epoch 7/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.3855 - accuracy: 0.0975 - val_loss: 2.4164 - val_accuracy: 0.1024\n",
      "Epoch 8/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.3847 - accuracy: 0.0987 - val_loss: 2.3941 - val_accuracy: 0.0970\n",
      "Epoch 9/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.3835 - accuracy: 0.0998 - val_loss: 2.4590 - val_accuracy: 0.1016\n",
      "Epoch 10/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.3818 - accuracy: 0.1016 - val_loss: 2.4453 - val_accuracy: 0.0946\n",
      "Epoch 11/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 14572.9365 - accuracy: 0.1000 - val_loss: 2.5064 - val_accuracy: 0.1010\n",
      "Epoch 12/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 299.4803 - accuracy: 0.1002 - val_loss: 2.3412 - val_accuracy: 0.1007\n",
      "Epoch 13/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 5360700.5000 - accuracy: 0.0998 - val_loss: 2.4168 - val_accuracy: 0.0988\n",
      "Epoch 14/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 358059.0000 - accuracy: 0.1002 - val_loss: 2.4223 - val_accuracy: 0.1024\n",
      "Epoch 15/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.3795 - accuracy: 0.1002 - val_loss: 2.4736 - val_accuracy: 0.1007\n",
      "Epoch 16/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.3776 - accuracy: 0.1004 - val_loss: 2.3514 - val_accuracy: 0.1024\n",
      "Epoch 17/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.3760 - accuracy: 0.1001 - val_loss: 2.3716 - val_accuracy: 0.0988\n",
      "Epoch 18/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.3814 - accuracy: 0.0980 - val_loss: 2.5696 - val_accuracy: 0.1010\n",
      "Epoch 19/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 38653.8242 - accuracy: 0.0996 - val_loss: 2.3729 - val_accuracy: 0.0988\n",
      "Epoch 20/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.3812 - accuracy: 0.0993 - val_loss: 2.3666 - val_accuracy: 0.0988\n",
      "Epoch 21/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.3800 - accuracy: 0.1019 - val_loss: 2.4121 - val_accuracy: 0.1007\n",
      "Epoch 22/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.3853 - accuracy: 0.0990 - val_loss: 2.3879 - val_accuracy: 0.1024\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 2.3879 - accuracy: 0.1024\n",
      "313/313 [==============================] - 0s 2ms/step - loss: 2.3855 - accuracy: 0.1000\n",
      "fitting lr = 0.005 Logged in: logs/fit/selu_alpha_0.005_20220704-163203 saved in models/selu_alpha_0.005_model.h5\n",
      "Epoch 1/100\n",
      "   2/1172 [..............................] - ETA: 23:19 - loss: 2.9022 - accuracy: 0.0938WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0090s vs `on_train_batch_end` time: 2.3829s). Check your callbacks.\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.4239 - accuracy: 0.0967 - val_loss: 2.3733 - val_accuracy: 0.1033\n",
      "Epoch 2/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.3659 - accuracy: 0.1032 - val_loss: 2.5708 - val_accuracy: 0.1024\n",
      "Epoch 3/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.4371 - accuracy: 0.0971 - val_loss: 2.3211 - val_accuracy: 0.1031\n",
      "Epoch 4/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.3567 - accuracy: 0.0996 - val_loss: 2.4046 - val_accuracy: 0.0970\n",
      "Epoch 5/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.3535 - accuracy: 0.1001 - val_loss: 2.4279 - val_accuracy: 0.1024\n",
      "Epoch 6/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 3.0478 - accuracy: 0.1023 - val_loss: 2.3558 - val_accuracy: 0.0975\n",
      "Epoch 7/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.3677 - accuracy: 0.1010 - val_loss: 2.4214 - val_accuracy: 0.1024\n",
      "Epoch 8/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.5030 - accuracy: 0.1024 - val_loss: 2.3354 - val_accuracy: 0.0970\n",
      "Epoch 9/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.3560 - accuracy: 0.1003 - val_loss: 2.4009 - val_accuracy: 0.0946\n",
      "Epoch 10/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 3.0400 - accuracy: 0.1008 - val_loss: 2.3717 - val_accuracy: 0.1024\n",
      "Epoch 11/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.3493 - accuracy: 0.1014 - val_loss: 2.3599 - val_accuracy: 0.1031\n",
      "Epoch 12/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.3503 - accuracy: 0.1007 - val_loss: 2.3491 - val_accuracy: 0.0988\n",
      "Epoch 13/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3489 - accuracy: 0.1006 - val_loss: 2.4050 - val_accuracy: 0.1016\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 2.4050 - accuracy: 0.1016\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.4084 - accuracy: 0.1000\n",
      "fitting lr = 0.0025 Logged in: logs/fit/selu_alpha_0.0025_20220704-163443 saved in models/selu_alpha_0.0025_model.h5\n",
      "Epoch 1/100\n",
      "   2/1172 [..............................] - ETA: 28:48 - loss: 2.9843 - accuracy: 0.0781WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0080s vs `on_train_batch_end` time: 2.9466s). Check your callbacks.\n",
      "1172/1172 [==============================] - 15s 13ms/step - loss: 2.3966 - accuracy: 0.1006 - val_loss: 2.3383 - val_accuracy: 0.0946\n",
      "Epoch 2/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.3368 - accuracy: 0.0992 - val_loss: 2.3418 - val_accuracy: 0.0946\n",
      "Epoch 3/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.3664 - accuracy: 0.0969 - val_loss: 2.3488 - val_accuracy: 0.0970\n",
      "Epoch 4/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.3364 - accuracy: 0.1006 - val_loss: 2.3494 - val_accuracy: 0.0988\n",
      "Epoch 5/100\n",
      "1172/1172 [==============================] - 12s 11ms/step - loss: 2.3327 - accuracy: 0.1008 - val_loss: 2.3538 - val_accuracy: 0.1033\n",
      "Epoch 6/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3324 - accuracy: 0.1009 - val_loss: 2.3352 - val_accuracy: 0.1031\n",
      "Epoch 7/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.3542 - accuracy: 0.0997 - val_loss: 2.3486 - val_accuracy: 0.1007\n",
      "Epoch 8/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.3317 - accuracy: 0.0991 - val_loss: 2.3423 - val_accuracy: 0.0975\n",
      "Epoch 9/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.3369 - accuracy: 0.1003 - val_loss: 2.3257 - val_accuracy: 0.1007\n",
      "Epoch 10/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.5003 - accuracy: 0.1018 - val_loss: 2.3532 - val_accuracy: 0.1010\n",
      "Epoch 11/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.3433 - accuracy: 0.0982 - val_loss: 2.3551 - val_accuracy: 0.1024\n",
      "Epoch 12/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.3316 - accuracy: 0.0972 - val_loss: 2.3481 - val_accuracy: 0.0988\n",
      "Epoch 13/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.3323 - accuracy: 0.1012 - val_loss: 2.3294 - val_accuracy: 0.1024\n",
      "Epoch 14/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.3335 - accuracy: 0.0990 - val_loss: 2.3449 - val_accuracy: 0.0946\n",
      "Epoch 15/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.3328 - accuracy: 0.0996 - val_loss: 2.3696 - val_accuracy: 0.1031\n",
      "Epoch 16/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.3334 - accuracy: 0.0999 - val_loss: 2.3353 - val_accuracy: 0.0970\n",
      "Epoch 17/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.3343 - accuracy: 0.1004 - val_loss: 2.3508 - val_accuracy: 0.1007\n",
      "Epoch 18/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.3354 - accuracy: 0.1007 - val_loss: 2.3200 - val_accuracy: 0.1031\n",
      "Epoch 19/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.3349 - accuracy: 0.0989 - val_loss: 2.3551 - val_accuracy: 0.0988\n",
      "Epoch 20/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.3333 - accuracy: 0.0975 - val_loss: 2.3547 - val_accuracy: 0.0946\n",
      "Epoch 21/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.3357 - accuracy: 0.0987 - val_loss: 2.3431 - val_accuracy: 0.1031\n",
      "Epoch 22/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.3345 - accuracy: 0.0975 - val_loss: 2.3652 - val_accuracy: 0.1007\n",
      "Epoch 23/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.3348 - accuracy: 0.0985 - val_loss: 2.3520 - val_accuracy: 0.0988\n",
      "Epoch 24/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.3346 - accuracy: 0.0993 - val_loss: 2.3242 - val_accuracy: 0.1016\n",
      "Epoch 25/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.3338 - accuracy: 0.0997 - val_loss: 2.3165 - val_accuracy: 0.1031\n",
      "Epoch 26/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.3347 - accuracy: 0.1001 - val_loss: 2.3162 - val_accuracy: 0.1031\n",
      "Epoch 27/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.3351 - accuracy: 0.0999 - val_loss: 2.3569 - val_accuracy: 0.1007\n",
      "Epoch 28/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.3342 - accuracy: 0.0975 - val_loss: 2.3237 - val_accuracy: 0.0988\n",
      "Epoch 29/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.3359 - accuracy: 0.0974 - val_loss: 2.3358 - val_accuracy: 0.1031\n",
      "Epoch 30/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.3334 - accuracy: 0.1009 - val_loss: 2.3483 - val_accuracy: 0.0946\n",
      "Epoch 31/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.3361 - accuracy: 0.0987 - val_loss: 2.3422 - val_accuracy: 0.0988\n",
      "Epoch 32/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.3326 - accuracy: 0.1027 - val_loss: 2.3337 - val_accuracy: 0.0970\n",
      "Epoch 33/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.3338 - accuracy: 0.0994 - val_loss: 2.3557 - val_accuracy: 0.1031\n",
      "Epoch 34/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.3342 - accuracy: 0.1008 - val_loss: 2.3739 - val_accuracy: 0.0988\n",
      "Epoch 35/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.3339 - accuracy: 0.1036 - val_loss: 2.3315 - val_accuracy: 0.0946\n",
      "Epoch 36/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.3335 - accuracy: 0.0996 - val_loss: 2.3691 - val_accuracy: 0.1010\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 2.3691 - accuracy: 0.1010\n",
      "313/313 [==============================] - 0s 2ms/step - loss: 2.3680 - accuracy: 0.1000\n",
      "fitting lr = 0.001 Logged in: logs/fit/selu_alpha_0.001_20220704-164154 saved in models/selu_alpha_0.001_model.h5\n",
      "Epoch 1/100\n",
      "   2/1172 [..............................] - ETA: 23:05 - loss: 2.9660 - accuracy: 0.0469WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0091s vs `on_train_batch_end` time: 2.3597s). Check your callbacks.\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3804 - accuracy: 0.1001 - val_loss: 2.3100 - val_accuracy: 0.1006\n",
      "Epoch 2/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.3185 - accuracy: 0.1000 - val_loss: 2.4006 - val_accuracy: 0.0970\n",
      "Epoch 3/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.2596 - accuracy: 0.1275 - val_loss: 7.8524 - val_accuracy: 0.1255\n",
      "Epoch 4/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.1331 - accuracy: 0.1718 - val_loss: 8.5378 - val_accuracy: 0.1416\n",
      "Epoch 5/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.1135 - accuracy: 0.1757 - val_loss: 5.9311 - val_accuracy: 0.1664\n",
      "Epoch 6/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.1102 - accuracy: 0.1751 - val_loss: 5.9300 - val_accuracy: 0.1260\n",
      "Epoch 7/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.0982 - accuracy: 0.1785 - val_loss: 7.7967 - val_accuracy: 0.1352\n",
      "Epoch 8/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.1322 - accuracy: 0.1703 - val_loss: 3.8033 - val_accuracy: 0.1366\n",
      "Epoch 9/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.1193 - accuracy: 0.1699 - val_loss: 4.7557 - val_accuracy: 0.1437\n",
      "Epoch 10/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.0985 - accuracy: 0.1746 - val_loss: 4.5960 - val_accuracy: 0.1450\n",
      "Epoch 11/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.0880 - accuracy: 0.1779 - val_loss: 4.3831 - val_accuracy: 0.1573\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 4.3831 - accuracy: 0.1573\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 4.4423 - accuracy: 0.1567\n",
      "fitting lr = 0.0005 Logged in: logs/fit/selu_alpha_0.0005_20220704-164409 saved in models/selu_alpha_0.0005_model.h5\n",
      "Epoch 1/100\n",
      "   2/1172 [..............................] - ETA: 26:34 - loss: 3.0021 - accuracy: 0.1250WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0090s vs `on_train_batch_end` time: 2.7151s). Check your callbacks.\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.4182 - accuracy: 0.0999 - val_loss: 2.3160 - val_accuracy: 0.1031\n",
      "Epoch 2/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.3140 - accuracy: 0.0982 - val_loss: 2.3169 - val_accuracy: 0.0988\n",
      "Epoch 3/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.3113 - accuracy: 0.1000 - val_loss: 2.3136 - val_accuracy: 0.1024\n",
      "Epoch 4/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.3110 - accuracy: 0.0991 - val_loss: 2.3106 - val_accuracy: 0.0988\n",
      "Epoch 5/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.3108 - accuracy: 0.1015 - val_loss: 2.3127 - val_accuracy: 0.0946\n",
      "Epoch 6/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.3267 - accuracy: 0.0985 - val_loss: 2.3088 - val_accuracy: 0.1095\n",
      "Epoch 7/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.2923 - accuracy: 0.1096 - val_loss: 5.9072 - val_accuracy: 0.1430\n",
      "Epoch 8/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.1383 - accuracy: 0.1674 - val_loss: 6.1236 - val_accuracy: 0.1622\n",
      "Epoch 9/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.1137 - accuracy: 0.1773 - val_loss: 4.3582 - val_accuracy: 0.1781\n",
      "Epoch 10/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.1034 - accuracy: 0.1810 - val_loss: 5.6041 - val_accuracy: 0.1626\n",
      "Epoch 11/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.0891 - accuracy: 0.1832 - val_loss: 5.8026 - val_accuracy: 0.1850\n",
      "Epoch 12/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.0765 - accuracy: 0.1898 - val_loss: 4.7842 - val_accuracy: 0.1922\n",
      "Epoch 13/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.0655 - accuracy: 0.2001 - val_loss: 4.8695 - val_accuracy: 0.1943\n",
      "Epoch 14/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.0566 - accuracy: 0.2053 - val_loss: 6.2956 - val_accuracy: 0.1706\n",
      "Epoch 15/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.0645 - accuracy: 0.2018 - val_loss: 6.9918 - val_accuracy: 0.1054\n",
      "Epoch 16/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.0566 - accuracy: 0.2038 - val_loss: 3.2934 - val_accuracy: 0.1831\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 3.2934 - accuracy: 0.1831\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 3.3548 - accuracy: 0.1886\n",
      "fitting lr = 0.00025 Logged in: logs/fit/selu_alpha_0.00025_20220704-164721 saved in models/selu_alpha_0.00025_model.h5\n",
      "Epoch 1/100\n",
      "   2/1172 [..............................] - ETA: 23:20 - loss: 3.1550 - accuracy: 0.1094WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0089s vs `on_train_batch_end` time: 2.3839s). Check your callbacks.\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.4919 - accuracy: 0.1021 - val_loss: 2.3901 - val_accuracy: 0.1024\n",
      "Epoch 2/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.3178 - accuracy: 0.0995 - val_loss: 2.3207 - val_accuracy: 0.0946\n",
      "Epoch 3/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.3106 - accuracy: 0.1019 - val_loss: 2.3128 - val_accuracy: 0.0970\n",
      "Epoch 4/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.3078 - accuracy: 0.0980 - val_loss: 2.3101 - val_accuracy: 0.1024\n",
      "Epoch 5/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.3079 - accuracy: 0.0997 - val_loss: 2.3047 - val_accuracy: 0.1024\n",
      "Epoch 6/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.3072 - accuracy: 0.0997 - val_loss: 2.3080 - val_accuracy: 0.0970\n",
      "Epoch 7/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.3080 - accuracy: 0.0982 - val_loss: 2.3126 - val_accuracy: 0.0946\n",
      "Epoch 8/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.3070 - accuracy: 0.1016 - val_loss: 2.3088 - val_accuracy: 0.1010\n",
      "Epoch 9/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.3072 - accuracy: 0.0987 - val_loss: 2.3074 - val_accuracy: 0.1016\n",
      "Epoch 10/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.3076 - accuracy: 0.0983 - val_loss: 2.3088 - val_accuracy: 0.1031\n",
      "Epoch 11/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.3073 - accuracy: 0.1010 - val_loss: 2.3111 - val_accuracy: 0.1010\n",
      "Epoch 12/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.3072 - accuracy: 0.1001 - val_loss: 2.3167 - val_accuracy: 0.0946\n",
      "Epoch 13/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.3070 - accuracy: 0.1009 - val_loss: 2.3245 - val_accuracy: 0.0975\n",
      "Epoch 14/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.3073 - accuracy: 0.0998 - val_loss: 2.3047 - val_accuracy: 0.0988\n",
      "Epoch 15/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.3078 - accuracy: 0.1014 - val_loss: 2.3137 - val_accuracy: 0.0970\n",
      "Epoch 16/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.3069 - accuracy: 0.1009 - val_loss: 2.3092 - val_accuracy: 0.0975\n",
      "Epoch 17/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.3075 - accuracy: 0.0983 - val_loss: 2.3061 - val_accuracy: 0.1024\n",
      "Epoch 18/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.3073 - accuracy: 0.0999 - val_loss: 2.3085 - val_accuracy: 0.0970\n",
      "Epoch 19/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.3075 - accuracy: 0.0994 - val_loss: 2.3044 - val_accuracy: 0.0975\n",
      "Epoch 20/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.3073 - accuracy: 0.0991 - val_loss: 2.3198 - val_accuracy: 0.0988\n",
      "Epoch 21/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.3074 - accuracy: 0.0978 - val_loss: 2.3067 - val_accuracy: 0.1024\n",
      "Epoch 22/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.3072 - accuracy: 0.1018 - val_loss: 2.3067 - val_accuracy: 0.1010\n",
      "Epoch 23/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.3070 - accuracy: 0.0997 - val_loss: 2.3144 - val_accuracy: 0.0975\n",
      "Epoch 24/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.3074 - accuracy: 0.1007 - val_loss: 2.3063 - val_accuracy: 0.0970\n",
      "Epoch 25/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.3077 - accuracy: 0.0994 - val_loss: 2.3072 - val_accuracy: 0.0975\n",
      "Epoch 26/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.3076 - accuracy: 0.0986 - val_loss: 2.3045 - val_accuracy: 0.1024\n",
      "Epoch 27/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.3072 - accuracy: 0.1026 - val_loss: 2.3052 - val_accuracy: 0.0975\n",
      "Epoch 28/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.3067 - accuracy: 0.1025 - val_loss: 2.3059 - val_accuracy: 0.1007\n",
      "Epoch 29/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.3077 - accuracy: 0.0972 - val_loss: 2.3100 - val_accuracy: 0.0946\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 2.3100 - accuracy: 0.0946\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3083 - accuracy: 0.1000\n",
      "fitting lr = 0.0001 Logged in: logs/fit/selu_alpha_0.0001_20220704-165306 saved in models/selu_alpha_0.0001_model.h5\n",
      "Epoch 1/100\n",
      "   2/1172 [..............................] - ETA: 23:57 - loss: 3.1912 - accuracy: 0.0469WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0080s vs `on_train_batch_end` time: 2.4474s). Check your callbacks.\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.6226 - accuracy: 0.1043 - val_loss: 2.3799 - val_accuracy: 0.1007\n",
      "Epoch 2/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.3903 - accuracy: 0.1010 - val_loss: 2.3236 - val_accuracy: 0.1033\n",
      "Epoch 3/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.3402 - accuracy: 0.0992 - val_loss: 2.3170 - val_accuracy: 0.1033\n",
      "Epoch 4/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.3196 - accuracy: 0.1007 - val_loss: 2.3126 - val_accuracy: 0.1024\n",
      "Epoch 5/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.3117 - accuracy: 0.1020 - val_loss: 2.3058 - val_accuracy: 0.1033\n",
      "Epoch 6/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.2436 - accuracy: 0.1292 - val_loss: 3.4368 - val_accuracy: 0.1734\n",
      "Epoch 7/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.1350 - accuracy: 0.1689 - val_loss: 3.7370 - val_accuracy: 0.1742\n",
      "Epoch 8/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.1203 - accuracy: 0.1708 - val_loss: 5.3292 - val_accuracy: 0.1822\n",
      "Epoch 9/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.1051 - accuracy: 0.1782 - val_loss: 4.6879 - val_accuracy: 0.1774\n",
      "Epoch 10/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.0994 - accuracy: 0.1806 - val_loss: 6.7163 - val_accuracy: 0.1830\n",
      "Epoch 11/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.0891 - accuracy: 0.1806 - val_loss: 4.9831 - val_accuracy: 0.1891\n",
      "Epoch 12/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.0804 - accuracy: 0.1868 - val_loss: 6.0663 - val_accuracy: 0.1828\n",
      "Epoch 13/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.0739 - accuracy: 0.1867 - val_loss: 6.5472 - val_accuracy: 0.1822\n",
      "Epoch 14/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.0654 - accuracy: 0.1921 - val_loss: 6.2435 - val_accuracy: 0.1805\n",
      "Epoch 15/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.0579 - accuracy: 0.1933 - val_loss: 6.4695 - val_accuracy: 0.1800\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 6.4695 - accuracy: 0.1800\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 6.6069 - accuracy: 0.1774\n",
      "fitting lr = 5e-05 Logged in: logs/fit/selu_alpha_5e-05_20220704-165617 saved in models/selu_alpha_5e-05_model.h5\n",
      "Epoch 1/100\n",
      "   2/1172 [..............................] - ETA: 23:47 - loss: 3.0521 - accuracy: 0.0469WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0090s vs `on_train_batch_end` time: 2.4286s). Check your callbacks.\n",
      "1172/1172 [==============================] - 15s 12ms/step - loss: 2.8090 - accuracy: 0.1000 - val_loss: 2.5478 - val_accuracy: 0.0898\n",
      "Epoch 2/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.5470 - accuracy: 0.0995 - val_loss: 2.3503 - val_accuracy: 0.1031\n",
      "Epoch 3/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.4315 - accuracy: 0.1019 - val_loss: 2.3322 - val_accuracy: 0.1427\n",
      "Epoch 4/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.2753 - accuracy: 0.1444 - val_loss: 2.3833 - val_accuracy: 0.1601\n",
      "Epoch 5/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.2027 - accuracy: 0.1570 - val_loss: 2.3620 - val_accuracy: 0.1562\n",
      "Epoch 6/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.1710 - accuracy: 0.1598 - val_loss: 2.3383 - val_accuracy: 0.1657\n",
      "Epoch 7/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.1474 - accuracy: 0.1641 - val_loss: 2.4735 - val_accuracy: 0.1709\n",
      "Epoch 8/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.1354 - accuracy: 0.1648 - val_loss: 2.5050 - val_accuracy: 0.1652\n",
      "Epoch 9/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.1260 - accuracy: 0.1672 - val_loss: 2.4567 - val_accuracy: 0.1677\n",
      "Epoch 10/100\n",
      "1172/1172 [==============================] - 12s 11ms/step - loss: 2.1155 - accuracy: 0.1710 - val_loss: 2.5357 - val_accuracy: 0.1803\n",
      "Epoch 11/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.1081 - accuracy: 0.1765 - val_loss: 2.7937 - val_accuracy: 0.1828\n",
      "Epoch 12/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.0989 - accuracy: 0.1739 - val_loss: 2.9747 - val_accuracy: 0.1868\n",
      "Epoch 13/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.0943 - accuracy: 0.1764 - val_loss: 3.2134 - val_accuracy: 0.1846\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 3.2134 - accuracy: 0.1846\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 3.2454 - accuracy: 0.1888\n",
      "Best validation performance: 18.46% for lr: 5e-05\n",
      "Best test performance: 18.88% for lr: 5e-05\n",
      "validation accuracy: [0.10239999741315842, 0.10159999877214432, 0.10103999823331833, 0.15727999806404114, 0.183119997382164, 0.09455999732017517, 0.18000000715255737, 0.18464000523090363]\n",
      "test accuracy: [0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.156700000166893, 0.18860000371932983, 0.10000000149011612, 0.17739999294281006, 0.18880000710487366]\n"
     ]
    }
   ],
   "source": [
    "#Start with 0.1 dropout\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from functools import partial\n",
    "\n",
    "selu_layer = partial(keras.layers.Dense,kernel_initializer=\"lecun_normal\", activation=\"selu\")\n",
    "\n",
    "def make_DNN_alpha (layer_function, n_hidden,use_normalization=False,dropout=0.1):\n",
    "    architecture = [keras.layers.Flatten(input_shape=[32,32,3])]\n",
    "    architecture.append(keras.layers.AlphaDropout(rate=dropout))\n",
    "    for _ in range(n_hidden):\n",
    "        architecture.append(layer_function(100))\n",
    "        architecture.append(keras.layers.AlphaDropout(rate=dropout))\n",
    "        \n",
    "    architecture.append(keras.layers.Dense(10,activation=\"softmax\"))\n",
    "    return(keras.models.Sequential(architecture))\n",
    "\n",
    "def selu_alpha_model():\n",
    "    return make_DNN_alpha(selu_layer,20,False,0.1)\n",
    "\n",
    "#selu_model().summary()\n",
    "\n",
    "lrs=[0.01, 0.005, 0.0025, 0.001, 0.0005, 0.00025, 0.0001, 0.00005]\n",
    "\n",
    "valid_accuracy,test_accuracy,logs, best_models = tune_lrs(selu_alpha_model, lrs, \"selu_alpha\")\n",
    "\n",
    "print(f\"validation accuracy: {valid_accuracy}\")\n",
    "print(f\"test accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting lr = 0.01 Logged in: logs/fit/selu_alpha_lower_0.01_20220704-170042 saved in models/selu_alpha_lower_0.01_model.h5\n",
      "Epoch 1/100\n",
      "   2/1172 [..............................] - ETA: 27:35 - loss: 3.1331 - accuracy: 0.1250WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0080s vs `on_train_batch_end` time: 2.8215s). Check your callbacks.\n",
      "1172/1172 [==============================] - 15s 13ms/step - loss: 50.8687 - accuracy: 0.1017 - val_loss: 2.3644 - val_accuracy: 0.0970\n",
      "Epoch 2/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.3923 - accuracy: 0.0991 - val_loss: 2.3975 - val_accuracy: 0.0975\n",
      "Epoch 3/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.4023 - accuracy: 0.0967 - val_loss: 2.3985 - val_accuracy: 0.1010\n",
      "Epoch 4/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.4409 - accuracy: 0.0978 - val_loss: 2.4425 - val_accuracy: 0.1024\n",
      "Epoch 5/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.3835 - accuracy: 0.0992 - val_loss: 2.3991 - val_accuracy: 0.1024\n",
      "Epoch 6/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 4.4664 - accuracy: 0.0983 - val_loss: 2.4535 - val_accuracy: 0.1031\n",
      "Epoch 7/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.3816 - accuracy: 0.0981 - val_loss: 2.3834 - val_accuracy: 0.0970\n",
      "Epoch 8/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 515573.2812 - accuracy: 0.1013 - val_loss: 2.4904 - val_accuracy: 0.1024\n",
      "Epoch 9/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 183.2465 - accuracy: 0.0988 - val_loss: 2.3468 - val_accuracy: 0.1031\n",
      "Epoch 10/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.3804 - accuracy: 0.1003 - val_loss: 2.3638 - val_accuracy: 0.0946\n",
      "Epoch 11/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.3807 - accuracy: 0.0995 - val_loss: 2.4729 - val_accuracy: 0.1031\n",
      "Epoch 12/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.3857 - accuracy: 0.0988 - val_loss: 2.5411 - val_accuracy: 0.0975\n",
      "Epoch 13/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.3835 - accuracy: 0.1014 - val_loss: 2.3701 - val_accuracy: 0.1007\n",
      "Epoch 14/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.3840 - accuracy: 0.1017 - val_loss: 2.5145 - val_accuracy: 0.0946\n",
      "Epoch 15/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.3856 - accuracy: 0.0987 - val_loss: 2.3960 - val_accuracy: 0.1007\n",
      "Epoch 16/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.3856 - accuracy: 0.1002 - val_loss: 2.3701 - val_accuracy: 0.0988\n",
      "Epoch 17/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 215.2559 - accuracy: 0.1005 - val_loss: 2.3880 - val_accuracy: 0.1024\n",
      "Epoch 18/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.3851 - accuracy: 0.0997 - val_loss: 2.4981 - val_accuracy: 0.0946\n",
      "Epoch 19/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.3878 - accuracy: 0.0997 - val_loss: 2.5029 - val_accuracy: 0.1016\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 2.5029 - accuracy: 0.1016\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.5005 - accuracy: 0.1000\n",
      "fitting lr = 0.005 Logged in: logs/fit/selu_alpha_lower_0.005_20220704-170435 saved in models/selu_alpha_lower_0.005_model.h5\n",
      "Epoch 1/100\n",
      "   2/1172 [..............................] - ETA: 23:32 - loss: 3.3343 - accuracy: 0.0312WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0080s vs `on_train_batch_end` time: 2.4071s). Check your callbacks.\n",
      "1172/1172 [==============================] - 15s 12ms/step - loss: 5.1407 - accuracy: 0.0995 - val_loss: 2.3748 - val_accuracy: 0.0946\n",
      "Epoch 2/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.3331 - accuracy: 0.0982 - val_loss: 2.3301 - val_accuracy: 0.0975\n",
      "Epoch 3/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.3362 - accuracy: 0.0996 - val_loss: 2.3264 - val_accuracy: 0.1024\n",
      "Epoch 4/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.3402 - accuracy: 0.0993 - val_loss: 2.4082 - val_accuracy: 0.1033\n",
      "Epoch 5/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.3460 - accuracy: 0.0994 - val_loss: 2.3535 - val_accuracy: 0.1016\n",
      "Epoch 6/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.4866 - accuracy: 0.1010 - val_loss: 2.3691 - val_accuracy: 0.1016\n",
      "Epoch 7/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.5765 - accuracy: 0.0999 - val_loss: 2.3971 - val_accuracy: 0.1016\n",
      "Epoch 8/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.3569 - accuracy: 0.0973 - val_loss: 2.3458 - val_accuracy: 0.1007\n",
      "Epoch 9/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.3523 - accuracy: 0.0977 - val_loss: 2.3913 - val_accuracy: 0.0970\n",
      "Epoch 10/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.3538 - accuracy: 0.0981 - val_loss: 2.3682 - val_accuracy: 0.1016\n",
      "Epoch 11/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.3517 - accuracy: 0.0985 - val_loss: 2.3423 - val_accuracy: 0.1007\n",
      "Epoch 12/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.3536 - accuracy: 0.1023 - val_loss: 2.3744 - val_accuracy: 0.0988\n",
      "Epoch 13/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.3558 - accuracy: 0.0980 - val_loss: 2.4169 - val_accuracy: 0.1007\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 2.4169 - accuracy: 0.1007\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.4137 - accuracy: 0.1000\n",
      "fitting lr = 0.0025 Logged in: logs/fit/selu_alpha_lower_0.0025_20220704-170724 saved in models/selu_alpha_lower_0.0025_model.h5\n",
      "Epoch 1/100\n",
      "   2/1172 [..............................] - ETA: 26:11 - loss: 3.2331 - accuracy: 0.1094WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0090s vs `on_train_batch_end` time: 2.6760s). Check your callbacks.\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.3998 - accuracy: 0.0994 - val_loss: 2.3878 - val_accuracy: 0.1033\n",
      "Epoch 2/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.3372 - accuracy: 0.1012 - val_loss: 2.3584 - val_accuracy: 0.1024\n",
      "Epoch 3/100\n",
      "1172/1172 [==============================] - 12s 11ms/step - loss: 2.3396 - accuracy: 0.1009 - val_loss: 2.3518 - val_accuracy: 0.0946\n",
      "Epoch 4/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.3388 - accuracy: 0.1003 - val_loss: 2.3567 - val_accuracy: 0.1010\n",
      "Epoch 5/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.3338 - accuracy: 0.1000 - val_loss: 2.3319 - val_accuracy: 0.0975\n",
      "Epoch 6/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.3325 - accuracy: 0.0991 - val_loss: 2.3278 - val_accuracy: 0.0988\n",
      "Epoch 7/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.3332 - accuracy: 0.1004 - val_loss: 2.3796 - val_accuracy: 0.0946\n",
      "Epoch 8/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 3.1957 - accuracy: 0.0999 - val_loss: 2.3338 - val_accuracy: 0.0988\n",
      "Epoch 9/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.3328 - accuracy: 0.1021 - val_loss: 2.3461 - val_accuracy: 0.0988\n",
      "Epoch 10/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.3240 - accuracy: 0.1017 - val_loss: 2.3352 - val_accuracy: 0.0946\n",
      "Epoch 11/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.3263 - accuracy: 0.1002 - val_loss: 2.3305 - val_accuracy: 0.0946\n",
      "Epoch 12/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.3294 - accuracy: 0.1009 - val_loss: 2.3365 - val_accuracy: 0.1033\n",
      "Epoch 13/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.3312 - accuracy: 0.0968 - val_loss: 2.3458 - val_accuracy: 0.1016\n",
      "Epoch 14/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.3494 - accuracy: 0.1012 - val_loss: 2.3146 - val_accuracy: 0.1010\n",
      "Epoch 15/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.3347 - accuracy: 0.1014 - val_loss: 2.3356 - val_accuracy: 0.0975\n",
      "Epoch 16/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.3808 - accuracy: 0.0993 - val_loss: 2.3388 - val_accuracy: 0.0946\n",
      "Epoch 17/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.3348 - accuracy: 0.1008 - val_loss: 2.3293 - val_accuracy: 0.1016\n",
      "Epoch 18/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 2.3347 - accuracy: 0.0982 - val_loss: 2.3536 - val_accuracy: 0.1016\n",
      "Epoch 19/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.3359 - accuracy: 0.1006 - val_loss: 2.3545 - val_accuracy: 0.1007\n",
      "Epoch 20/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.3353 - accuracy: 0.1006 - val_loss: 2.3152 - val_accuracy: 0.0988\n",
      "Epoch 21/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.3360 - accuracy: 0.1005 - val_loss: 2.3334 - val_accuracy: 0.0970\n",
      "Epoch 22/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.3371 - accuracy: 0.0985 - val_loss: 2.3386 - val_accuracy: 0.1010\n",
      "Epoch 23/100\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 2.3364 - accuracy: 0.1008 - val_loss: 2.3263 - val_accuracy: 0.1010\n",
      "Epoch 24/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.3371 - accuracy: 0.0985 - val_loss: 2.3423 - val_accuracy: 0.0988\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.3432 - accuracy: 0.09 - 1s 2ms/step - loss: 2.3423 - accuracy: 0.0988\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3388 - accuracy: 0.1000\n",
      "fitting lr = 0.001 Logged in: logs/fit/selu_alpha_lower_0.001_20220704-171206 saved in models/selu_alpha_lower_0.001_model.h5\n",
      "Epoch 1/100\n",
      "   2/1172 [..............................] - ETA: 23:35 - loss: 3.0483 - accuracy: 0.0781WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0101s vs `on_train_batch_end` time: 2.4097s). Check your callbacks.\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.2331 - accuracy: 0.1518 - val_loss: 2.2128 - val_accuracy: 0.1574\n",
      "Epoch 2/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.2660 - accuracy: 0.1216 - val_loss: 2.3134 - val_accuracy: 0.1061\n",
      "Epoch 3/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.3147 - accuracy: 0.1072 - val_loss: 2.3160 - val_accuracy: 0.0990\n",
      "Epoch 4/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.1486 - accuracy: 0.1621 - val_loss: 2.2323 - val_accuracy: 0.1617\n",
      "Epoch 5/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.1071 - accuracy: 0.1703 - val_loss: 2.3277 - val_accuracy: 0.1816\n",
      "Epoch 6/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.0739 - accuracy: 0.1843 - val_loss: 2.2521 - val_accuracy: 0.1544\n",
      "Epoch 7/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.1678 - accuracy: 0.1647 - val_loss: 2.2214 - val_accuracy: 0.1889\n",
      "Epoch 8/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.0670 - accuracy: 0.1869 - val_loss: 2.2057 - val_accuracy: 0.1926\n",
      "Epoch 9/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.0448 - accuracy: 0.1980 - val_loss: 2.2008 - val_accuracy: 0.1812\n",
      "Epoch 10/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.0407 - accuracy: 0.1963 - val_loss: 2.2526 - val_accuracy: 0.1957\n",
      "Epoch 11/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.0545 - accuracy: 0.1920 - val_loss: 2.3746 - val_accuracy: 0.1923\n",
      "Epoch 12/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.0565 - accuracy: 0.1946 - val_loss: 2.3058 - val_accuracy: 0.2110\n",
      "Epoch 13/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.0280 - accuracy: 0.2103 - val_loss: 2.1618 - val_accuracy: 0.2196\n",
      "Epoch 14/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.1198 - accuracy: 0.1858 - val_loss: 4.1426 - val_accuracy: 0.1254\n",
      "Epoch 15/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.0526 - accuracy: 0.1940 - val_loss: 2.3373 - val_accuracy: 0.1792\n",
      "Epoch 16/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.1601 - accuracy: 0.1726 - val_loss: 2.9476 - val_accuracy: 0.1531\n",
      "Epoch 17/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.0940 - accuracy: 0.1845 - val_loss: 2.6505 - val_accuracy: 0.1832\n",
      "Epoch 18/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.0687 - accuracy: 0.1895 - val_loss: 2.6686 - val_accuracy: 0.1710\n",
      "Epoch 19/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.0503 - accuracy: 0.1948 - val_loss: 2.2256 - val_accuracy: 0.1922\n",
      "Epoch 20/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.0380 - accuracy: 0.2018 - val_loss: 2.2935 - val_accuracy: 0.1643\n",
      "Epoch 21/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.0231 - accuracy: 0.2102 - val_loss: 2.4674 - val_accuracy: 0.1808\n",
      "Epoch 22/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.0158 - accuracy: 0.2154 - val_loss: 2.5766 - val_accuracy: 0.1846\n",
      "Epoch 23/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.0009 - accuracy: 0.2223 - val_loss: 2.8680 - val_accuracy: 0.1751\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 2.8680 - accuracy: 0.1751\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.8827 - accuracy: 0.1758\n",
      "fitting lr = 0.0005 Logged in: logs/fit/selu_alpha_lower_0.0005_20220704-171639 saved in models/selu_alpha_lower_0.0005_model.h5\n",
      "Epoch 1/100\n",
      "   2/1172 [..............................] - ETA: 23:17 - loss: 3.2602 - accuracy: 0.0938WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0089s vs `on_train_batch_end` time: 2.3811s). Check your callbacks.\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.2362 - accuracy: 0.1539 - val_loss: 2.1897 - val_accuracy: 0.1735\n",
      "Epoch 2/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.0954 - accuracy: 0.1807 - val_loss: 2.1507 - val_accuracy: 0.1966\n",
      "Epoch 3/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.0483 - accuracy: 0.2049 - val_loss: 2.7292 - val_accuracy: 0.1992\n",
      "Epoch 4/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.0154 - accuracy: 0.2301 - val_loss: 2.2697 - val_accuracy: 0.2387\n",
      "Epoch 5/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 1.9667 - accuracy: 0.2503 - val_loss: 2.4361 - val_accuracy: 0.2550\n",
      "Epoch 6/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 1.9408 - accuracy: 0.2597 - val_loss: 2.2132 - val_accuracy: 0.2528\n",
      "Epoch 7/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 1.9193 - accuracy: 0.2706 - val_loss: 2.3922 - val_accuracy: 0.2897\n",
      "Epoch 8/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 1.8996 - accuracy: 0.2793 - val_loss: 2.2920 - val_accuracy: 0.2826\n",
      "Epoch 9/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 1.8845 - accuracy: 0.2945 - val_loss: 2.2232 - val_accuracy: 0.2816\n",
      "Epoch 10/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 1.8680 - accuracy: 0.2988 - val_loss: 2.5015 - val_accuracy: 0.3074\n",
      "Epoch 11/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 1.8565 - accuracy: 0.3061 - val_loss: 2.7190 - val_accuracy: 0.2990\n",
      "Epoch 12/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 1.8440 - accuracy: 0.3161 - val_loss: 2.3751 - val_accuracy: 0.3187\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 2.3751 - accuracy: 0.3187\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3337 - accuracy: 0.3185\n",
      "fitting lr = 0.00025 Logged in: logs/fit/selu_alpha_lower_0.00025_20220704-171911 saved in models/selu_alpha_lower_0.00025_model.h5\n",
      "Epoch 1/100\n",
      "   2/1172 [..............................] - ETA: 24:13 - loss: 3.1834 - accuracy: 0.1094WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0100s vs `on_train_batch_end` time: 2.4749s). Check your callbacks.\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.3334 - accuracy: 0.1386 - val_loss: 2.1642 - val_accuracy: 0.1824\n",
      "Epoch 2/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.1230 - accuracy: 0.1725 - val_loss: 2.0631 - val_accuracy: 0.1902\n",
      "Epoch 3/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.0843 - accuracy: 0.1846 - val_loss: 2.1487 - val_accuracy: 0.1820\n",
      "Epoch 4/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.0387 - accuracy: 0.2157 - val_loss: 1.9913 - val_accuracy: 0.2570\n",
      "Epoch 5/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.9789 - accuracy: 0.2427 - val_loss: 2.2064 - val_accuracy: 0.2452\n",
      "Epoch 6/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.9483 - accuracy: 0.2621 - val_loss: 2.1631 - val_accuracy: 0.2758\n",
      "Epoch 7/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 1.9166 - accuracy: 0.2796 - val_loss: 2.2866 - val_accuracy: 0.2951\n",
      "Epoch 8/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 1.8963 - accuracy: 0.2931 - val_loss: 2.2590 - val_accuracy: 0.3021\n",
      "Epoch 9/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 1.8804 - accuracy: 0.2985 - val_loss: 2.1423 - val_accuracy: 0.3170\n",
      "Epoch 10/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 1.8636 - accuracy: 0.3062 - val_loss: 2.0638 - val_accuracy: 0.3290\n",
      "Epoch 11/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 1.8471 - accuracy: 0.3142 - val_loss: 2.1646 - val_accuracy: 0.3302\n",
      "Epoch 12/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.8312 - accuracy: 0.3210 - val_loss: 2.1353 - val_accuracy: 0.3358\n",
      "Epoch 13/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 1.8190 - accuracy: 0.3293 - val_loss: 2.1772 - val_accuracy: 0.3398\n",
      "Epoch 14/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 1.8110 - accuracy: 0.3351 - val_loss: 2.2035 - val_accuracy: 0.3390\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 2.2035 - accuracy: 0.3390\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.1924 - accuracy: 0.3455\n",
      "fitting lr = 0.0001 Logged in: logs/fit/selu_alpha_lower_0.0001_20220704-172207 saved in models/selu_alpha_lower_0.0001_model.h5\n",
      "Epoch 1/100\n",
      "   2/1172 [..............................] - ETA: 24:02 - loss: 3.2512 - accuracy: 0.0781WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0100s vs `on_train_batch_end` time: 2.4563s). Check your callbacks.\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.4577 - accuracy: 0.1303 - val_loss: 2.1206 - val_accuracy: 0.1817\n",
      "Epoch 2/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.1884 - accuracy: 0.1623 - val_loss: 2.0739 - val_accuracy: 0.1866\n",
      "Epoch 3/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.1340 - accuracy: 0.1724 - val_loss: 2.0867 - val_accuracy: 0.1923\n",
      "Epoch 4/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.1004 - accuracy: 0.1827 - val_loss: 2.0251 - val_accuracy: 0.2096\n",
      "Epoch 5/100\n",
      "1172/1172 [==============================] - 12s 11ms/step - loss: 2.0612 - accuracy: 0.2084 - val_loss: 2.0500 - val_accuracy: 0.2455\n",
      "Epoch 6/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.0136 - accuracy: 0.2281 - val_loss: 2.0507 - val_accuracy: 0.2582\n",
      "Epoch 7/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.9782 - accuracy: 0.2444 - val_loss: 2.1584 - val_accuracy: 0.2622\n",
      "Epoch 8/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 1.9546 - accuracy: 0.2555 - val_loss: 2.1307 - val_accuracy: 0.2847\n",
      "Epoch 9/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 1.9353 - accuracy: 0.2679 - val_loss: 2.0928 - val_accuracy: 0.2829\n",
      "Epoch 10/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 1.9216 - accuracy: 0.2757 - val_loss: 2.0556 - val_accuracy: 0.2866\n",
      "Epoch 11/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 1.9105 - accuracy: 0.2817 - val_loss: 2.1408 - val_accuracy: 0.2999\n",
      "Epoch 12/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 1.8974 - accuracy: 0.2881 - val_loss: 2.0230 - val_accuracy: 0.3108\n",
      "Epoch 13/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 1.8878 - accuracy: 0.2947 - val_loss: 2.0407 - val_accuracy: 0.3142\n",
      "Epoch 14/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 1.8789 - accuracy: 0.2972 - val_loss: 2.1494 - val_accuracy: 0.3074\n",
      "Epoch 15/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 1.8638 - accuracy: 0.3051 - val_loss: 2.0597 - val_accuracy: 0.3248\n",
      "Epoch 16/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 1.8587 - accuracy: 0.3087 - val_loss: 2.1221 - val_accuracy: 0.3201\n",
      "Epoch 17/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 1.8554 - accuracy: 0.3122 - val_loss: 1.9684 - val_accuracy: 0.3381\n",
      "Epoch 18/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 1.8462 - accuracy: 0.3142 - val_loss: 1.9515 - val_accuracy: 0.3386\n",
      "Epoch 19/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 1.8341 - accuracy: 0.3214 - val_loss: 2.0666 - val_accuracy: 0.3405\n",
      "Epoch 20/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 1.8260 - accuracy: 0.3226 - val_loss: 2.0168 - val_accuracy: 0.3408\n",
      "Epoch 21/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 1.8211 - accuracy: 0.3250 - val_loss: 2.0063 - val_accuracy: 0.3469\n",
      "Epoch 22/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 1.8103 - accuracy: 0.3318 - val_loss: 1.9486 - val_accuracy: 0.3529\n",
      "Epoch 23/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 1.8054 - accuracy: 0.3369 - val_loss: 2.0945 - val_accuracy: 0.3478\n",
      "Epoch 24/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 1.7975 - accuracy: 0.3355 - val_loss: 1.9745 - val_accuracy: 0.3576\n",
      "Epoch 25/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 1.7878 - accuracy: 0.3446 - val_loss: 1.9544 - val_accuracy: 0.3552\n",
      "Epoch 26/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 1.7843 - accuracy: 0.3514 - val_loss: 1.9553 - val_accuracy: 0.3711\n",
      "Epoch 27/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 1.7794 - accuracy: 0.3510 - val_loss: 1.9498 - val_accuracy: 0.3630\n",
      "Epoch 28/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 1.7708 - accuracy: 0.3541 - val_loss: 2.0057 - val_accuracy: 0.3685\n",
      "Epoch 29/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 1.7596 - accuracy: 0.3618 - val_loss: 1.9883 - val_accuracy: 0.3786\n",
      "Epoch 30/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 1.7574 - accuracy: 0.3616 - val_loss: 2.1005 - val_accuracy: 0.3745\n",
      "Epoch 31/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 1.7480 - accuracy: 0.3643 - val_loss: 1.8963 - val_accuracy: 0.3870\n",
      "Epoch 32/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.7402 - accuracy: 0.3654 - val_loss: 2.0321 - val_accuracy: 0.3883\n",
      "Epoch 33/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 1.7379 - accuracy: 0.3672 - val_loss: 1.9346 - val_accuracy: 0.3910\n",
      "Epoch 34/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 1.7321 - accuracy: 0.3761 - val_loss: 1.9315 - val_accuracy: 0.4013\n",
      "Epoch 35/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 1.7296 - accuracy: 0.3785 - val_loss: 2.0242 - val_accuracy: 0.4018\n",
      "Epoch 36/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 1.7229 - accuracy: 0.3770 - val_loss: 1.9666 - val_accuracy: 0.4058\n",
      "Epoch 37/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 1.7141 - accuracy: 0.3838 - val_loss: 2.0975 - val_accuracy: 0.3876\n",
      "Epoch 38/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 1.7086 - accuracy: 0.3858 - val_loss: 2.1004 - val_accuracy: 0.4074\n",
      "Epoch 39/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 1.7054 - accuracy: 0.3830 - val_loss: 2.0440 - val_accuracy: 0.4016\n",
      "Epoch 40/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 1.7058 - accuracy: 0.3873 - val_loss: 1.8998 - val_accuracy: 0.4020\n",
      "Epoch 41/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.6948 - accuracy: 0.3922 - val_loss: 1.9516 - val_accuracy: 0.4094\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 1.9516 - accuracy: 0.4094\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 1.9331 - accuracy: 0.4185\n",
      "fitting lr = 5e-05 Logged in: logs/fit/selu_alpha_lower_5e-05_20220704-173010 saved in models/selu_alpha_lower_5e-05_model.h5\n",
      "Epoch 1/100\n",
      "   2/1172 [..............................] - ETA: 25:24 - loss: 2.8895 - accuracy: 0.1094WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0088s vs `on_train_batch_end` time: 2.5965s). Check your callbacks.\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.6412 - accuracy: 0.0994 - val_loss: 2.3437 - val_accuracy: 0.0988\n",
      "Epoch 2/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.3102 - accuracy: 0.1419 - val_loss: 2.1409 - val_accuracy: 0.1679\n",
      "Epoch 3/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.1844 - accuracy: 0.1626 - val_loss: 2.0888 - val_accuracy: 0.1732\n",
      "Epoch 4/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.1464 - accuracy: 0.1671 - val_loss: 2.0560 - val_accuracy: 0.1950\n",
      "Epoch 5/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.1220 - accuracy: 0.1717 - val_loss: 2.0651 - val_accuracy: 0.1963\n",
      "Epoch 6/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.1071 - accuracy: 0.1778 - val_loss: 2.0627 - val_accuracy: 0.1942\n",
      "Epoch 7/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.0929 - accuracy: 0.1822 - val_loss: 2.0359 - val_accuracy: 0.1960\n",
      "Epoch 8/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.0825 - accuracy: 0.1863 - val_loss: 2.0165 - val_accuracy: 0.2099\n",
      "Epoch 9/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.0677 - accuracy: 0.1893 - val_loss: 2.0579 - val_accuracy: 0.2094\n",
      "Epoch 10/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 2.0561 - accuracy: 0.1957 - val_loss: 2.0353 - val_accuracy: 0.2122\n",
      "Epoch 11/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.0514 - accuracy: 0.2000 - val_loss: 2.0071 - val_accuracy: 0.2226\n",
      "Epoch 12/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.0364 - accuracy: 0.2103 - val_loss: 2.0002 - val_accuracy: 0.2444\n",
      "Epoch 13/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.0200 - accuracy: 0.2193 - val_loss: 2.0963 - val_accuracy: 0.2376\n",
      "Epoch 14/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.0071 - accuracy: 0.2323 - val_loss: 1.9844 - val_accuracy: 0.2540\n",
      "Epoch 15/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.9863 - accuracy: 0.2430 - val_loss: 2.0869 - val_accuracy: 0.2630\n",
      "Epoch 16/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.9711 - accuracy: 0.2474 - val_loss: 2.0156 - val_accuracy: 0.2724\n",
      "Epoch 17/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.9522 - accuracy: 0.2520 - val_loss: 2.0406 - val_accuracy: 0.2766\n",
      "Epoch 18/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.9397 - accuracy: 0.2619 - val_loss: 2.0444 - val_accuracy: 0.2900\n",
      "Epoch 19/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.9304 - accuracy: 0.2667 - val_loss: 2.2040 - val_accuracy: 0.2782\n",
      "Epoch 20/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.9189 - accuracy: 0.2760 - val_loss: 2.0266 - val_accuracy: 0.2935\n",
      "Epoch 21/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.9133 - accuracy: 0.2777 - val_loss: 2.0212 - val_accuracy: 0.3082\n",
      "Epoch 22/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.9022 - accuracy: 0.2816 - val_loss: 2.0180 - val_accuracy: 0.3095\n",
      "Epoch 23/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.8945 - accuracy: 0.2924 - val_loss: 2.0269 - val_accuracy: 0.3132\n",
      "Epoch 24/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.8885 - accuracy: 0.2942 - val_loss: 1.9913 - val_accuracy: 0.3206\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 1.9913 - accuracy: 0.3206\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 1.9684 - accuracy: 0.3271\n",
      "Best validation performance: 40.94% for lr: 0.0001\n",
      "Best test performance: 41.85% for lr: 0.0001\n",
      "validation accuracy: [0.10159999877214432, 0.10072000324726105, 0.09880000352859497, 0.17511999607086182, 0.3187200129032135, 0.3389599919319153, 0.40935999155044556, 0.3205600082874298]\n",
      "test accuracy: [0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.17579999566078186, 0.31850001215934753, 0.34549999237060547, 0.41850000619888306, 0.32710000872612]\n"
     ]
    }
   ],
   "source": [
    "def selu_alpha_model_lower():\n",
    "    return make_DNN_alpha(selu_layer,20,False,0.05)\n",
    "\n",
    "lrs=[0.01, 0.005, 0.0025, 0.001, 0.0005, 0.00025, 0.0001, 0.00005]\n",
    "\n",
    "valid_accuracy,test_accuracy,logs, best_models = tune_lrs(selu_alpha_model_lower, lrs, \"selu_alpha_lower\")\n",
    "\n",
    "print(f\"validation accuracy: {valid_accuracy}\")\n",
    "print(f\"test accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting lr = 0.01 Logged in: logs/fit/selu_alpha_even_lower_0.01_20220704-173843 saved in models/selu_alpha_even_lower_0.01_model.h5\n",
      "Epoch 1/100\n",
      "   2/1172 [..............................] - ETA: 25:11 - loss: 3.9613 - accuracy: 0.0469 WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0100s vs `on_train_batch_end` time: 2.5736s). Check your callbacks.\n",
      "1172/1172 [==============================] - 15s 13ms/step - loss: 2.6521 - accuracy: 0.0962 - val_loss: 2.3870 - val_accuracy: 0.0946\n",
      "Epoch 2/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 34377088.0000 - accuracy: 0.0969 - val_loss: 7.1973 - val_accuracy: 0.1016\n",
      "Epoch 3/100\n",
      "1172/1172 [==============================] - 12s 11ms/step - loss: 132.1255 - accuracy: 0.0988 - val_loss: 2.6452 - val_accuracy: 0.1016\n",
      "Epoch 4/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 56.3791 - accuracy: 0.1014 - val_loss: 2.4481 - val_accuracy: 0.1016\n",
      "Epoch 5/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 8552.8535 - accuracy: 0.0994 - val_loss: 2.5133 - val_accuracy: 0.1010\n",
      "Epoch 6/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 47.0732 - accuracy: 0.1040 - val_loss: 2.4568 - val_accuracy: 0.1016\n",
      "Epoch 7/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 48.1857 - accuracy: 0.1010 - val_loss: 2.4279 - val_accuracy: 0.1016\n",
      "Epoch 8/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 140.9463 - accuracy: 0.0992 - val_loss: 2.4472 - val_accuracy: 0.1016\n",
      "Epoch 9/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 10.9996 - accuracy: 0.0987 - val_loss: 2.4211 - val_accuracy: 0.1016\n",
      "Epoch 10/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 22.6957 - accuracy: 0.1015 - val_loss: 2.4327 - val_accuracy: 0.1016\n",
      "Epoch 11/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 8.9867 - accuracy: 0.1013 - val_loss: 2.4106 - val_accuracy: 0.1016\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 2.4106 - accuracy: 0.1016\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.4100 - accuracy: 0.1000: 0s - loss: 2.4039 - accuracy: 0. - ETA: 0s - loss: 2.4112 - accu\n",
      "fitting lr = 0.005 Logged in: logs/fit/selu_alpha_even_lower_0.005_20220704-174114 saved in models/selu_alpha_even_lower_0.005_model.h5\n",
      "Epoch 1/100\n",
      "   2/1172 [..............................] - ETA: 31:14 - loss: 3.1965 - accuracy: 0.0938WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0080s vs `on_train_batch_end` time: 3.1978s). Check your callbacks.\n",
      "1172/1172 [==============================] - 16s 14ms/step - loss: 2.4428 - accuracy: 0.1003 - val_loss: 2.3609 - val_accuracy: 0.1024\n",
      "Epoch 2/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 26.0323 - accuracy: 0.0968 - val_loss: 2.3608 - val_accuracy: 0.0988\n",
      "Epoch 3/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.3294 - accuracy: 0.1024 - val_loss: 2.3503 - val_accuracy: 0.0975\n",
      "Epoch 4/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.3292 - accuracy: 0.1001 - val_loss: 2.3222 - val_accuracy: 0.0975\n",
      "Epoch 5/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.3341 - accuracy: 0.0987 - val_loss: 2.3351 - val_accuracy: 0.0970\n",
      "Epoch 6/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.3356 - accuracy: 0.1013 - val_loss: 2.3267 - val_accuracy: 0.0946\n",
      "Epoch 7/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.3390 - accuracy: 0.1007 - val_loss: 2.3367 - val_accuracy: 0.1007\n",
      "Epoch 8/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.3425 - accuracy: 0.0987 - val_loss: 2.3642 - val_accuracy: 0.0946\n",
      "Epoch 9/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.3447 - accuracy: 0.0987 - val_loss: 2.3737 - val_accuracy: 0.0975\n",
      "Epoch 10/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 81.4147 - accuracy: 0.1009 - val_loss: 2.4490 - val_accuracy: 0.0946\n",
      "Epoch 11/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 3.8471 - accuracy: 0.1014 - val_loss: 2.3471 - val_accuracy: 0.0946\n",
      "Epoch 12/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.4126 - accuracy: 0.0993 - val_loss: 2.3948 - val_accuracy: 0.0946\n",
      "Epoch 13/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.3905 - accuracy: 0.0997 - val_loss: 2.3337 - val_accuracy: 0.1033\n",
      "Epoch 14/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.3726 - accuracy: 0.1018 - val_loss: 2.3848 - val_accuracy: 0.0946\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 2.3848 - accuracy: 0.0946\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3796 - accuracy: 0.1000\n",
      "fitting lr = 0.0025 Logged in: logs/fit/selu_alpha_even_lower_0.0025_20220704-174419 saved in models/selu_alpha_even_lower_0.0025_model.h5\n",
      "Epoch 1/100\n",
      "   2/1172 [..............................] - ETA: 24:20 - loss: 3.1421 - accuracy: 0.0938WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0100s vs `on_train_batch_end` time: 2.4871s). Check your callbacks.\n",
      "1172/1172 [==============================] - 15s 12ms/step - loss: 2.4039 - accuracy: 0.1020 - val_loss: 2.3308 - val_accuracy: 0.0988\n",
      "Epoch 2/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.4658 - accuracy: 0.0994 - val_loss: 2.3479 - val_accuracy: 0.1007\n",
      "Epoch 3/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.3321 - accuracy: 0.1007 - val_loss: 2.3735 - val_accuracy: 0.0970\n",
      "Epoch 4/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.3337 - accuracy: 0.1013 - val_loss: 2.3371 - val_accuracy: 0.1033\n",
      "Epoch 5/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.3338 - accuracy: 0.1009 - val_loss: 2.3476 - val_accuracy: 0.1007\n",
      "Epoch 6/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.3719 - accuracy: 0.1006 - val_loss: 2.3099 - val_accuracy: 0.1016\n",
      "Epoch 7/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.3333 - accuracy: 0.0996 - val_loss: 2.3342 - val_accuracy: 0.1033\n",
      "Epoch 8/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.3336 - accuracy: 0.0997 - val_loss: 2.3529 - val_accuracy: 0.0946\n",
      "Epoch 9/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.3356 - accuracy: 0.0995 - val_loss: 2.3495 - val_accuracy: 0.0946\n",
      "Epoch 10/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.3350 - accuracy: 0.0967 - val_loss: 2.3463 - val_accuracy: 0.0946\n",
      "Epoch 11/100\n",
      "1172/1172 [==============================] - 12s 11ms/step - loss: 2.4103 - accuracy: 0.0995 - val_loss: 2.3628 - val_accuracy: 0.1033\n",
      "Epoch 12/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.3346 - accuracy: 0.0996 - val_loss: 2.3447 - val_accuracy: 0.0946\n",
      "Epoch 13/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.3350 - accuracy: 0.1021 - val_loss: 2.3421 - val_accuracy: 0.0970\n",
      "Epoch 14/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.3358 - accuracy: 0.0990 - val_loss: 2.3145 - val_accuracy: 0.1031\n",
      "Epoch 15/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.3382 - accuracy: 0.0993 - val_loss: 2.3300 - val_accuracy: 0.1007\n",
      "Epoch 16/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.3359 - accuracy: 0.1001 - val_loss: 2.3345 - val_accuracy: 0.1010\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 2.3345 - accuracy: 0.1010\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3344 - accuracy: 0.1000\n",
      "fitting lr = 0.001 Logged in: logs/fit/selu_alpha_even_lower_0.001_20220704-174745 saved in models/selu_alpha_even_lower_0.001_model.h5\n",
      "Epoch 1/100\n",
      "   2/1172 [..............................] - ETA: 25:32 - loss: 2.9391 - accuracy: 0.0781WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0088s vs `on_train_batch_end` time: 2.6106s). Check your callbacks.\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 2.2168 - accuracy: 0.1562 - val_loss: 2.1079 - val_accuracy: 0.1946\n",
      "Epoch 2/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.1000 - accuracy: 0.1820 - val_loss: 2.1120 - val_accuracy: 0.1942\n",
      "Epoch 3/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.0882 - accuracy: 0.1854 - val_loss: 2.0865 - val_accuracy: 0.1919\n",
      "Epoch 4/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.0831 - accuracy: 0.1890 - val_loss: 2.5265 - val_accuracy: 0.1065\n",
      "Epoch 5/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.0904 - accuracy: 0.1799 - val_loss: 2.4275 - val_accuracy: 0.1636\n",
      "Epoch 6/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.0345 - accuracy: 0.1954 - val_loss: 2.0694 - val_accuracy: 0.2001\n",
      "Epoch 7/100\n",
      "1172/1172 [==============================] - 12s 11ms/step - loss: 2.0527 - accuracy: 0.1980 - val_loss: 2.2683 - val_accuracy: 0.1824\n",
      "Epoch 8/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.0905 - accuracy: 0.1804 - val_loss: 2.1931 - val_accuracy: 0.1849\n",
      "Epoch 9/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.0284 - accuracy: 0.1961 - val_loss: 2.1287 - val_accuracy: 0.1774\n",
      "Epoch 10/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.0135 - accuracy: 0.2040 - val_loss: 2.0944 - val_accuracy: 0.2108\n",
      "Epoch 11/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.9880 - accuracy: 0.2237 - val_loss: 2.0168 - val_accuracy: 0.2150\n",
      "Epoch 12/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.0501 - accuracy: 0.2098 - val_loss: 2.2928 - val_accuracy: 0.1609\n",
      "Epoch 13/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.0843 - accuracy: 0.1871 - val_loss: 2.1414 - val_accuracy: 0.1883\n",
      "Epoch 14/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.0314 - accuracy: 0.2011 - val_loss: 2.2566 - val_accuracy: 0.1685\n",
      "Epoch 15/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.0196 - accuracy: 0.1954 - val_loss: 2.2892 - val_accuracy: 0.1674\n",
      "Epoch 16/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.0029 - accuracy: 0.2191 - val_loss: 2.1742 - val_accuracy: 0.2026\n",
      "Epoch 17/100\n",
      "1172/1172 [==============================] - 12s 11ms/step - loss: 2.0474 - accuracy: 0.1986 - val_loss: 2.2889 - val_accuracy: 0.1679\n",
      "Epoch 18/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 2.0094 - accuracy: 0.2061 - val_loss: 2.0951 - val_accuracy: 0.2007\n",
      "Epoch 19/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.9881 - accuracy: 0.2260 - val_loss: 2.0678 - val_accuracy: 0.2053\n",
      "Epoch 20/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.1021 - accuracy: 0.2080 - val_loss: 2.3647 - val_accuracy: 0.1471\n",
      "Epoch 21/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.0414 - accuracy: 0.2027 - val_loss: 2.1300 - val_accuracy: 0.1767\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 2.1300 - accuracy: 0.1767\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.1204 - accuracy: 0.1827\n",
      "fitting lr = 0.0005 Logged in: logs/fit/selu_alpha_even_lower_0.0005_20220704-175215 saved in models/selu_alpha_even_lower_0.0005_model.h5\n",
      "Epoch 1/100\n",
      "   2/1172 [..............................] - ETA: 26:06 - loss: 3.1711 - accuracy: 0.0625WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0099s vs `on_train_batch_end` time: 2.6662s). Check your callbacks.\n",
      "1172/1172 [==============================] - 15s 13ms/step - loss: 2.2154 - accuracy: 0.1593 - val_loss: 2.0600 - val_accuracy: 0.1782\n",
      "Epoch 2/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.0563 - accuracy: 0.2093 - val_loss: 1.9808 - val_accuracy: 0.2600\n",
      "Epoch 3/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.9482 - accuracy: 0.2587 - val_loss: 2.0092 - val_accuracy: 0.2766\n",
      "Epoch 4/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.9026 - accuracy: 0.2793 - val_loss: 1.8950 - val_accuracy: 0.2877\n",
      "Epoch 5/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.8723 - accuracy: 0.2939 - val_loss: 1.9619 - val_accuracy: 0.2825\n",
      "Epoch 6/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.8448 - accuracy: 0.3041 - val_loss: 1.8188 - val_accuracy: 0.3418\n",
      "Epoch 7/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.8181 - accuracy: 0.3250 - val_loss: 1.9236 - val_accuracy: 0.3162\n",
      "Epoch 8/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 1.8048 - accuracy: 0.3337 - val_loss: 1.8226 - val_accuracy: 0.3460\n",
      "Epoch 9/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.7814 - accuracy: 0.3476 - val_loss: 1.8706 - val_accuracy: 0.3571\n",
      "Epoch 10/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.7650 - accuracy: 0.3537 - val_loss: 1.8189 - val_accuracy: 0.3778\n",
      "Epoch 11/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.7434 - accuracy: 0.3665 - val_loss: 1.8727 - val_accuracy: 0.3794\n",
      "Epoch 12/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.7323 - accuracy: 0.3740 - val_loss: 1.8163 - val_accuracy: 0.3918\n",
      "Epoch 13/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.7190 - accuracy: 0.3773 - val_loss: 1.7830 - val_accuracy: 0.3949\n",
      "Epoch 14/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.7024 - accuracy: 0.3846 - val_loss: 1.7385 - val_accuracy: 0.4078\n",
      "Epoch 15/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.6927 - accuracy: 0.3922 - val_loss: 1.6982 - val_accuracy: 0.4014\n",
      "Epoch 16/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.6841 - accuracy: 0.3968 - val_loss: 1.7436 - val_accuracy: 0.4150\n",
      "Epoch 17/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.6793 - accuracy: 0.4000 - val_loss: 1.6970 - val_accuracy: 0.4184\n",
      "Epoch 18/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.6672 - accuracy: 0.4023 - val_loss: 1.7254 - val_accuracy: 0.4238\n",
      "Epoch 19/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.6573 - accuracy: 0.4048 - val_loss: 1.6712 - val_accuracy: 0.4184\n",
      "Epoch 20/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.6456 - accuracy: 0.4129 - val_loss: 1.7384 - val_accuracy: 0.4191\n",
      "Epoch 21/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.6344 - accuracy: 0.4152 - val_loss: 1.6987 - val_accuracy: 0.4215\n",
      "Epoch 22/100\n",
      "1172/1172 [==============================] - 12s 11ms/step - loss: 1.6260 - accuracy: 0.4243 - val_loss: 1.7372 - val_accuracy: 0.4215\n",
      "Epoch 23/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 1.6224 - accuracy: 0.4225 - val_loss: 1.7927 - val_accuracy: 0.4287\n",
      "Epoch 24/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 1.6108 - accuracy: 0.4251 - val_loss: 1.8032 - val_accuracy: 0.4256\n",
      "Epoch 25/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.6021 - accuracy: 0.4321 - val_loss: 1.6835 - val_accuracy: 0.4391\n",
      "Epoch 26/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.5994 - accuracy: 0.4334 - val_loss: 1.7494 - val_accuracy: 0.4212\n",
      "Epoch 27/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.5889 - accuracy: 0.4341 - val_loss: 1.7151 - val_accuracy: 0.4392\n",
      "Epoch 28/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.5826 - accuracy: 0.4378 - val_loss: 1.7623 - val_accuracy: 0.4486\n",
      "Epoch 29/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.5802 - accuracy: 0.4402 - val_loss: 1.6651 - val_accuracy: 0.4520\n",
      "Epoch 30/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.5784 - accuracy: 0.4393 - val_loss: 1.7363 - val_accuracy: 0.4100\n",
      "Epoch 31/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.5777 - accuracy: 0.4383 - val_loss: 1.6797 - val_accuracy: 0.4426\n",
      "Epoch 32/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.5616 - accuracy: 0.4482 - val_loss: 1.7271 - val_accuracy: 0.4557\n",
      "Epoch 33/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.5604 - accuracy: 0.4432 - val_loss: 1.7032 - val_accuracy: 0.4490\n",
      "Epoch 34/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.5513 - accuracy: 0.4494 - val_loss: 1.6832 - val_accuracy: 0.4393\n",
      "Epoch 35/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 1.5810 - accuracy: 0.4378 - val_loss: 1.6887 - val_accuracy: 0.4319\n",
      "Epoch 36/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 1.5474 - accuracy: 0.4530 - val_loss: 1.8616 - val_accuracy: 0.4154\n",
      "Epoch 37/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.5443 - accuracy: 0.4534 - val_loss: 1.6668 - val_accuracy: 0.4513\n",
      "Epoch 38/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.5401 - accuracy: 0.4585 - val_loss: 1.6101 - val_accuracy: 0.4634\n",
      "Epoch 39/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.5387 - accuracy: 0.4562 - val_loss: 1.6229 - val_accuracy: 0.4613\n",
      "Epoch 40/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5340 - accuracy: 0.4587 - val_loss: 1.6774 - val_accuracy: 0.4660\n",
      "Epoch 41/100\n",
      "1172/1172 [==============================] - 12s 11ms/step - loss: 1.5279 - accuracy: 0.4585 - val_loss: 1.6127 - val_accuracy: 0.4653\n",
      "Epoch 42/100\n",
      "1172/1172 [==============================] - 12s 11ms/step - loss: 1.5172 - accuracy: 0.4634 - val_loss: 1.7006 - val_accuracy: 0.4462\n",
      "Epoch 43/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.5141 - accuracy: 0.4638 - val_loss: 1.7406 - val_accuracy: 0.4493\n",
      "Epoch 44/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.5184 - accuracy: 0.4634 - val_loss: 1.6557 - val_accuracy: 0.4583\n",
      "Epoch 45/100\n",
      "1172/1172 [==============================] - 12s 11ms/step - loss: 1.5173 - accuracy: 0.4622 - val_loss: 1.6560 - val_accuracy: 0.4438\n",
      "Epoch 46/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.5046 - accuracy: 0.4693 - val_loss: 1.6111 - val_accuracy: 0.4719\n",
      "Epoch 47/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.4983 - accuracy: 0.4714 - val_loss: 1.6743 - val_accuracy: 0.4657\n",
      "Epoch 48/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.5005 - accuracy: 0.4712 - val_loss: 1.7234 - val_accuracy: 0.4601\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 1.7234 - accuracy: 0.4601\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 1.7227 - accuracy: 0.4567\n",
      "fitting lr = 0.00025 Logged in: logs/fit/selu_alpha_even_lower_0.00025_20220704-180205 saved in models/selu_alpha_even_lower_0.00025_model.h5\n",
      "Epoch 1/100\n",
      "   2/1172 [..............................] - ETA: 25:52 - loss: 2.9462 - accuracy: 0.1094WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0100s vs `on_train_batch_end` time: 2.6436s). Check your callbacks.\n",
      "1172/1172 [==============================] - 15s 13ms/step - loss: 2.2399 - accuracy: 0.1571 - val_loss: 2.0716 - val_accuracy: 0.1894\n",
      "Epoch 2/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 2.0659 - accuracy: 0.2059 - val_loss: 2.0086 - val_accuracy: 0.2509\n",
      "Epoch 3/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.9553 - accuracy: 0.2642 - val_loss: 1.8997 - val_accuracy: 0.3298\n",
      "Epoch 4/100\n",
      "1172/1172 [==============================] - 12s 11ms/step - loss: 1.8859 - accuracy: 0.3104 - val_loss: 1.8529 - val_accuracy: 0.3452\n",
      "Epoch 5/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.8432 - accuracy: 0.3301 - val_loss: 1.8432 - val_accuracy: 0.3556\n",
      "Epoch 6/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.8115 - accuracy: 0.3449 - val_loss: 1.8098 - val_accuracy: 0.3728\n",
      "Epoch 7/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.7813 - accuracy: 0.3569 - val_loss: 1.7650 - val_accuracy: 0.3756\n",
      "Epoch 8/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.7619 - accuracy: 0.3656 - val_loss: 1.7677 - val_accuracy: 0.3888\n",
      "Epoch 9/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.7406 - accuracy: 0.3753 - val_loss: 1.7290 - val_accuracy: 0.3995\n",
      "Epoch 10/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.7222 - accuracy: 0.3823 - val_loss: 1.7176 - val_accuracy: 0.4111\n",
      "Epoch 11/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.7031 - accuracy: 0.3870 - val_loss: 1.6839 - val_accuracy: 0.4146\n",
      "Epoch 12/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.6872 - accuracy: 0.3951 - val_loss: 1.6968 - val_accuracy: 0.4042\n",
      "Epoch 13/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.6725 - accuracy: 0.4033 - val_loss: 1.6743 - val_accuracy: 0.4276\n",
      "Epoch 14/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.6598 - accuracy: 0.4042 - val_loss: 1.6746 - val_accuracy: 0.4344\n",
      "Epoch 15/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.6460 - accuracy: 0.4124 - val_loss: 1.6800 - val_accuracy: 0.4254\n",
      "Epoch 16/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.6366 - accuracy: 0.4145 - val_loss: 1.6624 - val_accuracy: 0.4319\n",
      "Epoch 17/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.6207 - accuracy: 0.4223 - val_loss: 1.6973 - val_accuracy: 0.4302\n",
      "Epoch 18/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.6178 - accuracy: 0.4193 - val_loss: 1.6555 - val_accuracy: 0.4437\n",
      "Epoch 19/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.6097 - accuracy: 0.4281 - val_loss: 1.6271 - val_accuracy: 0.4470\n",
      "Epoch 20/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5975 - accuracy: 0.4290 - val_loss: 1.7139 - val_accuracy: 0.4402\n",
      "Epoch 21/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.5891 - accuracy: 0.4314 - val_loss: 1.6386 - val_accuracy: 0.4478\n",
      "Epoch 22/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.5822 - accuracy: 0.4345 - val_loss: 1.5966 - val_accuracy: 0.4573\n",
      "Epoch 23/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.5757 - accuracy: 0.4377 - val_loss: 1.6615 - val_accuracy: 0.4458\n",
      "Epoch 24/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.5625 - accuracy: 0.4430 - val_loss: 1.5856 - val_accuracy: 0.4545\n",
      "Epoch 25/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5549 - accuracy: 0.4462 - val_loss: 1.7079 - val_accuracy: 0.4536\n",
      "Epoch 26/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.5476 - accuracy: 0.4452 - val_loss: 1.6433 - val_accuracy: 0.4594\n",
      "Epoch 27/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.5468 - accuracy: 0.4493 - val_loss: 1.6265 - val_accuracy: 0.4526\n",
      "Epoch 28/100\n",
      "1172/1172 [==============================] - 12s 11ms/step - loss: 1.5359 - accuracy: 0.4542 - val_loss: 1.6837 - val_accuracy: 0.4493\n",
      "Epoch 29/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.5296 - accuracy: 0.4562 - val_loss: 1.6613 - val_accuracy: 0.4628\n",
      "Epoch 30/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.5222 - accuracy: 0.4605 - val_loss: 1.6130 - val_accuracy: 0.4614\n",
      "Epoch 31/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5158 - accuracy: 0.4569 - val_loss: 1.6038 - val_accuracy: 0.4614\n",
      "Epoch 32/100\n",
      "1172/1172 [==============================] - 12s 11ms/step - loss: 1.5087 - accuracy: 0.4646 - val_loss: 1.6188 - val_accuracy: 0.4666\n",
      "Epoch 33/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.5062 - accuracy: 0.4627 - val_loss: 1.5945 - val_accuracy: 0.4630\n",
      "Epoch 34/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.5056 - accuracy: 0.4623 - val_loss: 1.6617 - val_accuracy: 0.4662\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 1.6617 - accuracy: 0.4662\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 1.6499 - accuracy: 0.4744\n",
      "fitting lr = 0.0001 Logged in: logs/fit/selu_alpha_even_lower_0.0001_20220704-180919 saved in models/selu_alpha_even_lower_0.0001_model.h5\n",
      "Epoch 1/100\n",
      "   2/1172 [..............................] - ETA: 21:25 - loss: 2.9749 - accuracy: 0.0781WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0090s vs `on_train_batch_end` time: 2.1883s). Check your callbacks.\n",
      "1172/1172 [==============================] - 15s 13ms/step - loss: 2.3056 - accuracy: 0.1534 - val_loss: 2.0638 - val_accuracy: 0.1730\n",
      "Epoch 2/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.0879 - accuracy: 0.2066 - val_loss: 1.9480 - val_accuracy: 0.2677\n",
      "Epoch 3/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.0001 - accuracy: 0.2426 - val_loss: 1.8719 - val_accuracy: 0.2965\n",
      "Epoch 4/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.9424 - accuracy: 0.2716 - val_loss: 1.8265 - val_accuracy: 0.3210\n",
      "Epoch 5/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.8988 - accuracy: 0.2936 - val_loss: 1.8198 - val_accuracy: 0.3386\n",
      "Epoch 6/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.8715 - accuracy: 0.3075 - val_loss: 1.8020 - val_accuracy: 0.3454\n",
      "Epoch 7/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.8445 - accuracy: 0.3245 - val_loss: 1.7518 - val_accuracy: 0.3682\n",
      "Epoch 8/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.8176 - accuracy: 0.3343 - val_loss: 1.7317 - val_accuracy: 0.3765\n",
      "Epoch 9/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.7949 - accuracy: 0.3453 - val_loss: 1.7470 - val_accuracy: 0.3763\n",
      "Epoch 10/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.7695 - accuracy: 0.3576 - val_loss: 1.7224 - val_accuracy: 0.3876\n",
      "Epoch 11/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.7537 - accuracy: 0.3653 - val_loss: 1.6921 - val_accuracy: 0.3954\n",
      "Epoch 12/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.7388 - accuracy: 0.3714 - val_loss: 1.6684 - val_accuracy: 0.4064\n",
      "Epoch 13/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.7287 - accuracy: 0.3758 - val_loss: 1.6445 - val_accuracy: 0.4106\n",
      "Epoch 14/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.7091 - accuracy: 0.3827 - val_loss: 1.6725 - val_accuracy: 0.4051\n",
      "Epoch 15/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.7000 - accuracy: 0.3838 - val_loss: 1.6602 - val_accuracy: 0.4194\n",
      "Epoch 16/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.6874 - accuracy: 0.3895 - val_loss: 1.6578 - val_accuracy: 0.4112\n",
      "Epoch 17/100\n",
      "1172/1172 [==============================] - 12s 11ms/step - loss: 1.6768 - accuracy: 0.3953 - val_loss: 1.6042 - val_accuracy: 0.4287\n",
      "Epoch 18/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.6646 - accuracy: 0.4003 - val_loss: 1.6240 - val_accuracy: 0.4282\n",
      "Epoch 19/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.6554 - accuracy: 0.4024 - val_loss: 1.6544 - val_accuracy: 0.4225\n",
      "Epoch 20/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.6463 - accuracy: 0.4102 - val_loss: 1.6304 - val_accuracy: 0.4323\n",
      "Epoch 21/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.6416 - accuracy: 0.4112 - val_loss: 1.6275 - val_accuracy: 0.4284\n",
      "Epoch 22/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.6338 - accuracy: 0.4125 - val_loss: 1.6347 - val_accuracy: 0.4290\n",
      "Epoch 23/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.6234 - accuracy: 0.4131 - val_loss: 1.5973 - val_accuracy: 0.4443\n",
      "Epoch 24/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.6188 - accuracy: 0.4134 - val_loss: 1.6179 - val_accuracy: 0.4396\n",
      "Epoch 25/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.6097 - accuracy: 0.4209 - val_loss: 1.5978 - val_accuracy: 0.4460\n",
      "Epoch 26/100\n",
      "1172/1172 [==============================] - 12s 11ms/step - loss: 1.6066 - accuracy: 0.4244 - val_loss: 1.5941 - val_accuracy: 0.4472\n",
      "Epoch 27/100\n",
      "1172/1172 [==============================] - 12s 11ms/step - loss: 1.6027 - accuracy: 0.4246 - val_loss: 1.5736 - val_accuracy: 0.4562\n",
      "Epoch 28/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.5957 - accuracy: 0.4269 - val_loss: 1.6498 - val_accuracy: 0.4371\n",
      "Epoch 29/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.5889 - accuracy: 0.4292 - val_loss: 1.6143 - val_accuracy: 0.4463\n",
      "Epoch 30/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.5811 - accuracy: 0.4303 - val_loss: 1.5941 - val_accuracy: 0.4523\n",
      "Epoch 31/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.5733 - accuracy: 0.4343 - val_loss: 1.5669 - val_accuracy: 0.4553\n",
      "Epoch 32/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5694 - accuracy: 0.4359 - val_loss: 1.5668 - val_accuracy: 0.4572\n",
      "Epoch 33/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.5645 - accuracy: 0.4355 - val_loss: 1.6134 - val_accuracy: 0.4532\n",
      "Epoch 34/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.5557 - accuracy: 0.4396 - val_loss: 1.6028 - val_accuracy: 0.4472\n",
      "Epoch 35/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.5500 - accuracy: 0.4423 - val_loss: 1.5997 - val_accuracy: 0.4526\n",
      "Epoch 36/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.5492 - accuracy: 0.4433 - val_loss: 1.5896 - val_accuracy: 0.4583\n",
      "Epoch 37/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.5396 - accuracy: 0.4475 - val_loss: 1.5751 - val_accuracy: 0.4634\n",
      "Epoch 38/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.5357 - accuracy: 0.4491 - val_loss: 1.6010 - val_accuracy: 0.4659\n",
      "Epoch 39/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.5318 - accuracy: 0.4516 - val_loss: 1.5533 - val_accuracy: 0.4683\n",
      "Epoch 40/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.5269 - accuracy: 0.4557 - val_loss: 1.5779 - val_accuracy: 0.4664\n",
      "Epoch 41/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.5169 - accuracy: 0.4559 - val_loss: 1.5973 - val_accuracy: 0.4614\n",
      "Epoch 42/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.5159 - accuracy: 0.4553 - val_loss: 1.6283 - val_accuracy: 0.4631\n",
      "Epoch 43/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.5202 - accuracy: 0.4558 - val_loss: 1.6008 - val_accuracy: 0.4666\n",
      "Epoch 44/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.5071 - accuracy: 0.4583 - val_loss: 1.5402 - val_accuracy: 0.4677\n",
      "Epoch 45/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.4983 - accuracy: 0.4593 - val_loss: 1.5882 - val_accuracy: 0.4757\n",
      "Epoch 46/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5008 - accuracy: 0.4630 - val_loss: 1.6016 - val_accuracy: 0.4686\n",
      "Epoch 47/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.4975 - accuracy: 0.4635 - val_loss: 1.5380 - val_accuracy: 0.4777\n",
      "Epoch 48/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.4884 - accuracy: 0.4705 - val_loss: 1.5923 - val_accuracy: 0.4726\n",
      "Epoch 49/100\n",
      "1172/1172 [==============================] - 12s 11ms/step - loss: 1.4889 - accuracy: 0.4682 - val_loss: 1.5414 - val_accuracy: 0.4761\n",
      "Epoch 50/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.4841 - accuracy: 0.4684 - val_loss: 1.5469 - val_accuracy: 0.4769\n",
      "Epoch 51/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.4780 - accuracy: 0.4707 - val_loss: 1.5678 - val_accuracy: 0.4802\n",
      "Epoch 52/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.4788 - accuracy: 0.4735 - val_loss: 1.5691 - val_accuracy: 0.4788\n",
      "Epoch 53/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.4725 - accuracy: 0.4732 - val_loss: 1.5641 - val_accuracy: 0.4809\n",
      "Epoch 54/100\n",
      "1172/1172 [==============================] - 12s 11ms/step - loss: 1.4693 - accuracy: 0.4776 - val_loss: 1.5733 - val_accuracy: 0.4773\n",
      "Epoch 55/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.4642 - accuracy: 0.4746 - val_loss: 1.5697 - val_accuracy: 0.4773\n",
      "Epoch 56/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.4646 - accuracy: 0.4750 - val_loss: 1.5459 - val_accuracy: 0.4864\n",
      "Epoch 57/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.4604 - accuracy: 0.4808 - val_loss: 1.5395 - val_accuracy: 0.4841\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 1.5395 - accuracy: 0.4841\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 1.5173 - accuracy: 0.4793\n",
      "fitting lr = 5e-05 Logged in: logs/fit/selu_alpha_even_lower_5e-05_20220704-182112 saved in models/selu_alpha_even_lower_5e-05_model.h5\n",
      "Epoch 1/100\n",
      "   2/1172 [..............................] - ETA: 24:25 - loss: 2.8701 - accuracy: 0.1406WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0099s vs `on_train_batch_end` time: 2.4956s). Check your callbacks.\n",
      "1172/1172 [==============================] - 15s 13ms/step - loss: 2.4325 - accuracy: 0.1357 - val_loss: 2.1512 - val_accuracy: 0.1813\n",
      "Epoch 2/100\n",
      "1172/1172 [==============================] - 12s 11ms/step - loss: 2.1972 - accuracy: 0.1661 - val_loss: 2.0709 - val_accuracy: 0.1992\n",
      "Epoch 3/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.1404 - accuracy: 0.1766 - val_loss: 2.0105 - val_accuracy: 0.2106\n",
      "Epoch 4/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 2.0587 - accuracy: 0.2186 - val_loss: 1.9158 - val_accuracy: 0.2809\n",
      "Epoch 5/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.9925 - accuracy: 0.2522 - val_loss: 1.8672 - val_accuracy: 0.3027\n",
      "Epoch 6/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.9497 - accuracy: 0.2719 - val_loss: 1.8610 - val_accuracy: 0.3179\n",
      "Epoch 7/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.9228 - accuracy: 0.2834 - val_loss: 1.8262 - val_accuracy: 0.3255\n",
      "Epoch 8/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.8983 - accuracy: 0.2961 - val_loss: 1.8407 - val_accuracy: 0.3243\n",
      "Epoch 9/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.8825 - accuracy: 0.3045 - val_loss: 1.8189 - val_accuracy: 0.3351\n",
      "Epoch 10/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.8646 - accuracy: 0.3137 - val_loss: 1.7839 - val_accuracy: 0.3544\n",
      "Epoch 11/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.8480 - accuracy: 0.3235 - val_loss: 1.7612 - val_accuracy: 0.3621\n",
      "Epoch 12/100\n",
      "1172/1172 [==============================] - 12s 11ms/step - loss: 1.8290 - accuracy: 0.3338 - val_loss: 1.7506 - val_accuracy: 0.3676\n",
      "Epoch 13/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.8188 - accuracy: 0.3353 - val_loss: 1.7851 - val_accuracy: 0.3653\n",
      "Epoch 14/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.8034 - accuracy: 0.3464 - val_loss: 1.7776 - val_accuracy: 0.3711\n",
      "Epoch 15/100\n",
      "1172/1172 [==============================] - 12s 11ms/step - loss: 1.7976 - accuracy: 0.3479 - val_loss: 1.7218 - val_accuracy: 0.3758\n",
      "Epoch 16/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.7840 - accuracy: 0.3513 - val_loss: 1.7340 - val_accuracy: 0.3808\n",
      "Epoch 17/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.7771 - accuracy: 0.3581 - val_loss: 1.7186 - val_accuracy: 0.3898\n",
      "Epoch 18/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.7655 - accuracy: 0.3605 - val_loss: 1.7135 - val_accuracy: 0.3804\n",
      "Epoch 19/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.7557 - accuracy: 0.3648 - val_loss: 1.7283 - val_accuracy: 0.3914\n",
      "Epoch 20/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.7463 - accuracy: 0.3711 - val_loss: 1.6698 - val_accuracy: 0.4014\n",
      "Epoch 21/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.7349 - accuracy: 0.3756 - val_loss: 1.6788 - val_accuracy: 0.4014\n",
      "Epoch 22/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.7270 - accuracy: 0.3764 - val_loss: 1.6807 - val_accuracy: 0.4011\n",
      "Epoch 23/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.7160 - accuracy: 0.3801 - val_loss: 1.6751 - val_accuracy: 0.4007\n",
      "Epoch 24/100\n",
      "1172/1172 [==============================] - 12s 11ms/step - loss: 1.7098 - accuracy: 0.3831 - val_loss: 1.6575 - val_accuracy: 0.4113\n",
      "Epoch 25/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 1.7073 - accuracy: 0.3811 - val_loss: 1.6669 - val_accuracy: 0.4106\n",
      "Epoch 26/100\n",
      "1172/1172 [==============================] - 12s 11ms/step - loss: 1.6964 - accuracy: 0.3896 - val_loss: 1.6350 - val_accuracy: 0.4201\n",
      "Epoch 27/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.6870 - accuracy: 0.3912 - val_loss: 1.6344 - val_accuracy: 0.4236\n",
      "Epoch 28/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.6809 - accuracy: 0.3945 - val_loss: 1.6371 - val_accuracy: 0.4177\n",
      "Epoch 29/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.6717 - accuracy: 0.3937 - val_loss: 1.6376 - val_accuracy: 0.4193\n",
      "Epoch 30/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.6628 - accuracy: 0.3982 - val_loss: 1.6628 - val_accuracy: 0.4164\n",
      "Epoch 31/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.6582 - accuracy: 0.4034 - val_loss: 1.6496 - val_accuracy: 0.4210\n",
      "Epoch 32/100\n",
      "1172/1172 [==============================] - 12s 11ms/step - loss: 1.6560 - accuracy: 0.4013 - val_loss: 1.5884 - val_accuracy: 0.4330\n",
      "Epoch 33/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.6455 - accuracy: 0.4072 - val_loss: 1.6177 - val_accuracy: 0.4333\n",
      "Epoch 34/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 1.6388 - accuracy: 0.4061 - val_loss: 1.6089 - val_accuracy: 0.4276\n",
      "Epoch 35/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 1.6400 - accuracy: 0.4098 - val_loss: 1.5919 - val_accuracy: 0.4312\n",
      "Epoch 36/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.6317 - accuracy: 0.4131 - val_loss: 1.5994 - val_accuracy: 0.4366\n",
      "Epoch 37/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.6280 - accuracy: 0.4147 - val_loss: 1.5846 - val_accuracy: 0.4387\n",
      "Epoch 38/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.6243 - accuracy: 0.4177 - val_loss: 1.5816 - val_accuracy: 0.4414\n",
      "Epoch 39/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.6181 - accuracy: 0.4156 - val_loss: 1.6001 - val_accuracy: 0.4399\n",
      "Epoch 40/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.6082 - accuracy: 0.4207 - val_loss: 1.5820 - val_accuracy: 0.4354\n",
      "Epoch 41/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.6080 - accuracy: 0.4192 - val_loss: 1.5854 - val_accuracy: 0.4449\n",
      "Epoch 42/100\n",
      "1172/1172 [==============================] - 12s 11ms/step - loss: 1.6060 - accuracy: 0.4220 - val_loss: 1.5799 - val_accuracy: 0.4458\n",
      "Epoch 43/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.6015 - accuracy: 0.4255 - val_loss: 1.5676 - val_accuracy: 0.4493\n",
      "Epoch 44/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.5923 - accuracy: 0.4301 - val_loss: 1.5778 - val_accuracy: 0.4438\n",
      "Epoch 45/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.5953 - accuracy: 0.4278 - val_loss: 1.5552 - val_accuracy: 0.4494\n",
      "Epoch 46/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5857 - accuracy: 0.4282 - val_loss: 1.5769 - val_accuracy: 0.4506\n",
      "Epoch 47/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5798 - accuracy: 0.4294 - val_loss: 1.5846 - val_accuracy: 0.4534\n",
      "Epoch 48/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.5810 - accuracy: 0.4321 - val_loss: 1.5568 - val_accuracy: 0.4508\n",
      "Epoch 49/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.5725 - accuracy: 0.4338 - val_loss: 1.5562 - val_accuracy: 0.4530\n",
      "Epoch 50/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.5664 - accuracy: 0.4359 - val_loss: 1.5766 - val_accuracy: 0.4510\n",
      "Epoch 51/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.5689 - accuracy: 0.4373 - val_loss: 1.5551 - val_accuracy: 0.4596\n",
      "Epoch 52/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.5666 - accuracy: 0.4371 - val_loss: 1.5521 - val_accuracy: 0.4596\n",
      "Epoch 53/100\n",
      "1172/1172 [==============================] - 12s 11ms/step - loss: 1.5613 - accuracy: 0.4413 - val_loss: 1.5717 - val_accuracy: 0.4562\n",
      "Epoch 54/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5558 - accuracy: 0.4389 - val_loss: 1.5565 - val_accuracy: 0.4620\n",
      "Epoch 55/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5499 - accuracy: 0.4404 - val_loss: 1.5445 - val_accuracy: 0.4610\n",
      "Epoch 56/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5475 - accuracy: 0.4463 - val_loss: 1.5356 - val_accuracy: 0.4642\n",
      "Epoch 57/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.5492 - accuracy: 0.4442 - val_loss: 1.5198 - val_accuracy: 0.4638\n",
      "Epoch 58/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.5479 - accuracy: 0.4418 - val_loss: 1.5266 - val_accuracy: 0.4685\n",
      "Epoch 59/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.5381 - accuracy: 0.4478 - val_loss: 1.5462 - val_accuracy: 0.4655\n",
      "Epoch 60/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.5332 - accuracy: 0.4482 - val_loss: 1.5590 - val_accuracy: 0.4610\n",
      "Epoch 61/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 1.5334 - accuracy: 0.4528 - val_loss: 1.5336 - val_accuracy: 0.4640\n",
      "Epoch 62/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5286 - accuracy: 0.4519 - val_loss: 1.5398 - val_accuracy: 0.4694\n",
      "Epoch 63/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5296 - accuracy: 0.4530 - val_loss: 1.5730 - val_accuracy: 0.4541\n",
      "Epoch 64/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 1.5220 - accuracy: 0.4531 - val_loss: 1.5319 - val_accuracy: 0.4690\n",
      "Epoch 65/100\n",
      "1172/1172 [==============================] - 16s 14ms/step - loss: 1.5231 - accuracy: 0.4528 - val_loss: 1.5206 - val_accuracy: 0.4661\n",
      "Epoch 66/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5138 - accuracy: 0.4554 - val_loss: 1.5329 - val_accuracy: 0.4677\n",
      "Epoch 67/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5111 - accuracy: 0.4545 - val_loss: 1.5160 - val_accuracy: 0.4715\n",
      "Epoch 68/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 1.5154 - accuracy: 0.4538 - val_loss: 1.5042 - val_accuracy: 0.4735\n",
      "Epoch 69/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5062 - accuracy: 0.4599 - val_loss: 1.5239 - val_accuracy: 0.4711\n",
      "Epoch 70/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 1.5076 - accuracy: 0.4587 - val_loss: 1.5200 - val_accuracy: 0.4713\n",
      "Epoch 71/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.5053 - accuracy: 0.4605 - val_loss: 1.5427 - val_accuracy: 0.4720\n",
      "Epoch 72/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.4988 - accuracy: 0.4619 - val_loss: 1.5371 - val_accuracy: 0.4761\n",
      "Epoch 73/100\n",
      "1172/1172 [==============================] - 12s 11ms/step - loss: 1.4911 - accuracy: 0.4650 - val_loss: 1.4999 - val_accuracy: 0.4787\n",
      "Epoch 74/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.4894 - accuracy: 0.4681 - val_loss: 1.5122 - val_accuracy: 0.4766\n",
      "Epoch 75/100\n",
      "1172/1172 [==============================] - 12s 11ms/step - loss: 1.4973 - accuracy: 0.4646 - val_loss: 1.4976 - val_accuracy: 0.4788\n",
      "Epoch 76/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.4937 - accuracy: 0.4639 - val_loss: 1.5195 - val_accuracy: 0.4787\n",
      "Epoch 77/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.4871 - accuracy: 0.4685 - val_loss: 1.5065 - val_accuracy: 0.4771\n",
      "Epoch 78/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.4807 - accuracy: 0.4713 - val_loss: 1.5367 - val_accuracy: 0.4781\n",
      "Epoch 79/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.4803 - accuracy: 0.4688 - val_loss: 1.5019 - val_accuracy: 0.4809\n",
      "Epoch 80/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 1.4776 - accuracy: 0.4704 - val_loss: 1.5071 - val_accuracy: 0.4766\n",
      "Epoch 81/100\n",
      "1172/1172 [==============================] - 12s 11ms/step - loss: 1.4756 - accuracy: 0.4720 - val_loss: 1.5035 - val_accuracy: 0.4823\n",
      "Epoch 82/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.4746 - accuracy: 0.4704 - val_loss: 1.5050 - val_accuracy: 0.4831\n",
      "Epoch 83/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.4758 - accuracy: 0.4694 - val_loss: 1.5152 - val_accuracy: 0.4811\n",
      "Epoch 84/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.4653 - accuracy: 0.4766 - val_loss: 1.4954 - val_accuracy: 0.4773\n",
      "Epoch 85/100\n",
      "1172/1172 [==============================] - 12s 11ms/step - loss: 1.4611 - accuracy: 0.4793 - val_loss: 1.5204 - val_accuracy: 0.4836\n",
      "Epoch 86/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.4631 - accuracy: 0.4747 - val_loss: 1.5309 - val_accuracy: 0.4835\n",
      "Epoch 87/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 1.4645 - accuracy: 0.4743 - val_loss: 1.4913 - val_accuracy: 0.4838\n",
      "Epoch 88/100\n",
      "1172/1172 [==============================] - 14s 12ms/step - loss: 1.4631 - accuracy: 0.4778 - val_loss: 1.4863 - val_accuracy: 0.4882\n",
      "Epoch 89/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.4574 - accuracy: 0.4760 - val_loss: 1.5019 - val_accuracy: 0.4879\n",
      "Epoch 90/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.4580 - accuracy: 0.4795 - val_loss: 1.4899 - val_accuracy: 0.4843\n",
      "Epoch 91/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.4543 - accuracy: 0.4784 - val_loss: 1.5414 - val_accuracy: 0.4822\n",
      "Epoch 92/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.4506 - accuracy: 0.4845 - val_loss: 1.4720 - val_accuracy: 0.4906\n",
      "Epoch 93/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.4498 - accuracy: 0.4805 - val_loss: 1.4946 - val_accuracy: 0.4905\n",
      "Epoch 94/100\n",
      "1172/1172 [==============================] - 12s 10ms/step - loss: 1.4463 - accuracy: 0.4821 - val_loss: 1.4884 - val_accuracy: 0.4822\n",
      "Epoch 95/100\n",
      "1172/1172 [==============================] - 15s 12ms/step - loss: 1.4405 - accuracy: 0.4844 - val_loss: 1.4760 - val_accuracy: 0.4947\n",
      "Epoch 96/100\n",
      "1172/1172 [==============================] - 16s 14ms/step - loss: 1.4425 - accuracy: 0.4833 - val_loss: 1.5008 - val_accuracy: 0.4930\n",
      "Epoch 97/100\n",
      "1172/1172 [==============================] - 13s 11ms/step - loss: 1.4454 - accuracy: 0.4838 - val_loss: 1.4735 - val_accuracy: 0.4904\n",
      "Epoch 98/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 1.4422 - accuracy: 0.4832 - val_loss: 1.4803 - val_accuracy: 0.4962\n",
      "Epoch 99/100\n",
      "1172/1172 [==============================] - 11s 10ms/step - loss: 1.4368 - accuracy: 0.4864 - val_loss: 1.4765 - val_accuracy: 0.4911\n",
      "Epoch 100/100\n",
      "1172/1172 [==============================] - 11s 9ms/step - loss: 1.4364 - accuracy: 0.4888 - val_loss: 1.4891 - val_accuracy: 0.4906\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 1.4891 - accuracy: 0.4906\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 1.4727 - accuracy: 0.4944\n",
      "Best validation performance: 49.06% for lr: 5e-05\n",
      "Best test performance: 49.44% for lr: 5e-05\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19248/1711707906.py\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mlrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.005\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0025\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0005\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.00025\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.00005\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mvalid_accuracy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_accuracy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtune_lrs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mselu_alpha_model_even_lower\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"selu_alpha_even_lower\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"validation accuracy: {valid_accuracy}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "def selu_alpha_model_even_lower():\n",
    "    return make_DNN_alpha(selu_layer,20,False,0.025)\n",
    "\n",
    "lrs=[0.01, 0.005, 0.0025, 0.001, 0.0005, 0.00025, 0.0001, 0.00005]\n",
    "\n",
    "valid_accuracy,test_accuracy,logs, best_models = tune_lrs(selu_alpha_model_even_lower, lrs, \"selu_alpha_even_lower\")\n",
    "\n",
    "print(f\"validation accuracy: {valid_accuracy}\")\n",
    "print(f\"test accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: models/selu_alpha_even_lower_0.01_model.h5\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 2.3870 - accuracy: 0.0946\n",
      "validation accuracy: 9.46%\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3905 - accuracy: 0.1000\n",
      "test accuracy 10.00%\n",
      "model: models/selu_alpha_even_lower_0.005_model.h5\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 2.3222 - accuracy: 0.0975\n",
      "validation accuracy: 9.75%\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3202 - accuracy: 0.1000\n",
      "test accuracy 10.00%\n",
      "model: models/selu_alpha_even_lower_0.0025_model.h5\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 2.3099 - accuracy: 0.1016\n",
      "validation accuracy: 10.16%\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3096 - accuracy: 0.1000\n",
      "test accuracy 10.00%\n",
      "model: models/selu_alpha_even_lower_0.001_model.h5\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 2.0168 - accuracy: 0.2150\n",
      "validation accuracy: 21.50%\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.0096 - accuracy: 0.2176\n",
      "test accuracy 21.76%\n",
      "model: models/selu_alpha_even_lower_0.0005_model.h5\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 1.6101 - accuracy: 0.4634\n",
      "validation accuracy: 46.34%\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 1.6101 - accuracy: 0.4669\n",
      "test accuracy 46.69%\n",
      "model: models/selu_alpha_even_lower_0.00025_model.h5\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 1.5856 - accuracy: 0.4545\n",
      "validation accuracy: 45.45%\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 1.5662 - accuracy: 0.4644\n",
      "test accuracy 46.44%\n",
      "model: models/selu_alpha_even_lower_0.0001_model.h5\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 1.5380 - accuracy: 0.4777\n",
      "validation accuracy: 47.77%\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 1.5130 - accuracy: 0.4825\n",
      "test accuracy 48.25%\n",
      "model: models/selu_alpha_even_lower_5e-05_model.h5\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 1.4720 - accuracy: 0.4906\n",
      "validation accuracy: 49.06%\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 1.4562 - accuracy: 0.4915\n",
      "test accuracy 49.15%\n",
      "Best test accuracy with 0.025 ALPHA dropout: 49.15% with lr: 5e-05\n"
     ]
    }
   ],
   "source": [
    "#failed to add best_models to the list of assignments, so doing it manually...\n",
    "import numpy as np\n",
    "lrs = [0.01, 0.005, 0.0025, 0.001, 0.0005, 0.00025, 0.0001, 0.00005]\n",
    "\n",
    "models = [\"models/selu_alpha_even_lower_0.005_model.h5\",\n",
    "            \"models/selu_alpha_even_lower_0.005_model.h5\",\n",
    "            \"models/selu_alpha_even_lower_0.005_model.h5\",\n",
    "            \"models/selu_alpha_even_lower_0.005_model.h5\",\n",
    "            \"models/selu_alpha_even_lower_0.005_model.h5\"]\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "test_acc = []\n",
    "for lr in lrs:\n",
    "    model_file = f\"models/selu_alpha_even_lower_{lr}_model.h5\"\n",
    "    model = selu_alpha_model_even_lower()\n",
    "    model.compile(loss='sparse_categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "    model.load_weights(model_file)\n",
    "    print(f\"model: {model_file}\")\n",
    "    print(f\"validation accuracy: {model.evaluate(X_valid,y_valid)[1]:.2%}\")\n",
    "    test_acc.append(model.evaluate(X_test,y_test)[1])\n",
    "    print(f\"test accuracy {test_acc[-1]:.2%}\")\n",
    "\n",
    "print(f\"Best test accuracy with 0.025 ALPHA dropout: {np.max(test_acc):.2%} with lr: {lrs[np.argmax(test_acc)]}\")\n",
    "#TODO: replace the evaluation code below with the code above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48248\n"
     ]
    }
   ],
   "source": [
    "#MC_dropout\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#This is softmax classification so prediction will be prtobabilities for the 10 types\n",
    "y_preds = np.stack([model(X_valid,training=True) for sample in range(100)])\n",
    "y_pred = y_preds.mean(axis=0)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "accuracy = np.sum(y_pred == y_valid)/len(y_valid)\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('PandasNumpyMathplotlib')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40a6d3daba109dd36035c486cbe134237beb3103005e5f3b9c526cde33e5461d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
